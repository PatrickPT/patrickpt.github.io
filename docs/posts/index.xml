<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on  </title>
    <link>https://patrickpt.github.io/posts/</link>
    <description>Recent content in Posts on  </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 21 Jun 2022 15:06:45 -0500</lastBuildDate><atom:link href="https://patrickpt.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why DeepSeek-R1 was a significant step for Open Source AI</title>
      <link>https://patrickpt.github.io/posts/deepseek/</link>
      <pubDate>Sun, 09 Feb 2025 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/deepseek/</guid>
      <description>TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.
Why all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind.</description>
    </item>
    
    <item>
      <title>A view on Byte Latent Transformers</title>
      <link>https://patrickpt.github.io/posts/blt/</link>
      <pubDate>Thu, 02 Jan 2025 09:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/blt/</guid>
      <description>TL;DR This blog post explains the Byte Latent Transformer (BLT), a tokenizer-free architecture for NLP tasks. BLT processes raw byte data dynamically, making it a scalable, efficient, and robust alternative to traditional token-based models.
Why Should I Care? Traditional LLMs rely on tokenizationâ€”a preprocessing step that compresses text into a fixed vocabulary. While effective, tokenization introduces several challenges:
High Costs: Fine-tuning LLMs with domain-specific data demands extensive computational resources, often requiring significant financial investments.</description>
    </item>
    
    <item>
      <title>LLM Quantization in a nutshell</title>
      <link>https://patrickpt.github.io/posts/quantllm/</link>
      <pubDate>Sun, 28 Jan 2024 09:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/quantllm/</guid>
      <description>LLM Quantization in a Nutshell: A Detailed Exploration Quantization is a critical technique to reduce the precision of large language models (LLMs), making them lighter and more efficient without a significant loss in performance. This post delves into the nuts and bolts of quantization, explains the underlying math, and examines practical aspects using tools like llama.cpp.
TL;DR LLM quantization reduces the numerical precision of model parameters to decrease memory usage and improve inference speed.</description>
    </item>
    
    <item>
      <title>What is Parameter Efficient Finetuning?</title>
      <link>https://patrickpt.github.io/posts/peft/</link>
      <pubDate>Wed, 01 Nov 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/peft/</guid>
      <description>TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.</description>
    </item>
    
    <item>
      <title>Hands on with Retrieval Augmented Generation</title>
      <link>https://patrickpt.github.io/posts/hands-on-rag/</link>
      <pubDate>Sat, 07 Oct 2023 10:30:00 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/hands-on-rag/</guid>
      <description>TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&amp;rsquo;s with long context windows.</description>
    </item>
    
    <item>
      <title>What is Retrieval Augmented Generation?</title>
      <link>https://patrickpt.github.io/posts/rag_intro/</link>
      <pubDate>Fri, 29 Sep 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/rag_intro/</guid>
      <description>TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.
Why should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.
picture from Neo4J
But fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:</description>
    </item>
    
    <item>
      <title>Generalized Models vs Specialized Models</title>
      <link>https://patrickpt.github.io/posts/generalvsspecializedmodels/</link>
      <pubDate>Tue, 05 Sep 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/generalvsspecializedmodels/</guid>
      <description>TL;DR This blogpost focusses on ML Design and Architecture and tries to give some intuition and hints for deciding between one generalized and multiple specialized models for the same business requriement and dataset.
Consider it a nudge to dive deeper into the topic
Why should i care? Transparency in machine learning is crucial for business stakeholders because it fosters trust and informed decision-making. Business Stakeholders need to understand not only the potential benefits but also the limitations and risks associated with machine learning models.</description>
    </item>
    
    <item>
      <title>What are Transformers?</title>
      <link>https://patrickpt.github.io/posts/transformers/</link>
      <pubDate>Wed, 30 Aug 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/transformers/</guid>
      <description>Transformers: A Deep Dive into the Transformer Architecture Below is a revised, more detailed version of the article that removes the BERT code snippet, expands on both the mathematical background and implementation details, and includes additional illustrative images.
Transformers: A Deep Dive into the Transformer Architecture Transformers are a revolutionary type of neural network architecture introduced in the seminal paper Attention is All You Need by Vaswani et al. (2017). They have dramatically advanced the field of Natural Language Processing (NLP) and beyond, powering tasks like language translation, text summarization, and even applications in computer vision.</description>
    </item>
    
    <item>
      <title>Road to GCP Professional Data Engineer</title>
      <link>https://patrickpt.github.io/posts/PDE/</link>
      <pubDate>Sun, 02 Jul 2023 15:01:23 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/PDE/</guid>
      <description>I enlisted for the free Get Certified Program from Google in June 2023 which offers a 10 week curated curriculum to prepare for the Data Engineer exam. Unfortunately the content is restricted and cannot be shared. Following article should summarize all additional sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.</description>
    </item>
    
    <item>
      <title>What are (Large) Language Models?</title>
      <link>https://patrickpt.github.io/posts/language_models/</link>
      <pubDate>Tue, 02 May 2023 13:42:28 +0200</pubDate>
      
      <guid>https://patrickpt.github.io/posts/language_models/</guid>
      <description>TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?</description>
    </item>
    
    <item>
      <title>How to use a Reinforment Learning Agent to play Super Mario</title>
      <link>https://patrickpt.github.io/posts/super_mario/</link>
      <pubDate>Fri, 31 Mar 2023 11:58:22 +0200</pubDate>
      
      <guid>https://patrickpt.github.io/posts/super_mario/</guid>
      <description>TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.
Train your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.</description>
    </item>
    
    <item>
      <title>Road to GCP Professional Machine Learning Engineer</title>
      <link>https://patrickpt.github.io/posts/GCPMLE/</link>
      <pubDate>Fri, 24 Feb 2023 21:01:23 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/GCPMLE/</guid>
      <description>Today i passed the Google Professional Machine Learning Engineer Exam in a onsite test center in Duesseldorf. I prepared for it 4 weeks with different ressources. I have multiple years experience on Data Science and ML and about one year experience with ML on GCP. Following article should summarize all sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.</description>
    </item>
    
    <item>
      <title>Creating slides with MARP</title>
      <link>https://patrickpt.github.io/posts/marp/</link>
      <pubDate>Tue, 31 Jan 2023 09:06:20 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/marp/</guid>
      <description>Storytelling and presentation are the keys to a succesful adoption of your ideas. While storytelling is often verbal, a presentation is what stays and often is the thing that decideds whether your stakeholders pay attention or not so much. So it has to be aesthetic, clear and simple.
I show a lot of ideas and often i tend to recreate stuff from my code documentation enriched with diagrams. It is often a tedious work, recreating things in Powerpoint.</description>
    </item>
    
    <item>
      <title>Latent Diffusion Models: What is all the fuzzÂ about?</title>
      <link>https://patrickpt.github.io/posts/latent-diffusion-models/</link>
      <pubDate>Fri, 20 Jan 2023 14:50:18 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/latent-diffusion-models/</guid>
      <description>TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.
The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.
Illustrations by Jay Alammar
Sohl-Dickstein et al., 2015</description>
    </item>
    
    <item>
      <title>Hands on with Latent Diffusion Models</title>
      <link>https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/</link>
      <pubDate>Fri, 13 Jan 2023 09:28:09 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/</guid>
      <description>Prerequitises
To test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:
Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.</description>
    </item>
    
    <item>
      <title>A short summary on ChatGPT</title>
      <link>https://patrickpt.github.io/posts/summary-chatgpt/</link>
      <pubDate>Thu, 12 Jan 2023 22:34:01 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/summary-chatgpt/</guid>
      <description>This summary is based on another post: An introduction to ChatGPT written by ChatGPT
While testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous. In this post i tried to condense the meaningful information from the other post.
2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content.</description>
    </item>
    
    <item>
      <title>An introduction to ChatGPT written by ChatGPT</title>
      <link>https://patrickpt.github.io/posts/chatgpt-on-chatgpt/</link>
      <pubDate>Sat, 07 Jan 2023 00:55:07 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/chatgpt-on-chatgpt/</guid>
      <description>ChatGPT: Optimizing Language Models for Dialogue
What to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.
What is ChatGPT? Are you tired of boring, robotic chatbots that canâ€™t hold a conversation or understand your needs?</description>
    </item>
    
    <item>
      <title>Why and how do you create your own AI blog?</title>
      <link>https://patrickpt.github.io/posts/how-to-blog/</link>
      <pubDate>Sun, 01 Jan 2023 09:00:00 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/how-to-blog/</guid>
      <description>Welcome to my blog.
Why not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.
Isn&amp;rsquo;t it actually dumb to create your own blog instead of using these possibilities?
Maybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.</description>
    </item>
    
  </channel>
</rss>
