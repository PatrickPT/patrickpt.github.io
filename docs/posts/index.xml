<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Notes on AI</title>
    <link>https://patrickpt.github.io/posts/</link>
    <description>Recent content in Posts on Notes on AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 21 Jun 2022 15:06:45 -0500</lastBuildDate><atom:link href="https://patrickpt.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Summary: LLM Quantization with llama.cpp</title>
      <link>https://patrickpt.github.io/posts/quantllm/</link>
      <pubDate>Sun, 28 Jan 2024 09:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/quantllm/</guid>
      <description>Introduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&amp;rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.
Understanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels. This is akin to reducing the bit depth in an audio file, where instead of infinite gradations, you have a limited number of steps.</description>
    </item>
    
    <item>
      <title>What is Parameter Efficient Finetuning?</title>
      <link>https://patrickpt.github.io/posts/peft/</link>
      <pubDate>Wed, 01 Nov 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/peft/</guid>
      <description>TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.</description>
    </item>
    
    <item>
      <title>Hands on with Retrieval Augmented Generation</title>
      <link>https://patrickpt.github.io/posts/hands-on-rag/</link>
      <pubDate>Sat, 07 Oct 2023 10:30:00 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/hands-on-rag/</guid>
      <description>TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&amp;rsquo;s with long context windows.</description>
    </item>
    
    <item>
      <title>What is Retrieval Augmented Generation?</title>
      <link>https://patrickpt.github.io/posts/rag_intro/</link>
      <pubDate>Fri, 29 Sep 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/rag_intro/</guid>
      <description>TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.
Why should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.
picture from Neo4J
But fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:</description>
    </item>
    
    <item>
      <title>Generalized Models vs Specialized Models</title>
      <link>https://patrickpt.github.io/posts/generalvsspecializedmodels/</link>
      <pubDate>Tue, 05 Sep 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/generalvsspecializedmodels/</guid>
      <description>TL;DR This blogpost focusses on ML Design and Architecture and tries to give some intuition and hints for deciding between one generalized and multiple specialized models for the same business requriement and dataset.
Consider it a nudge to dive deeper into the topic
Why should i care? Transparency in machine learning is crucial for business stakeholders because it fosters trust and informed decision-making. Business Stakeholders need to understand not only the potential benefits but also the limitations and risks associated with machine learning models.</description>
    </item>
    
    <item>
      <title>What are Transformers?</title>
      <link>https://patrickpt.github.io/posts/transformers/</link>
      <pubDate>Wed, 30 Aug 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/transformers/</guid>
      <description>What are Transformers Transformers are a type of neural network architecture that was introduced in the paper &amp;ldquo;Attention is All You Need&amp;rdquo; by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification. Furthermore it is the foundation for Language Models and their application.
The key innovation of the Transformer architecture is the use of attention mechanisms.</description>
    </item>
    
    <item>
      <title>Road to GCP Professional Data Engineer</title>
      <link>https://patrickpt.github.io/posts/PDE/</link>
      <pubDate>Sun, 02 Jul 2023 15:01:23 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/PDE/</guid>
      <description>I enlisted for the free Get Certified Program from Google in June 2023 which offers a 10 week curated curriculum to prepare for the Data Engineer exam. Unfortunately the content is restricted and cannot be shared. Following article should summarize all additional sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.</description>
    </item>
    
    <item>
      <title>What are (Large) Language Models?</title>
      <link>https://patrickpt.github.io/posts/language_models/</link>
      <pubDate>Tue, 02 May 2023 13:42:28 +0200</pubDate>
      
      <guid>https://patrickpt.github.io/posts/language_models/</guid>
      <description>TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?</description>
    </item>
    
    <item>
      <title>How to use a Reinforment Learning Agent to play Super Mario</title>
      <link>https://patrickpt.github.io/posts/super_mario/</link>
      <pubDate>Fri, 31 Mar 2023 11:58:22 +0200</pubDate>
      
      <guid>https://patrickpt.github.io/posts/super_mario/</guid>
      <description>TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.
Train your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.</description>
    </item>
    
    <item>
      <title>Road to GCP Professional Machine Learning Engineer</title>
      <link>https://patrickpt.github.io/posts/GCPMLE/</link>
      <pubDate>Fri, 24 Feb 2023 21:01:23 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/GCPMLE/</guid>
      <description>Today i passed the Google Professional Machine Learning Engineer Exam in a onsite test center in Duesseldorf. I prepared for it 4 weeks with different ressources. I have multiple years experience on Data Science and ML and about one year experience with ML on GCP. Following article should summarize all sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.</description>
    </item>
    
    <item>
      <title>Creating slides with MARP</title>
      <link>https://patrickpt.github.io/posts/marp/</link>
      <pubDate>Tue, 31 Jan 2023 09:06:20 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/marp/</guid>
      <description>Storytelling and presentation are the keys to a succesful adoption of your ideas. While storytelling is often verbal, a presentation is what stays and often is the thing that decideds whether your stakeholders pay attention or not so much. So it has to be aesthetic, clear and simple.
I show a lot of ideas and often i tend to recreate stuff from my code documentation enriched with diagrams. It is often a tedious work, recreating things in Powerpoint.</description>
    </item>
    
    <item>
      <title>Latent Diffusion Models: What is all the fuzzÂ about?</title>
      <link>https://patrickpt.github.io/posts/latent-diffusion-models/</link>
      <pubDate>Fri, 20 Jan 2023 14:50:18 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/latent-diffusion-models/</guid>
      <description>TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.
The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.
Illustrations by Jay Alammar
Sohl-Dickstein et al., 2015</description>
    </item>
    
    <item>
      <title>Hands on with Latent Diffusion Models</title>
      <link>https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/</link>
      <pubDate>Fri, 13 Jan 2023 09:28:09 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/</guid>
      <description>Prerequitises
To test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:
Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.</description>
    </item>
    
    <item>
      <title>A short summary on ChatGPT</title>
      <link>https://patrickpt.github.io/posts/summary-chatgpt/</link>
      <pubDate>Thu, 12 Jan 2023 22:34:01 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/summary-chatgpt/</guid>
      <description>This summary is based on another post: An introduction to ChatGPT written by ChatGPT
While testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous. In this post i tried to condense the meaningful information from the other post.
2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content.</description>
    </item>
    
    <item>
      <title>An introduction to ChatGPT written by ChatGPT</title>
      <link>https://patrickpt.github.io/posts/chatgpt-on-chatgpt/</link>
      <pubDate>Sat, 07 Jan 2023 00:55:07 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/chatgpt-on-chatgpt/</guid>
      <description>ChatGPT: Optimizing Language Models for Dialogue
What to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.
What is ChatGPT? Are you tired of boring, robotic chatbots that canât hold a conversation or understand your needs?</description>
    </item>
    
    <item>
      <title>Why and how do you create your own AI blog?</title>
      <link>https://patrickpt.github.io/posts/how-to-blog/</link>
      <pubDate>Sun, 01 Jan 2023 09:00:00 +0100</pubDate>
      
      <guid>https://patrickpt.github.io/posts/how-to-blog/</guid>
      <description>Welcome to my blog.
Why not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.
Isn&amp;rsquo;t it actually dumb to create your own blog instead of using these possibilities?
Maybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.</description>
    </item>
    
  </channel>
</rss>
