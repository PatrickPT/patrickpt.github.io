<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Hands on with (Large) Language Models | Notes on AI</title>
<meta name="keywords" content="LLM, Hands On">
<meta name="description" content="TL;DR A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/language_models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Hands on with (Large) Language Models" />
<meta property="og:description" content="TL;DR A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/language_models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-02T13:42:28+02:00" />
<meta property="article:modified_time" content="2023-04-02T13:42:28+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hands on with (Large) Language Models"/>
<meta name="twitter:description" content="TL;DR A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Hands on with (Large) Language Models",
      "item": "https://patrickpt.github.io/posts/language_models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hands on with (Large) Language Models",
  "name": "Hands on with (Large) Language Models",
  "description": "TL;DR A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?",
  "keywords": [
    "LLM", "Hands On"
  ],
  "articleBody": "TL;DR A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?\nLarge language models rely basically on neural networks – more specifically, deep neural networks – to process natural language and can be trained on large amounts of text data, such as books, articles, and social media posts. They parse through the data and generate predictions about what words will come next in a sentence or phrase.\nThe larger the dataset, the more information that can be fed into the network for analysis. This increases the performance since the model has more data to evaluate against.\nOne of the key techniques used in large language models is called “pretraining.” This involves training the model on a large corpus of text data using a technique called unsupervised learning, where the model tries to learn the underlying structure of the language without any explicit supervision. This pretraining step helps the model to learn a general understanding of the language, which can then be fine-tuned for specific tasks like language translation or text generation.\nAnother important concept in large language models is “attention,” which refers to the ability of the model to focus on specific parts of the input sequence when making predictions. Attention mechanisms allow the model to selectively weight the importance of different parts of the input sequence based on their relevance to the task at hand.\nArchitectures There are several architectures used for Large Language Models (LLMs), but some of the most popular ones are:\nRecurrent Neural Networks (RNNs) RNNs are a type of neural network that is commonly used for processing sequences, including natural language. They work by processing each input element (in this case, each word in a sentence) in a sequence, and updating their internal state based on the previous inputs. This makes them well-suited for language modeling tasks, where the model needs to keep track of the context of the sentence to make predictions. Convolutional Neural Networks (CNNs): CNNs are another type of neural network that have been used for LLMs. They are typically used for image recognition tasks, but they can also be used for language processing tasks by treating the words in a sentence as a 1-dimensional signal. In this case, the convolution operation can help the model to identify important features of the sentence, such as n-grams or phrases. Transformer Models: Transformer models are a more recent architecture that has been widely used for LLMs. They are based on a self-attention mechanism that allows the model to selectively attend to different parts of the input sequence. This makes them highly effective for modeling long-range dependencies in text data, and they have been used to achieve state-of-the-art results in a wide range of language processing tasks. GPT (Generative Pre-trained Transformer) Models: GPT models are a type of transformer model that are pre-trained on large amounts of text data using unsupervised learning. They have been used for a wide range of natural language processing tasks, including language translation, text summarization, and text generation. The GPT series has evolved over time with each version improving on the previous in terms of size and performance. BERT (Bidirectional Encoder Representations from Transformers) Models: BERT models are also based on transformer architecture but are trained using a different pre-training objective. They are trained to understand the context of words based on their surrounding words, both before and after, unlike traditional LLMs which are only trained to consider context before the word. This makes BERT models particularly useful for tasks such as question answering and sentiment analysis. Each of these architectures has its own strengths and weaknesses, and the choice of architecture depends on the specific task and the nature of the data being processed. However, in recent years, transformer-based models such as GPT and BERT have emerged as the most effective architectures for large language modeling tasks.\nWhy the Hype The statistical methods behind LLM’s are rather simple. The recent innovation comes from investing a huge amount of money and a clever strategy to finetune. Reinforcement Learning on Human Feedback.\nReinforcement Learning with Human Feedback (RLHF) is a subfield of reinforcement learning (RL) that aims to incorporate human feedback into the training of RL agents. In RLHF, humans provide feedback to the RL agent during the training process to help guide its behavior.\nThe basic idea behind RLHF is to leverage the unique strengths of both humans and machines. While machines are capable of processing large amounts of data quickly and making decisions based on objective criteria, humans are better at handling uncertainty, understanding complex contexts, and incorporating ethical considerations into decision-making.\nThere are several approaches to incorporating human feedback into RL training. One common approach is called “reward shaping,” where humans provide additional reward signals to the agent to help guide its behavior. For example, if an agent is learning to play a game, a human might provide additional rewards for certain actions that the agent is not considering on its own.\nAnother approach is called “active learning,” where the agent actively seeks out feedback from humans to improve its performance. For example, the agent might ask a human to provide feedback on a specific decision it is considering, or it might ask for help in identifying certain features of the environment that are important for making decisions.\nOverall, RLHF has the potential to make RL agents more effective in real-world applications, where the agent must interact with humans or operate in complex, uncertain environments. However, it also raises important ethical considerations, such as how to ensure that the human feedback is fair and unbiased, and how to handle situations where the human feedback is conflicting or inconsistent.\nHands On As said it is rather simple to set up your own Language Model and that is what we are going to do.\nWe will use the nanoGPT implementation of Andrej Karpathy on GitHub](https://github.com/karpathy/nanoGPT) to test out our own model.\nHow to train your own Language Model WORK IN PROGRESS\nCorpus Training Results Ressources Large Language Models\nChatGPT\nnanoGPT\n",
  "wordCount" : "1073",
  "inLanguage": "en",
  "datePublished": "2023-04-02T13:42:28+02:00",
  "dateModified": "2023-04-02T13:42:28+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/language_models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes on AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="Notes on AI (Alt + H)">Notes on AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/recent/" title="Recent">
                    <span>Recent</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Hands on with (Large) Language Models
    </h1>
    <div class="post-meta"><span title='2023-04-02 13:42:28 +0200 CEST'>April 2, 2023</span>&nbsp;·&nbsp;6 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#what-are-language-models" aria-label="What are Language Models">What are Language Models</a></li>
                <li>
                    <a href="#architectures" aria-label="Architectures">Architectures</a></li>
                <li>
                    <a href="#why-the-hype" aria-label="Why the Hype">Why the Hype</a></li>
                <li>
                    <a href="#hands-on" aria-label="Hands On">Hands On</a><ul>
                        
                <li>
                    <a href="#how-to-train-your-own-language-model" aria-label="How to train your own Language Model">How to train your own Language Model</a></li>
                <li>
                    <a href="#corpus" aria-label="Corpus">Corpus</a></li>
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li></ul>
                </li>
                <li>
                    <a href="#ressources" aria-label="Ressources">Ressources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>A short Hands On with (Large) Language Models: What are the ideas behind Language Models and how can you train your own with nanoGPT?</p>
<h1 id="what-are-language-models">What are Language Models<a hidden class="anchor" aria-hidden="true" href="#what-are-language-models">#</a></h1>
<p>Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML.
But what is the foundation for the products that could impact our future world significantly?</p>
<p>Large language models rely basically on neural networks – more specifically, deep neural networks – to process natural language and can be trained on large amounts of text data, such as books, articles, and social media posts. They parse through the data and generate predictions about what words will come next in a sentence or phrase.</p>
<p>The larger the dataset, the more information that can be fed into the network for analysis. This increases the performance since the model has more data to evaluate against.</p>
<p>One of the key techniques used in large language models is called &ldquo;pretraining.&rdquo; This involves training the model on a large corpus of text data using a technique called unsupervised learning, where the model tries to learn the underlying structure of the language without any explicit supervision. This pretraining step helps the model to learn a general understanding of the language, which can then be fine-tuned for specific tasks like language translation or text generation.</p>
<p>Another important concept in large language models is &ldquo;attention,&rdquo; which refers to the ability of the model to focus on specific parts of the input sequence when making predictions. Attention mechanisms allow the model to selectively weight the importance of different parts of the input sequence based on their relevance to the task at hand.</p>
<h1 id="architectures">Architectures<a hidden class="anchor" aria-hidden="true" href="#architectures">#</a></h1>
<p>There are several architectures used for Large Language Models (LLMs), but some of the most popular ones are:</p>
<ul>
<li><strong>Recurrent Neural Networks (RNNs)</strong> RNNs are a type of neural network that is commonly used for processing sequences, including natural language. They work by processing each input element (in this case, each word in a sentence) in a sequence, and updating their internal state based on the previous inputs. This makes them well-suited for language modeling tasks, where the model needs to keep track of the context of the sentence to make predictions.</li>
<li><strong>Convolutional Neural Networks (CNNs)</strong>: CNNs are another type of neural network that have been used for LLMs. They are typically used for image recognition tasks, but they can also be used for language processing tasks by treating the words in a sentence as a 1-dimensional signal. In this case, the convolution operation can help the model to identify important features of the sentence, such as n-grams or phrases.</li>
<li><strong>Transformer Models</strong>: Transformer models are a more recent architecture that has been widely used for LLMs. They are based on a self-attention mechanism that allows the model to selectively attend to different parts of the input sequence. This makes them highly effective for modeling long-range dependencies in text data, and they have been used to achieve state-of-the-art results in a wide range of language processing tasks.</li>
<li><strong>GPT (Generative Pre-trained Transformer) Models</strong>: GPT models are a type of transformer model that are pre-trained on large amounts of text data using unsupervised learning. They have been used for a wide range of natural language processing tasks, including language translation, text summarization, and text generation. The GPT series has evolved over time with each version improving on the previous in terms of size and performance.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers) Models</strong>: BERT models are also based on transformer architecture but are trained using a different pre-training objective. They are trained to understand the context of words based on their surrounding words, both before and after, unlike traditional LLMs which are only trained to consider context before the word. This makes BERT models particularly useful for tasks such as question answering and sentiment analysis.</li>
</ul>
<p>Each of these architectures has its own strengths and weaknesses, and the choice of architecture depends on the specific task and the nature of the data being processed. However, in recent years, transformer-based models such as GPT and BERT have emerged as the most effective architectures for large language modeling tasks.</p>
<h1 id="why-the-hype">Why the Hype<a hidden class="anchor" aria-hidden="true" href="#why-the-hype">#</a></h1>
<p>The statistical methods behind LLM&rsquo;s are rather simple. The recent innovation comes from investing a huge amount of money and a clever strategy to finetune. Reinforcement Learning on Human Feedback.</p>
<p>Reinforcement Learning with Human Feedback (RLHF) is a subfield of reinforcement learning (RL) that aims to incorporate human feedback into the training of RL agents. In RLHF, humans provide feedback to the RL agent during the training process to help guide its behavior.</p>
<p>The basic idea behind RLHF is to leverage the unique strengths of both humans and machines. While machines are capable of processing large amounts of data quickly and making decisions based on objective criteria, humans are better at handling uncertainty, understanding complex contexts, and incorporating ethical considerations into decision-making.</p>
<p>There are several approaches to incorporating human feedback into RL training. One common approach is called &ldquo;reward shaping,&rdquo; where humans provide additional reward signals to the agent to help guide its behavior. For example, if an agent is learning to play a game, a human might provide additional rewards for certain actions that the agent is not considering on its own.</p>
<p>Another approach is called &ldquo;active learning,&rdquo; where the agent actively seeks out feedback from humans to improve its performance. For example, the agent might ask a human to provide feedback on a specific decision it is considering, or it might ask for help in identifying certain features of the environment that are important for making decisions.</p>
<p>Overall, RLHF has the potential to make RL agents more effective in real-world applications, where the agent must interact with humans or operate in complex, uncertain environments. However, it also raises important ethical considerations, such as how to ensure that the human feedback is fair and unbiased, and how to handle situations where the human feedback is conflicting or inconsistent.</p>
<h1 id="hands-on">Hands On<a hidden class="anchor" aria-hidden="true" href="#hands-on">#</a></h1>
<p>As said it is rather simple to set up your own Language Model and that is what we are going to do.</p>
<p>We will use the nanoGPT implementation of Andrej Karpathy on GitHub](<a href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a>) to test out our own model.</p>
<h2 id="how-to-train-your-own-language-model">How to train your own Language Model<a hidden class="anchor" aria-hidden="true" href="#how-to-train-your-own-language-model">#</a></h2>
<p><em>WORK IN PROGRESS</em></p>
<h2 id="corpus">Corpus<a hidden class="anchor" aria-hidden="true" href="#corpus">#</a></h2>
<h2 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h2>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<h1 id="ressources">Ressources<a hidden class="anchor" aria-hidden="true" href="#ressources">#</a></h1>
<p><a href="https://huggingface.co/blog/large-language-models">Large Language Models</a></p>
<p><a href="https://chat.openai.com/chat">ChatGPT</a></p>
<p><a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://patrickpt.github.io/tags/hands-on/">Hands On</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on twitter"
        href="https://twitter.com/intent/tweet/?text=Hands%20on%20with%20%28Large%29%20Language%20Models&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f&amp;hashtags=LLM%2cHandsOn">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f&amp;title=Hands%20on%20with%20%28Large%29%20Language%20Models&amp;summary=Hands%20on%20with%20%28Large%29%20Language%20Models&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f&title=Hands%20on%20with%20%28Large%29%20Language%20Models">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on whatsapp"
        href="https://api.whatsapp.com/send?text=Hands%20on%20with%20%28Large%29%20Language%20Models%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with (Large) Language Models on telegram"
        href="https://telegram.me/share/url?text=Hands%20on%20with%20%28Large%29%20Language%20Models&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flanguage_models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://patrickpt.github.io/">Notes on AI</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
