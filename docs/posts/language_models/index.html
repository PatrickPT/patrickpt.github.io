<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What are (Large) Language Models? | patrickschnass.de</title>
<meta name="keywords" content="LLM">
<meta name="description" content="TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?">
<meta name="author" content="">
<link rel="canonical" href="https://patrickschnass.de/posts/language_models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickschnass.de/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickschnass.de/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickschnass.de/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickschnass.de/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickschnass.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="What are (Large) Language Models?" />
<meta property="og:description" content="TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickschnass.de/posts/language_models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-02T13:42:28+02:00" />
<meta property="article:modified_time" content="2023-05-02T13:42:28+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What are (Large) Language Models?"/>
<meta name="twitter:description" content="TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?
What are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickschnass.de/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What are (Large) Language Models?",
      "item": "https://patrickschnass.de/posts/language_models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What are (Large) Language Models?",
  "name": "What are (Large) Language Models?",
  "description": "TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?",
  "keywords": [
    "LLM"
  ],
  "articleBody": "TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?\nLarge language models rely basically on neural networks – more specifically, deep neural networks – to process natural language and can be trained on large amounts of text data, such as books, articles, and social media posts. They parse through the data and generate predictions about what words will come next in a sentence or phrase.\nThe larger the dataset, the more information that can be fed into the network for analysis. This increases the performance since the model has more data to evaluate against.\nOne of the key techniques used in large language models is called “pretraining.” This involves training the model on a large corpus of text data using a technique called unsupervised learning, where the model tries to learn the underlying structure of the language without any explicit supervision. This pretraining step helps the model to learn a general understanding of the language, which can then be fine-tuned for specific tasks like language translation or text generation.\nAnother important concept in large language models is “attention,” which refers to the ability of the model to focus on specific parts of the input sequence when making predictions. Attention mechanisms allow the model to selectively weight the importance of different parts of the input sequence based on their relevance to the task at hand.\nArchitectures There are several architectures used for Large Language Models (LLMs), but some of the most popular ones are:\nRecurrent Neural Networks (RNNs) RNNs are a type of neural network that is commonly used for processing sequences, including natural language. They work by processing each input element (in this case, each word in a sentence) in a sequence, and updating their internal state based on the previous inputs. This makes them well-suited for language modeling tasks, where the model needs to keep track of the context of the sentence to make predictions. Convolutional Neural Networks (CNNs): CNNs are another type of neural network that have been used for LLMs. They are typically used for image recognition tasks, but they can also be used for language processing tasks by treating the words in a sentence as a 1-dimensional signal. In this case, the convolution operation can help the model to identify important features of the sentence, such as n-grams or phrases. Transformer Models: Transformer models are a more recent architecture that has been widely used for LLMs. They are based on a self-attention mechanism that allows the model to selectively attend to different parts of the input sequence. This makes them highly effective for modeling long-range dependencies in text data, and they have been used to achieve state-of-the-art results in a wide range of language processing tasks. GPT (Generative Pre-trained Transformer) Models: GPT models are a type of transformer model that are pre-trained on large amounts of text data using unsupervised learning. They have been used for a wide range of natural language processing tasks, including language translation, text summarization, and text generation. The GPT series has evolved over time with each version improving on the previous in terms of size and performance. BERT (Bidirectional Encoder Representations from Transformers) Models: BERT models are also based on transformer architecture but are trained using a different pre-training objective. They are trained to understand the context of words based on their surrounding words, both before and after, unlike traditional LLMs which are only trained to consider context before the word. This makes BERT models particularly useful for tasks such as question answering and sentiment analysis. Each of these architectures has its own strengths and weaknesses, and the choice of architecture depends on the specific task and the nature of the data being processed. However, in recent years, transformer-based models such as GPT and BERT have emerged as the most effective architectures for large language modeling tasks.\nWhy the Hype The statistical methods behind LLM’s are rather simple. The recent innovation comes from investing a huge amount of money and a clever strategy to finetune. Reinforcement Learning on Human Feedback.\nReinforcement Learning with Human Feedback (RLHF) is a subfield of reinforcement learning (RL) that aims to incorporate human feedback into the training of RL agents. In RLHF, humans provide feedback to the RL agent during the training process to help guide its behavior.\nThe basic idea behind RLHF is to leverage the unique strengths of both humans and machines. While machines are capable of processing large amounts of data quickly and making decisions based on objective criteria, humans are better at handling uncertainty, understanding complex contexts, and incorporating ethical considerations into decision-making.\nThere are several approaches to incorporating human feedback into RL training. One common approach is called “reward shaping,” where humans provide additional reward signals to the agent to help guide its behavior. For example, if an agent is learning to play a game, a human might provide additional rewards for certain actions that the agent is not considering on its own.\nAnother approach is called “active learning,” where the agent actively seeks out feedback from humans to improve its performance. For example, the agent might ask a human to provide feedback on a specific decision it is considering, or it might ask for help in identifying certain features of the environment that are important for making decisions.\nOverall, RLHF has the potential to make RL agents more effective in real-world applications, where the agent must interact with humans or operate in complex, uncertain environments. However, it also raises important ethical considerations, such as how to ensure that the human feedback is fair and unbiased, and how to handle situations where the human feedback is conflicting or inconsistent.\nHands On As said it is rather simple to set up your own Language Model and if you are eager to do it you can use the nanoGPT implementation of Andrej Karpathy on GitHub](https://github.com/karpathy/nanoGPT) to test out your own model.\nRessources Large Language Models\nChatGPT\nnanoGPT\n",
  "wordCount" : "1051",
  "inLanguage": "en",
  "datePublished": "2023-05-02T13:42:28+02:00",
  "dateModified": "2023-05-02T13:42:28+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickschnass.de/posts/language_models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickschnass.de",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickschnass.de/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickschnass.de/" accesskey="h" title="patrickschnass.de (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickschnass.de</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickschnass.de/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickschnass.de/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickschnass.de/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      What are (Large) Language Models?
    </h1>
    <div class="post-meta"><span title='2023-05-02 13:42:28 +0200 CEST'>May 2, 2023</span>&nbsp;·&nbsp;5 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#what-are-language-models" aria-label="What are Language Models">What are Language Models</a></li>
                <li>
                    <a href="#architectures" aria-label="Architectures">Architectures</a></li>
                <li>
                    <a href="#why-the-hype" aria-label="Why the Hype">Why the Hype</a></li>
                <li>
                    <a href="#hands-on" aria-label="Hands On">Hands On</a></li>
                <li>
                    <a href="#ressources" aria-label="Ressources">Ressources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?</p>
<h1 id="what-are-language-models">What are Language Models<a hidden class="anchor" aria-hidden="true" href="#what-are-language-models">#</a></h1>
<p>Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML.
But what is the foundation for the products that could impact our future world significantly?</p>
<p>Large language models rely basically on neural networks – more specifically, deep neural networks – to process natural language and can be trained on large amounts of text data, such as books, articles, and social media posts. They parse through the data and generate predictions about what words will come next in a sentence or phrase.</p>
<p>The larger the dataset, the more information that can be fed into the network for analysis. This increases the performance since the model has more data to evaluate against.</p>
<p>One of the key techniques used in large language models is called &ldquo;pretraining.&rdquo; This involves training the model on a large corpus of text data using a technique called unsupervised learning, where the model tries to learn the underlying structure of the language without any explicit supervision. This pretraining step helps the model to learn a general understanding of the language, which can then be fine-tuned for specific tasks like language translation or text generation.</p>
<p>Another important concept in large language models is &ldquo;attention,&rdquo; which refers to the ability of the model to focus on specific parts of the input sequence when making predictions. Attention mechanisms allow the model to selectively weight the importance of different parts of the input sequence based on their relevance to the task at hand.</p>
<h1 id="architectures">Architectures<a hidden class="anchor" aria-hidden="true" href="#architectures">#</a></h1>
<p>There are several architectures used for Large Language Models (LLMs), but some of the most popular ones are:</p>
<ul>
<li><strong>Recurrent Neural Networks (RNNs)</strong> RNNs are a type of neural network that is commonly used for processing sequences, including natural language. They work by processing each input element (in this case, each word in a sentence) in a sequence, and updating their internal state based on the previous inputs. This makes them well-suited for language modeling tasks, where the model needs to keep track of the context of the sentence to make predictions.</li>
<li><strong>Convolutional Neural Networks (CNNs)</strong>: CNNs are another type of neural network that have been used for LLMs. They are typically used for image recognition tasks, but they can also be used for language processing tasks by treating the words in a sentence as a 1-dimensional signal. In this case, the convolution operation can help the model to identify important features of the sentence, such as n-grams or phrases.</li>
<li><strong>Transformer Models</strong>: Transformer models are a more recent architecture that has been widely used for LLMs. They are based on a self-attention mechanism that allows the model to selectively attend to different parts of the input sequence. This makes them highly effective for modeling long-range dependencies in text data, and they have been used to achieve state-of-the-art results in a wide range of language processing tasks.</li>
<li><strong>GPT (Generative Pre-trained Transformer) Models</strong>: GPT models are a type of transformer model that are pre-trained on large amounts of text data using unsupervised learning. They have been used for a wide range of natural language processing tasks, including language translation, text summarization, and text generation. The GPT series has evolved over time with each version improving on the previous in terms of size and performance.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers) Models</strong>: BERT models are also based on transformer architecture but are trained using a different pre-training objective. They are trained to understand the context of words based on their surrounding words, both before and after, unlike traditional LLMs which are only trained to consider context before the word. This makes BERT models particularly useful for tasks such as question answering and sentiment analysis.</li>
</ul>
<p>Each of these architectures has its own strengths and weaknesses, and the choice of architecture depends on the specific task and the nature of the data being processed. However, in recent years, transformer-based models such as GPT and BERT have emerged as the most effective architectures for large language modeling tasks.</p>
<h1 id="why-the-hype">Why the Hype<a hidden class="anchor" aria-hidden="true" href="#why-the-hype">#</a></h1>
<p>The statistical methods behind LLM&rsquo;s are rather simple. The recent innovation comes from investing a huge amount of money and a clever strategy to finetune. Reinforcement Learning on Human Feedback.</p>
<p>Reinforcement Learning with Human Feedback (RLHF) is a subfield of reinforcement learning (RL) that aims to incorporate human feedback into the training of RL agents. In RLHF, humans provide feedback to the RL agent during the training process to help guide its behavior.</p>
<p>The basic idea behind RLHF is to leverage the unique strengths of both humans and machines. While machines are capable of processing large amounts of data quickly and making decisions based on objective criteria, humans are better at handling uncertainty, understanding complex contexts, and incorporating ethical considerations into decision-making.</p>
<p>There are several approaches to incorporating human feedback into RL training. One common approach is called &ldquo;reward shaping,&rdquo; where humans provide additional reward signals to the agent to help guide its behavior. For example, if an agent is learning to play a game, a human might provide additional rewards for certain actions that the agent is not considering on its own.</p>
<p>Another approach is called &ldquo;active learning,&rdquo; where the agent actively seeks out feedback from humans to improve its performance. For example, the agent might ask a human to provide feedback on a specific decision it is considering, or it might ask for help in identifying certain features of the environment that are important for making decisions.</p>
<p>Overall, RLHF has the potential to make RL agents more effective in real-world applications, where the agent must interact with humans or operate in complex, uncertain environments. However, it also raises important ethical considerations, such as how to ensure that the human feedback is fair and unbiased, and how to handle situations where the human feedback is conflicting or inconsistent.</p>
<h1 id="hands-on">Hands On<a hidden class="anchor" aria-hidden="true" href="#hands-on">#</a></h1>
<p>As said it is rather simple to set up your own Language Model and if you are eager to do it you can use the nanoGPT implementation of Andrej Karpathy on GitHub](<a href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a>) to test out your own model.</p>
<h1 id="ressources">Ressources<a hidden class="anchor" aria-hidden="true" href="#ressources">#</a></h1>
<p><a href="https://huggingface.co/blog/large-language-models">Large Language Models</a></p>
<p><a href="https://chat.openai.com/chat">ChatGPT</a></p>
<p><a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickschnass.de/tags/llm/">LLM</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on twitter"
        href="https://twitter.com/intent/tweet/?text=What%20are%20%28Large%29%20Language%20Models%3f&amp;url=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f&amp;hashtags=LLM">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f&amp;title=What%20are%20%28Large%29%20Language%20Models%3f&amp;summary=What%20are%20%28Large%29%20Language%20Models%3f&amp;source=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f&title=What%20are%20%28Large%29%20Language%20Models%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on whatsapp"
        href="https://api.whatsapp.com/send?text=What%20are%20%28Large%29%20Language%20Models%3f%20-%20https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What are (Large) Language Models? on telegram"
        href="https://telegram.me/share/url?text=What%20are%20%28Large%29%20Language%20Models%3f&amp;url=https%3a%2f%2fpatrickschnass.de%2fposts%2flanguage_models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://patrickschnass.de/">patrickschnass.de</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
