<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Hands on with Latent Diffusion Models | Notes on AI</title>
<meta name="keywords" content="DallE, stable-diffusion, latent-diffusion-models, NoCode, HuggingFace, HandsOn">
<meta name="description" content="Prerequitises
To test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:
Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Hands on with Latent Diffusion Models" />
<meta property="og:description" content="Prerequitises
To test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:
Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-13T09:28:09+00:00" />
<meta property="article:modified_time" content="2023-01-13T09:28:09+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hands on with Latent Diffusion Models"/>
<meta name="twitter:description" content="Prerequitises
To test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:
Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Hands on with Latent Diffusion Models",
      "item": "https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hands on with Latent Diffusion Models",
  "name": "Hands on with Latent Diffusion Models",
  "description": "Prerequitises\nTo test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:\nTools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.",
  "keywords": [
    "DallE", "stable-diffusion", "latent-diffusion-models", "NoCode", "HuggingFace", "HandsOn"
  ],
  "articleBody": "Prerequitises\nTo test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:\nTools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects. Recap on Latent Diffusion Models There are mutiple sites and blog posts which explain Latent Diffusion Models including my own Latent Diffusion Models: What is all the fuzz about?\nTo keep it a bit lightweight i can recommend one which explains everything with diagrams(Because i like diagrams for learning).\nBlogpost of Jay Alammar\nI don’t understand anything You don’t have any idea what this is all about?\nYou can generate beautiful pictures with the help of AI All you need to do is create a prompt and enter it into any tool using an algorithm like stable-diffusion which renders your image then. So\nThink of a prompt Examples with prompt search and Go to Dall-E Open an account and try it out. NoCode Quickstart You are not interested in getting your hands dirty? You don’t want to code? You just want to produce some nice looking images and test your prompt skills? You are not willing to pay a certain amount to use the capabilities of OpenAI’s Dall-E?\nThen this is for you:\nPrompt Ideas and References For starters, do you have any idea what you want to create and how to best create your initial prompt?\nYes\nAwesome, but as in Google Search: When you try to find the correct search prompt you need to tune the semantics of your thoughts to get what you want: How to write stable-diffusion prompts\nOf course AI can help you with this: Prompt Tuning\nNo\nNo worries, you are not the first one to create a prompt and there are already a lot of examples out there:\nExamples with prompt search\nAtlas on examples with topics\nUse an Endpoint with Stable Diffusion There are already a few websites giving you access to endpoints for free. I recommend to use one where you still have access to the codebase of the model and some evaluation. StabilityAI, the creators of stable-diffusion, an open source latent diffusion model host their model on Huggingface and give access to an endpoint (here called spaces) to test it out:\nStable Diffusion 2.1 Demo by Stability AI\nModel Card of Stable Diffusion v2\nExample Following prompt:\n“oil painting of a cat sitting on a rainbow”\nbecomes after finetuning:\n“oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha”\nand creates this picture with stable-diffusion: I like cats!(Like everyone else on the internet i guess)\nEnjoy exploring!\nIf you are interested in understanding how to create a Notebook with diffusors please see the following section.\nStable Diffusion …using Hugging Face’s diffusers\n*The following section focusses on inference and is based on Quickstart with diffusers and Intro on diffusers\nIf you want to get a more hands-on guide on training diffusion models, please have a look at Training with Diffusers\nSummary on Diffusion Models Diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\nThe underlying model, often a neural network, is trained to predict a way to slightly denoise the image in each step. After certain number of steps, a sample is obtained.\nThe diffusion process consists in taking random noise of the size of the desired output and pass it through the model several times. The process ends after a given number of steps, and the output image should represent a sample according to the training data distribution of the model, for instance an image of a cat.\nDuring training we show many samples of a given distribution, such as images of cat. After training, the model will be able to process random noise to generate similar cat images.\nWithout going in too much detail, the model is usually not trained to directly predict a slightly less noisy image, but rather to predict the “noise residual” which is the difference between a less noisy image and the input image (for a diffusion model called “DDPM”).\nTo do the denoising process, a specific noise scheduling algorithm is thus necessary and “wrap” the model to define how many diffusion steps are needed for inference as well as how to compute a less noisy image from the model’s output.\nSummary on diffusers Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nIt is created by the researchers and engineers from CompVis, Stability AI and LAION. It’s trained on 512x512 images from a subset of the LAION-5B database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs. See the model card for more information.\nHowever, most of the recent research on diffusion models, e.g. DALL-E 2 and Imagen, is unfortunately not accessible to the broader machine learning community and typically remains behind closed doors.\nHere comes Hugging Face’s library for diffusion model: diffusers with the goals to:\ngather recent diffusion models from independent repositories in a single and long-term maintained project that is built by and for the community, reproduce high impact machine learning systems such as DALLE and Imagen in a manner that is accessible for the public, and create an easy to use API that enables one to train their own models or re-use checkpoints from other repositories for inference. The core API of diffusers is divided into three components:\nPipelines: high-level classes designed to rapidly generate samples from popular trained diffusion models in a user-friendly fashion. Models: popular architectures for training new diffusion models, e.g. UNet. Schedulers: various techniques for generating images from noise during inference as well as to generate noisy images for training. How-to create an Image Install diffusers !pip install diffusers==0.11.0 !pip install transformers scipy ftfy accelerate !pip install \"ipywidgets\u003e=7,\u003c8\" !pip install safetensors Input your Hugging Face Token As mentioned earlier you need a token with huggingface to import the pretrained snapshots\nfrom huggingface_hub import notebook_login notebook_login() Pipeline StableDiffusionPipeline is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.\nFirst, we load the pre-trained weights of all components of the model. Here we use Stable Diffusion version 2.1 (stabilityai/stable-diffusion-2-1), but there are other variants that you may want to try:\nrunwayml/stable-diffusion-v1-5 stabilityai/stable-diffusion-2-1-base stabilityai/stable-diffusion-2-1. This version can produce images with a resolution of 768x768, while the others work at 512x512. This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.\nIn addition to the model id stabilityai/stable-diffusion-2-1, we’re also passing a specific torch_dtype to the from_pretrained method.\nThe weights are loaded from the half-precision branch fp16 and we need to tell diffusers to expect the weights in float16 precision by passing torch_dtype=torch.float16.\nWe can import the DDPMPipeline, which will allow you to do inference with a couple of lines of code. The from_pretrained() method allows downloading the model and its configuration from the Hugging Face Hub, a repository of over 60,000 models shared by the community.\nimport torch from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler model_id = \"stabilityai/stable-diffusion-2-1\" # Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16) pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) The pipe shows now all components contained in your desired process.\npipe StableDiffusionPipeline { \"_class_name\": \"StableDiffusionPipeline\", \"_diffusers_version\": \"0.11.0\", \"feature_extractor\": [ \"transformers\", \"CLIPImageProcessor\" ], \"requires_safety_checker\": false, \"safety_checker\": [ null, null ], \"scheduler\": [ \"diffusers\", \"DDIMScheduler\" ], \"text_encoder\": [ \"transformers\", \"CLIPTextModel\" ], \"tokenizer\": [ \"transformers\", \"CLIPTokenizer\" ], \"unet\": [ \"diffusers\", \"UNet2DConditionModel\" ], \"vae\": [ \"diffusers\", \"AutoencoderKL\" ] } Model Instances of the model class are neural networks that take a noisy sample as well as a timestep as inputs to predict a less noisy output sample.\nHere a simple UNet2DConditionModel which was released with the DDPM Paper is used.\npipe.unet Similarly to what we’ve seen for the pipeline class, we can load the model configuration and weights with one line, using the from_pretrained() method. It caches the model weights locally.\nScheduler Schedulers define the noise schedule which is used to add noise to the model during training, and also define the algorithm to compute the slightly less noisy sample given the model output (here noisy_residual).\nIt is important to stress here that while models have trainable weights, schedulers are usually parameter-free (in the sense they have no trainable weights) and simply define the algorithm to compute the slightly less noisy sample.\npipe.scheduler Generate Image To generate an image, we simply run the pipeline and don’t even need to give it any input, it will generate a random initial noise sample and then iterate the diffusion process. Here we use the inital prompt from above\nThe pipeline returns as output a dictionary with a generated sample of interest.\nprompt = \"oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha\" image = pipe(prompt).images[0] # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/) image.save(f\"rainbow_cat.png\") Et voila\nA good video on the topic combining intuition, code and a hands-on can be found on the Youtube Channel by Edan Meyer\nReferences Latent Diffusion Models: What is all the fuzz about?\nHugging Face\nBlogpost of Jay Alammar\nDall-E\nExamples with prompt search\nAtlas on examples with topics\nHow to write stable-diffusion prompts\nPrompt Tuning\nFurther Links What’s HuggingFace on Medium\n",
  "wordCount" : "1711",
  "inLanguage": "en",
  "datePublished": "2023-01-13T09:28:09Z",
  "dateModified": "2023-01-13T09:28:09Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/hands-on-latent-diffusion-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes on AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="Notes on AI (Alt + H)">Notes on AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Hands on with Latent Diffusion Models
    </h1>
    <div class="post-meta"><span title='2023-01-13 09:28:09 +0000 UTC'>January 13, 2023</span>&nbsp;·&nbsp;9 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#recap-on-latent-diffusion-models" aria-label="Recap on Latent Diffusion Models">Recap on Latent Diffusion Models</a><ul>
                        <ul>
                        
                <li>
                    <a href="#i-dont-understand-anything" aria-label="I don&amp;rsquo;t understand anything">I don&rsquo;t understand anything</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#nocode-quickstart" aria-label="NoCode Quickstart">NoCode Quickstart</a><ul>
                        
                <li>
                    <a href="#prompt-ideas-and-references" aria-label="Prompt Ideas and References">Prompt Ideas and References</a></li>
                <li>
                    <a href="#use-an-endpoint-with-stable-diffusion" aria-label="Use an Endpoint with Stable Diffusion">Use an Endpoint with Stable Diffusion</a></li>
                <li>
                    <a href="#example" aria-label="Example">Example</a></li></ul>
                </li>
                <li>
                    <a href="#stable-diffusion" aria-label="Stable Diffusion">Stable Diffusion</a><ul>
                        
                <li>
                    <a href="#summary-on-diffusion-models" aria-label="Summary on Diffusion Models">Summary on Diffusion Models</a></li>
                <li>
                    <a href="#summary-on-diffusers" aria-label="Summary on diffusers">Summary on diffusers</a></li>
                <li>
                    <a href="#how-to-create-an-image" aria-label="How-to create an Image">How-to create an Image</a><ul>
                        
                <li>
                    <a href="#install-diffusers" aria-label="Install diffusers">Install diffusers</a></li>
                <li>
                    <a href="#input-your-hugging-face-token" aria-label="Input your Hugging Face Token">Input your Hugging Face Token</a></li>
                <li>
                    <a href="#pipeline" aria-label="Pipeline">Pipeline</a></li>
                <li>
                    <a href="#model" aria-label="Model">Model</a></li>
                <li>
                    <a href="#scheduler" aria-label="Scheduler">Scheduler</a></li>
                <li>
                    <a href="#generate-image" aria-label="Generate Image">Generate Image</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a></li>
                <li>
                    <a href="#further-links" aria-label="Further Links">Further Links</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Prerequitises</strong></p>
<p>To test the models here you need to have an account with <a href="https://huggingface.co/">HuggingFace</a> - for loading the checkpoint or using the endpoints.
Hugging Face is a community and data science platform that provides:</p>
<ul>
<li>Tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies.</li>
<li>A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.</li>
</ul>
<h1 id="recap-on-latent-diffusion-models">Recap on Latent Diffusion Models<a hidden class="anchor" aria-hidden="true" href="#recap-on-latent-diffusion-models">#</a></h1>
<p>There are mutiple sites and blog posts which explain Latent Diffusion Models including my own <a href="/posts/latent-diffusion-models">Latent Diffusion Models: What is all the fuzz about?</a></p>
<p>To keep it a bit lightweight i can recommend one which explains everything with diagrams(Because i like diagrams for learning).</p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/">Blogpost of Jay Alammar</a></p>
<h3 id="i-dont-understand-anything">I don&rsquo;t understand anything<a hidden class="anchor" aria-hidden="true" href="#i-dont-understand-anything">#</a></h3>
<p>You don&rsquo;t have any idea what this is all about?</p>
<p><strong>You can generate beautiful pictures with the help of AI</strong>
All you need to do is create a prompt and enter it into any tool using an algorithm like stable-diffusion which renders your image then.
So</p>
<ol>
<li>Think of a prompt <a href="https://krea.ai//">Examples with prompt search</a> and</li>
<li>Go to <a href="https://openai.com/dall-e-2/">Dall-E</a></li>
<li>Open an account and try it out.</li>
</ol>
<h1 id="nocode-quickstart">NoCode Quickstart<a hidden class="anchor" aria-hidden="true" href="#nocode-quickstart">#</a></h1>
<p>You are not interested in getting your hands dirty?
You don&rsquo;t want to code?
You just want to produce some nice looking images and test your prompt skills?
You are not willing to pay a certain amount to use the capabilities of OpenAI&rsquo;s <a href="https://openai.com/dall-e-2/">Dall-E</a>?</p>
<p>Then this is for you:</p>
<h2 id="prompt-ideas-and-references">Prompt Ideas and References<a hidden class="anchor" aria-hidden="true" href="#prompt-ideas-and-references">#</a></h2>
<p>For starters, do you have any idea what you want to create and how to best create your initial prompt?</p>
<p><em>Yes</em></p>
<p>Awesome, but as in Google Search: When you try to find the correct search prompt you need to tune the semantics of your thoughts to get what you want:
<a href="https://www.howtogeek.com/833169/how-to-write-an-awesome-stable-diffusion-prompt/">How to write stable-diffusion prompts</a></p>
<p>Of course AI can help you with this:
<a href="https://gustavosta-magicprompt-stable-diffusion.hf.space">Prompt Tuning</a></p>
<p><em>No</em></p>
<p>No worries, you are not the first one to create a prompt and there are already a lot of examples out there:</p>
<p><a href="https://krea.ai//">Examples with prompt search</a></p>
<p><a href="https://atlas.nomic.ai/map/809ef16a-5b2d-4291-b772-a913f4c8ee61/9ed7d171-650b-4526-85bf-3592ee51ea31">Atlas on examples with topics</a></p>
<h2 id="use-an-endpoint-with-stable-diffusion">Use an Endpoint with Stable Diffusion<a hidden class="anchor" aria-hidden="true" href="#use-an-endpoint-with-stable-diffusion">#</a></h2>
<p>There are already a few websites giving you access to endpoints for free.
I recommend to use one where you still have access to the codebase of the model and some evaluation.
StabilityAI, the creators of stable-diffusion, an open source latent diffusion model host their model on Huggingface and give access to an endpoint (here called spaces) to test it out:</p>
<p><a href="https://stabilityai-stable-diffusion.hf.space/">Stable Diffusion 2.1 Demo by Stability AI</a></p>
<p><a href="https://huggingface.co/stabilityai/stable-diffusion-2">Model Card of Stable Diffusion v2</a></p>
<h2 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h2>
<p>Following prompt:</p>
<p><em>&ldquo;oil painting of a cat sitting on a rainbow&rdquo;</em></p>
<p>becomes after finetuning:</p>
<p><em>&ldquo;oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha&rdquo;</em></p>
<p>and creates this picture with stable-diffusion:
<a href="/posts/2023_01_12_hands_on_latent_diffusion_models/images/cat_rainbow.jpeg"><img loading="lazy" src="/posts/2023_01_12_hands_on_latent_diffusion_models/images/cat_rainbow.jpeg" alt="cat_rainbow"  />
</a></p>
<p>I like cats!(Like everyone else on the internet i guess)</p>
<p><strong>Enjoy exploring!</strong></p>
<p>If you are interested in understanding how to create a Notebook with diffusors please see the following section.</p>
<h1 id="stable-diffusion">Stable Diffusion<a hidden class="anchor" aria-hidden="true" href="#stable-diffusion">#</a></h1>
<p><em>&hellip;using Hugging Face&rsquo;s <code>diffusers</code></em></p>
<p>*The following section focusses on <strong>inference</strong> and is based on <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"><em>Quickstart with diffusers</em></a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb"><em>Intro on diffusers</em></a></p>
<p><em>If you want to get a more hands-on guide on <strong>training</strong> diffusion models, please have a look at</em>
<a href="https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb"><em>Training with Diffusers</em></a></p>
<h2 id="summary-on-diffusion-models">Summary on Diffusion Models<a hidden class="anchor" aria-hidden="true" href="#summary-on-diffusion-models">#</a></h2>
<p>Diffusion models are machine learning systems that are trained to <em>denoise</em> random gaussian noise step by step, to get to a sample of interest, such as an <em>image</em>.</p>
<p>The underlying model, often a neural network, is trained to predict a way to slightly denoise the image in each step. After certain number of steps, a sample is obtained.</p>
<p><a href="/posts/2023_01_12_hands_on_latent_diffusion_models/images/diffusion-process.jpg"><img loading="lazy" src="/posts/2023_01_12_hands_on_latent_diffusion_models/images/diffusion-process.jpg" alt="stable_diffusion"  />
</a></p>
<p>The diffusion process consists in taking random noise of the size of the desired output and pass it through the model several times. The process ends after a given number of steps, and the output image should represent a sample according to the training data distribution of the model, for instance an image of a cat.</p>
<p>During training we show many samples of a given distribution, such as images of cat. After training, the model will be able to process random noise to generate similar cat images.</p>
<p>Without going in too much detail, the model is usually not trained to directly predict a slightly less noisy image, but rather to predict the &ldquo;noise residual&rdquo; which is the difference between a less noisy image and the input image (for a diffusion model called &ldquo;DDPM&rdquo;).</p>
<p>To do the denoising process, a specific noise scheduling algorithm is thus necessary and &ldquo;wrap&rdquo; the model to define how many diffusion steps are needed for inference as well as how to <em>compute</em> a less noisy image from the model&rsquo;s output.</p>
<h2 id="summary-on-diffusers">Summary on diffusers<a hidden class="anchor" aria-hidden="true" href="#summary-on-diffusers">#</a></h2>
<p>Stable Diffusion is based on a particular type of diffusion model called <strong>Latent Diffusion</strong>, proposed in <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>.</p>
<p>It is created by the researchers and engineers from <a href="https://github.com/CompVis">CompVis</a>, <a href="https://stability.ai/">Stability AI</a> and <a href="https://laion.ai/">LAION</a>. It&rsquo;s trained on 512x512 images from a subset of the <a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs.
See the <a href="https://huggingface.co/CompVis/stable-diffusion">model card</a> for more information.</p>
<p>However, most of the recent research on diffusion models, e.g. DALL-E 2 and Imagen, is unfortunately not accessible to the broader machine learning community and typically remains behind closed doors.</p>
<p>Here comes Hugging Face&rsquo;s library for diffusion model: <a href="https://github.com/huggingface/diffusers"><code>diffusers</code></a> with the goals to:</p>
<ul>
<li>gather recent diffusion models from independent repositories in a single and long-term maintained project that is built by and for the community,</li>
<li>reproduce high impact machine learning systems such as DALLE and Imagen in a manner that is accessible for the public, and
create an easy to use API that enables one to train their own models or re-use checkpoints from other repositories for inference.</li>
</ul>
<p>The core API of <code>diffusers</code> is divided into three components:</p>
<ol>
<li><strong>Pipelines</strong>: high-level classes designed to rapidly generate samples from popular trained diffusion models in a user-friendly fashion.</li>
<li><strong>Models</strong>: popular architectures for training new diffusion models, <em>e.g.</em> <a href="https://arxiv.org/abs/1505.04597">UNet</a>.</li>
<li><strong>Schedulers</strong>: various techniques for generating images from noise during <em>inference</em> as well as to generate noisy images for <em>training</em>.</li>
</ol>
<h2 id="how-to-create-an-image">How-to create an Image<a hidden class="anchor" aria-hidden="true" href="#how-to-create-an-image">#</a></h2>
<h3 id="install-diffusers">Install diffusers<a hidden class="anchor" aria-hidden="true" href="#install-diffusers">#</a></h3>
<pre tabindex="0"><code>!pip install diffusers==0.11.0
!pip install transformers scipy ftfy accelerate
!pip install &#34;ipywidgets&gt;=7,&lt;8&#34;
!pip install safetensors
</code></pre><h3 id="input-your-hugging-face-token">Input your Hugging Face Token<a hidden class="anchor" aria-hidden="true" href="#input-your-hugging-face-token">#</a></h3>
<p>As mentioned earlier you need a token with huggingface to import the pretrained snapshots</p>
<pre tabindex="0"><code>from huggingface_hub import notebook_login
notebook_login()
</code></pre><h3 id="pipeline">Pipeline<a hidden class="anchor" aria-hidden="true" href="#pipeline">#</a></h3>
<p><code>StableDiffusionPipeline</code> is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.</p>
<p>First, we load the pre-trained weights of all components of the model. Here we use Stable Diffusion version 2.1 (<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">stabilityai/stable-diffusion-2-1</a>), but there are other variants that you may want to try:</p>
<ul>
<li><a href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml/stable-diffusion-v1-5</a></li>
<li><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-base">stabilityai/stable-diffusion-2-1-base</a></li>
<li><a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">stabilityai/stable-diffusion-2-1</a>. This version can produce images with a resolution of 768x768, while the others work at 512x512.</li>
</ul>
<p>This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.</p>
<p>In addition to the model id <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">stabilityai/stable-diffusion-2-1</a>, we&rsquo;re also passing a specific <code>torch_dtype</code> to the <code>from_pretrained</code> method.</p>
<p>The weights are loaded from the half-precision branch <a href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16"><code>fp16</code></a> and we need to tell <code>diffusers</code> to expect the weights in float16 precision by passing <code>torch_dtype=torch.float16</code>.</p>
<p>We can import the <code>DDPMPipeline</code>, which will allow you to do inference with a couple of lines of code.
The <code>from_pretrained()</code> method allows downloading the model and its configuration from <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1">the Hugging Face Hub</a>, a repository of over 60,000 models shared by the community.</p>
<pre tabindex="0"><code>import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler

model_id = &#34;stabilityai/stable-diffusion-2-1&#34;

# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
</code></pre><p>The pipe shows now all components contained in your desired process.</p>
<pre tabindex="0"><code>pipe
</code></pre><pre tabindex="0"><code>StableDiffusionPipeline {
  &#34;_class_name&#34;: &#34;StableDiffusionPipeline&#34;,
  &#34;_diffusers_version&#34;: &#34;0.11.0&#34;,
  &#34;feature_extractor&#34;: [
    &#34;transformers&#34;,
    &#34;CLIPImageProcessor&#34;
  ],
  &#34;requires_safety_checker&#34;: false,
  &#34;safety_checker&#34;: [
    null,
    null
  ],
  &#34;scheduler&#34;: [
    &#34;diffusers&#34;,
    &#34;DDIMScheduler&#34;
  ],
  &#34;text_encoder&#34;: [
    &#34;transformers&#34;,
    &#34;CLIPTextModel&#34;
  ],
  &#34;tokenizer&#34;: [
    &#34;transformers&#34;,
    &#34;CLIPTokenizer&#34;
  ],
  &#34;unet&#34;: [
    &#34;diffusers&#34;,
    &#34;UNet2DConditionModel&#34;
  ],
  &#34;vae&#34;: [
    &#34;diffusers&#34;,
    &#34;AutoencoderKL&#34;
  ]
}
</code></pre><h3 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h3>
<p>Instances of the model class are neural networks that take a noisy <code>sample</code> as well as a <code>timestep</code> as inputs to predict a less noisy output <code>sample</code>.</p>
<p>Here a simple <code>UNet2DConditionModel</code> which was released with the <a href="https://arxiv.org/abs/2006.11239">DDPM Paper</a> is used.</p>
<pre tabindex="0"><code>pipe.unet
</code></pre><p>Similarly to what we&rsquo;ve seen for the pipeline class, we can load the model configuration and weights with one line, using the <code>from_pretrained()</code> method. It caches the model weights locally.</p>
<h3 id="scheduler">Scheduler<a hidden class="anchor" aria-hidden="true" href="#scheduler">#</a></h3>
<p>Schedulers define the noise schedule which is used to add noise to the model during training, and also define the algorithm to <em>compute</em> the slightly less noisy sample given the model output (here <code>noisy_residual</code>).</p>
<p>It is important to stress here that while <em>models</em> have trainable weights, <em>schedulers</em> are usually <em>parameter-free</em> (in the sense they have no trainable weights) and simply define the algorithm to compute the slightly less noisy sample.</p>
<pre tabindex="0"><code>pipe.scheduler
</code></pre><h3 id="generate-image">Generate Image<a hidden class="anchor" aria-hidden="true" href="#generate-image">#</a></h3>
<p>To generate an image, we simply run the pipeline and don&rsquo;t even need to give it any input, it will generate a random initial noise sample and then iterate the diffusion process. Here we use the inital prompt from above</p>
<p>The pipeline returns as output a dictionary with a generated <code>sample</code> of interest.</p>
<pre tabindex="0"><code>prompt = &#34;oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha&#34;
image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)

image.save(f&#34;rainbow_cat.png&#34;)
</code></pre><p>Et voila</p>
<p><a href="/posts/2023_01_12_hands_on_latent_diffusion_models/images/cat_sitting_rainbow.png"><img loading="lazy" src="/posts/2023_01_12_hands_on_latent_diffusion_models/images/cat_sitting_rainbow.png" alt="cat_rainbow_stable_diffusion"  />
</a></p>
<p>A good video on the topic combining intuition, code and a hands-on can be found on the <a href="https://www.youtube.com/watch?v=ltLNYA3lWAQ">Youtube Channel by Edan Meyer</a></p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p><a href="content/posts/2023_01_11_latent_diffusion_models/2023_01_11_latent_diffusion_models">Latent Diffusion Models: What is all the fuzz about?</a></p>
<p><a href="https://huggingface.co/">Hugging Face</a></p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/">Blogpost of Jay Alammar</a></p>
<p><a href="https://openai.com/dall-e-2/">Dall-E</a></p>
<p><a href="https://krea.ai//">Examples with prompt search</a></p>
<p><a href="https://atlas.nomic.ai/map/809ef16a-5b2d-4291-b772-a913f4c8ee61/9ed7d171-650b-4526-85bf-3592ee51ea31">Atlas on examples with topics</a></p>
<p><a href="https://www.howtogeek.com/833169/how-to-write-an-awesome-stable-diffusion-prompt/">How to write stable-diffusion prompts</a></p>
<p><a href="https://gustavosta-magicprompt-stable-diffusion.hf.space">Prompt Tuning</a></p>
<h1 id="further-links">Further Links<a hidden class="anchor" aria-hidden="true" href="#further-links">#</a></h1>
<p><a href="https://towardsdatascience.com/whats-hugging-face-122f4e7eb11a">What&rsquo;s HuggingFace on Medium</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/dalle/">DallE</a></li>
      <li><a href="https://patrickpt.github.io/tags/stable-diffusion/">stable-diffusion</a></li>
      <li><a href="https://patrickpt.github.io/tags/latent-diffusion-models/">latent-diffusion-models</a></li>
      <li><a href="https://patrickpt.github.io/tags/nocode/">NoCode</a></li>
      <li><a href="https://patrickpt.github.io/tags/huggingface/">HuggingFace</a></li>
      <li><a href="https://patrickpt.github.io/tags/handson/">HandsOn</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on twitter"
        href="https://twitter.com/intent/tweet/?text=Hands%20on%20with%20Latent%20Diffusion%20Models&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f&amp;hashtags=DallE%2cstable-diffusion%2clatent-diffusion-models%2cNoCode%2cHuggingFace%2cHandsOn">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f&amp;title=Hands%20on%20with%20Latent%20Diffusion%20Models&amp;summary=Hands%20on%20with%20Latent%20Diffusion%20Models&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f&title=Hands%20on%20with%20Latent%20Diffusion%20Models">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on whatsapp"
        href="https://api.whatsapp.com/send?text=Hands%20on%20with%20Latent%20Diffusion%20Models%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Latent Diffusion Models on telegram"
        href="https://telegram.me/share/url?text=Hands%20on%20with%20Latent%20Diffusion%20Models&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-latent-diffusion-models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://patrickpt.github.io/">Notes on AI</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
