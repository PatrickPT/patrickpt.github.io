<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Why DeepSeek-R1 was a significant step for Open Source AI | patrickschnass.de</title>
<meta name="keywords" content="LLM, GenAI, Architecture">
<meta name="description" content="TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.
Why all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind.">
<meta name="author" content="">
<link rel="canonical" href="https://www.patrickschnass.de/posts/deepseek/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://www.patrickschnass.de/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.patrickschnass.de/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.patrickschnass.de/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.patrickschnass.de/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.patrickschnass.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Why DeepSeek-R1 was a significant step for Open Source AI" />
<meta property="og:description" content="TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.
Why all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.patrickschnass.de/posts/deepseek/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-02-09T18:30:57+00:00" />
<meta property="article:modified_time" content="2025-02-09T18:30:57+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Why DeepSeek-R1 was a significant step for Open Source AI"/>
<meta name="twitter:description" content="TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.
Why all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.patrickschnass.de/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Why DeepSeek-R1 was a significant step for Open Source AI",
      "item": "https://www.patrickschnass.de/posts/deepseek/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Why DeepSeek-R1 was a significant step for Open Source AI",
  "name": "Why DeepSeek-R1 was a significant step for Open Source AI",
  "description": "TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.\nWhy all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind.",
  "keywords": [
    "LLM", "GenAI", "Architecture"
  ],
  "articleBody": "TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.\nWhy all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind. Its Open Source approach including a MIT license further amplifies its disruptive potential, enabling unrestricted commercial use, even for direct competitors, without costly R\u0026D.\nBeyond accessibility, DeepSeek-R1 heats innovation by openly sharing its cost-effective training methodology, challenging the narrative that advanced AI requires exorbitant GPU investments. Operationally, it’s remarkably affordable: output tokens cost 30x less than OpenAI’s, and input tokens are 55x cheaper.\nR1’s blend of performance, openness, innovation and affordability could signal that open-source can still play a pivotal role in the AI race.\nIn s Nutshell Key Training Process \u0026 Innovations in DeepSeek-R1 1. Two Core Models DeepSeek-R1-Zero:\nPure RL Training: Trained directly on the base model (DeepSeek-V3-Base) without supervised fine-tuning (SFT). Algorithm: Uses GRPO (Group Relative Policy Optimization), a critic-free RL method that estimates baselines from group scores. Reward Design: Relies on rule-based rewards (accuracy + formatting) instead of neural reward models to avoid reward hacking. Emergent Behaviors: Self-verification, reflection, and extended reasoning (e.g., “aha moments”) emerged naturally during RL. Limitations: Poor readability, language mixing, and formatting inconsistencies. DeepSeek-R1:\nCold-Start Data: Starts with thousands of curated CoT examples to fine-tune the base model, improving readability and stability. Multi-Stage Training: Cold-Start SFT: Initial fine-tuning for readable CoT generation. Reasoning-Oriented RL: Applies GRPO with added language consistency rewards. Rejection Sampling + SFT: Generates high-quality data (600k reasoning + 200k non-reasoning samples) for retraining. Final RL Alignment: Balances reasoning performance with human preferences (helpfulness/harmlessness). 2. Key Innovations RL-First Approach:\nProves reasoning capabilities can be incentivized purely through RL without SFT (novel for open research). Achieves OpenAI-o1-0912-level performance (e.g., 71% → 86.7% on AIME with majority voting). Cold-Start Strategy:\nAddresses R1-Zero’s limitations by seeding RL with human-prioritized data (readability, structured outputs). Distillation Efficiency:\nSmaller models (1.5B–70B) distilled from DeepSeek-R1 outperform RL-trained counterparts (e.g., 72.6% vs. 47% on AIME for Qwen-32B). Distilled models surpass GPT-4o/Claude-3.5-Sonnet in math/coding tasks despite smaller size. Rule-Based Rewards:\nAvoids neural reward models, simplifying training and reducing hacking risks. 3. Critical Challenges \u0026 Insights Failed Attempts: Process Reward Models (PRMs): Struggled with fine-grained step validation and scalability. Monte Carlo Tree Search (MCTS): Token-generation complexity made iterative improvement impractical. Key Insight: Distillation is more cost-effective than RL for smaller models, but advancing SOTA requires large-scale RL on powerful base models. 4. Performance Highlights DeepSeek-R1: Matches OpenAI-o1-1217 on reasoning (79.8% pass@1 on AIME) and outperforms GPT-4o/Claude-3.5 in math/coding. Distilled Models: 7B model surpasses GPT-4o on MATH-500 (92.8% vs. 74.6%). 32B model outperforms QwQ-32B-Preview by 22.6% on AIME. Why It Stands Out:\nFirst open-source work validating pure RL for reasoning. Combines scalability (GRPO), human-aligned cold-start data, and efficient distillation. Open-sources models/data, enabling community-driven advancements. Understanding what DeepSeek did in more detail To gain a clearer insight into the core framework of DeepSeek-R1, let’s break down its foundational concepts:\nReinforcement Learning (RL): This approach involves a model learning through a system of rewards and penalties tied to its actions, refining its performance over time via trial and error. In the realm of large language models (LLMs), RL can be implemented through techniques such as policy optimization (e.g., Proximal Policy Optimization or PPO), value-based methods (e.g., Q-learning), or combined approaches like actor-critic architectures. For instance, when presented with a prompt like “2 + 2 =”, the model might receive a reward of +1 for generating the correct answer “4” and a penalty of -1 for any incorrect response. In advanced LLMs, rewards are often derived from human feedback (RLHF) or automated evaluation systems like GRPO.\nSupervised Fine-Tuning (SFT): This process involves retraining a base model using a labeled dataset to enhance its performance on a specific task. For example, an LLM could be fine-tuned with a dataset of customer service queries and responses to improve its accuracy in addressing common support questions. This method is particularly effective when a substantial amount of labeled data is available.\nCold Start Data: This refers to a small, minimally labeled dataset used to provide the model with a basic grasp of the task at hand. For instance, a chatbot might be fine-tuned using a simple dataset of frequently asked questions (FAQs) extracted from a website, helping it establish a foundational understanding. This approach is especially useful when labeled data is scarce.\nMulti-Stage Training: In this method, the model undergoes training in distinct phases, each targeting a specific improvement, such as accuracy or alignment with user expectations. For example, a model might first be trained on general text data and then further refined using reinforcement learning based on user feedback to enhance its conversational capabilities.\nRejection Sampling: This technique involves generating multiple potential outputs from a model, but only retaining those that meet predefined criteria, such as quality or relevance. For example, after a reinforcement learning process, the model might produce several responses, but only the most useful ones are selected for retraining or further use. This ensures that only high-quality outputs contribute to the model’s ongoing improvement.\nHow Does DeepSeek Work? DeepSeek-R1 is built on a multi-stage training regime that combines a carefully engineered supervised fine-tuning (SFT) phase with pure reinforcement learning (RL) techniques—most notably, a novel Group Relative Policy Optimization (GRPO) framework. This section outlines the architecture, training pipeline, and mathematical formulation underpinning DeepSeek-R1.\n1. Architectural Overview At its core, DeepSeek-R1 is based on a powerful base model (DeepSeek-V3-Base) which is subsequently refined through several stages:\nBase Model Initialization:\nThe process begins with a pre-trained large language model. This base model, having been trained on vast internet-scale data, provides the foundational language understanding and generative capabilities.\nChain-of-Thought (CoT) Representation:\nA unique aspect of DeepSeek-R1 is its explicit generation of chain-of-thought reasoning. During inference, the model produces both the reasoning steps (CoT) and the final answer. This transparency is achieved by encouraging multi-step reasoning during training.\n2. Multi-Stage Training Pipeline DeepSeek-R1’s training is divided into several well-defined phases:\na. Cold-Start Supervised Fine-Tuning (SFT) Objective:\nTo provide the model with an initial “readable” structure and coherent reasoning behavior. Method:\nA relatively small but high-quality dataset of thousands of chain-of-thought examples (referred to as cold-start data) is used to fine-tune the base model. Although the dataset is orders of magnitude smaller than typical supervised corpora, its curation for clarity and structured output is crucial for overcoming issues like language mixing and formatting inconsistencies. b. Pure Reinforcement Learning with GRPO Objective:\nTo enhance reasoning capabilities by learning directly from trial-and-error without any further labeled data.\nGRPO Overview:\nTraditional RL approaches in language modeling often employ a critic network or neural reward models. Instead, DeepSeek-R1 uses GRPO—a critic-free method where rewards are determined via rule-based measures (e.g., coherence, formatting, and logical consistency) and then compared relative to group-average scores.\nMathematical Formulation:\nLet the policy of the model be denoted as ( \\pi_\\theta(y|x) ) (with parameters (\\theta)), and assume a reference policy ( \\pi_{\\text{ref}}(y|x) ) derived from the cold-start SFT stage. For a given prompt ( x ) and two candidate responses ( y_w ) (winning) and ( y_l ) (losing), define the relative log-likelihood difference as:\n[ h_{\\pi_\\theta}(x, y_w, y_l) = \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} ]\nThe standard Direct Preference Optimization (DPO) loss (as used in earlier works) is given by:\n[ L_{\\text{DPO}}(\\pi_\\theta; D) = -\\mathbb{E}{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma\\left(\\beta \\cdot h{\\pi_\\theta}(x, y_w, y_l)\\right) \\right] ]\nwhere (\\sigma(\\cdot)) is the sigmoid function and (\\beta) is a scaling factor.\nIn GRPO, to account for multiple groups ( g \\in {1, \\ldots, K} ) (which may correspond to different task domains or user preferences), the objective is reformulated as a worst-case (minimax) problem:\n[ L_{\\text{GR}}(\\pi_\\theta) = \\max_{g \\in {1,\\ldots,K}} ; L_{\\text{DPO}}(\\pi_\\theta; D_g) ]\nAlternatively, using a weighted combination over groups with weights (\\alpha \\in \\Delta_K) (the (K)-simplex), we have:\n[ \\min_{\\pi_\\theta} ; \\max_{\\alpha \\in \\Delta_K} ; \\sum_{g=1}^{K} \\alpha_g ; \\mathbb{E}{(x_g, y_w, y_l) \\sim D_g} \\left[ -\\log \\sigma\\left(\\beta \\cdot h{\\pi_\\theta}(x_g, y_w, y_l)\\right) \\right] ]\nThis formulation ensures that groups with poorer performance (i.e., higher loss) receive higher weights during training, guiding the model to improve in those areas.\nGradient Update:\nThe gradient update for the parameters (\\theta) is derived from the weighted loss. If we denote the loss on a sample as:\n[ l(\\pi_\\theta; (x_g, y_w, y_l)) = \\log \\sigma\\left(\\beta \\cdot h_{\\pi_\\theta}(x_g, y_w, y_l)\\right) ]\nthen the gradient update (ignoring normalization factors) is:\n[ \\nabla_\\theta l(\\pi_\\theta; (x_g, y_w, y_l)) \\propto \\sigma\\Big(r_\\theta(x_g, y_l) - r_\\theta(x_g, y_w)\\Big) \\left[ \\nabla_\\theta \\log \\pi_\\theta(y_w|x_g) - \\nabla_\\theta \\log \\pi_\\theta(y_l|x_g) \\right] ]\nwhere ( r_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} ). This update explicitly increases the probability of the preferred response while decreasing that of the rejected one, with additional weighting from the group-specific factors (\\alpha_g).\nc. Rejection Sampling and Synthetic Data Generation Objective:\nTo refine the model’s outputs by selecting only high-quality samples. Method:\nOnce the RL process has nearly converged, the model generates multiple outputs for a given prompt. A rejection sampling mechanism filters these outputs using the same rule-based criteria (coherence, formatting, etc.), creating a synthetic dataset of high-quality reasoning examples. This dataset typically contains on the order of 600k reasoning samples and 200k non-reasoning samples, which are then used to further fine-tune the model via supervised training. d. Final RL Alignment Objective:\nTo balance high-level reasoning with human-aligned attributes such as helpfulness and harmlessness. Method:\nA final round of RL is applied over diverse prompts. This stage consolidates improvements from the previous stages while ensuring that the model’s behavior aligns with user expectations. The final objective remains similar to the GRPO loss, ensuring that no particular subgroup (or aspect of the task) is neglected. 3. Inference: Chain-of-Thought Generation During inference, DeepSeek-R1 generates a two-part output:\nReasoning Content:\nThe intermediate chain-of-thought (CoT) that details the step-by-step reasoning process. Final Answer:\nThe concise output generated after the reasoning steps. The dual output is a direct consequence of the multi-stage training pipeline. The training not only instills raw reasoning power but also structures the output to separate the thought process from the final answer—an innovation that facilitates transparency and error analysis.\n4. Distillation into Smaller Models An additional contribution of DeepSeek-R1 is the distillation of its reasoning capabilities into smaller models (ranging from 1.5B to 70B parameters). The distilled models inherit the chain-of-thought behavior and high reasoning performance, often outperforming models trained solely with RL. This distillation leverages the observation that reasoning patterns discovered by larger models can be effectively transferred to compact architectures without sacrificing performance.\nSummary DeepSeek-R1 works through a meticulously designed multi-stage training process:\nCold-Start SFT to provide a solid, human-readable foundation. Pure RL using GRPO to boost reasoning capability by optimizing for worst-case group performance. Rejection Sampling and SFT to refine outputs by creating a high-quality synthetic dataset. Final RL Alignment to ensure the model adheres to human values and achieves a balanced performance. Mathematically, the model’s optimization is expressed via a robust version of the Direct Preference Optimization (DPO) loss, where group-specific losses are balanced through a minimax formulation.\nImplications The release of DeepSeek-R1 was not just another model but it represents a shift in how advanced LLMs can be built and deployed:\nDemocratization of Advanced AI: Against the overwhelming trend of closed-source models, DeepSeek-R1 is open-sourced and still reaches on-par performance with commercial models. This openness invites collaborative improvements and accelerates innovation in AI.\nCost-Effective Scaling: DeepSeek-R1 dramatically reduces token costs (30× cheaper for outputs and 55× cheaper for inputs than comparable commercial models). This cost efficiency challenges the narrative that state-of-the-art reasoning demands vast GPU clusters and massive labeled datasets, initiating new innovation in AI training paradigms.\nValidation of Pure RL for Reasoning: The success of DeepSeek-R1-Zero proves that advanced reasoning can emerge solely through reinforcement learning. This breakthrough inspires further research into RL-first training paradigms, potentially reshaping how future LLMs are developed.\nInfluence on Commercial Strategies: By openly publishing its training methods—including its efficient distillation and group-based optimization techniques—DeepSeek-R1 pressures commercial entities to adopt more transparent and community-friendly approaches. This could lead to a more competitive and innovative AI ecosystem.\nPitfalls While DeepSeek-R1 sets a new standard, several challenges remain:\nReadability and Formatting:\nThe initial R1-Zero model exhibited issues such as poor readability and language mixing. Although the multi-stage training process addresses these concerns, some residual inconsistencies may persist in complex scenarios.\nInference Latency:\nDeepSeek-R1’s chain-of-thought generation is computationally intensive. Inference times are slower compared to models optimized solely for speed, which may limit its use in latency-critical applications.\nLimited Inference Flexibility:\nThe current API version does not support many adjustable parameters (e.g., temperature, top_p, etc.), making it harder to fine-tune output behavior for production environments.\nRisk of Overfitting to Rule-Based Rewards:\nRelying on fixed, rule-based rewards simplifies training but may also constrain the model’s adaptability. In cases where nuanced human judgment is required, this approach might not capture every subtlety.\nScalability of Pure RL:\nAlthough pure RL has proven effective here, it typically requires longer training times and can be sensitive to reward design. The balance between cost-effectiveness and training complexity remains a delicate one.\nConclusion In summary, DeepSeek-R1 shows that it’s possible to achieve advanced reasoning capabilities in large language models without the traditionally high costs. By using a modest yet carefully curated cold-start supervised phase combined with pure reinforcement learning via GRPO—and further refined with rejection sampling—the model delivers performance that can stand alongside commercial systems like those from OpenAI. Its open-source approach and transparent training process offer practical benefits for both researchers and practitioners.\nThat said, DeepSeek-R1 is not without its challenges. There remain issues with output readability, inference speed, and some limitations in fine-tuning flexibility. These factors indicate that there is still room for improvement as we continue to explore and refine these methods.\nOverall, DeepSeek-R1 represents a thoughtful step forward in the development of cost-efficient, robust, and accessible AI. As the community builds on these ideas, we can expect further progress that will help broaden the range of available tools and techniques in the field.\nRessources https://thelmbook.com/articles/#!./DeepSeek-R1.md\nhttps://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\nhttps://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it\nhttps://arxiv.org/pdf/2405.20304\n",
  "wordCount" : "2375",
  "inLanguage": "en",
  "datePublished": "2025-02-09T18:30:57Z",
  "dateModified": "2025-02-09T18:30:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.patrickschnass.de/posts/deepseek/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickschnass.de",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.patrickschnass.de/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.patrickschnass.de/" accesskey="h" title="patrickschnass.de (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickschnass.de</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.patrickschnass.de/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Why DeepSeek-R1 was a significant step for Open Source AI
    </h1>
    <div class="post-meta"><span title='2025-02-09 18:30:57 +0000 UTC'>February 9, 2025</span>&nbsp;·&nbsp;12 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#why-all-the-rumors" aria-label="Why all the rumors?">Why all the rumors?</a></li>
                <li>
                    <a href="#in-s-nutshell" aria-label="In s Nutshell">In s Nutshell</a><ul>
                        
                <li>
                    <a href="#key-training-process--innovations-in-deepseek-r1" aria-label="Key Training Process &amp;amp; Innovations in DeepSeek-R1"><strong>Key Training Process &amp; Innovations in DeepSeek-R1</strong></a><ul>
                        
                <li>
                    <a href="#1-two-core-models" aria-label="1. Two Core Models"><strong>1. Two Core Models</strong></a></li>
                <li>
                    <a href="#2-key-innovations" aria-label="2. Key Innovations"><strong>2. Key Innovations</strong></a></li>
                <li>
                    <a href="#3-critical-challenges--insights" aria-label="3. Critical Challenges &amp;amp; Insights"><strong>3. Critical Challenges &amp; Insights</strong></a></li>
                <li>
                    <a href="#4-performance-highlights" aria-label="4. Performance Highlights"><strong>4. Performance Highlights</strong></a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#understanding-what-deepseek-did-in-more-detail" aria-label="Understanding what DeepSeek did in more detail">Understanding what DeepSeek did in more detail</a><ul>
                        
                <li>
                    <a href="#how-does-deepseek-work" aria-label="How Does DeepSeek Work?">How Does DeepSeek Work?</a><ul>
                        
                <li>
                    <a href="#1-architectural-overview" aria-label="1. Architectural Overview">1. Architectural Overview</a></li>
                <li>
                    <a href="#2-multi-stage-training-pipeline" aria-label="2. Multi-Stage Training Pipeline">2. Multi-Stage Training Pipeline</a><ul>
                        
                <li>
                    <a href="#a-cold-start-supervised-fine-tuning-sft" aria-label="a. Cold-Start Supervised Fine-Tuning (SFT)">a. Cold-Start Supervised Fine-Tuning (SFT)</a></li>
                <li>
                    <a href="#b-pure-reinforcement-learning-with-grpo" aria-label="b. Pure Reinforcement Learning with GRPO">b. Pure Reinforcement Learning with GRPO</a></li>
                <li>
                    <a href="#c-rejection-sampling-and-synthetic-data-generation" aria-label="c. Rejection Sampling and Synthetic Data Generation">c. Rejection Sampling and Synthetic Data Generation</a></li>
                <li>
                    <a href="#d-final-rl-alignment" aria-label="d. Final RL Alignment">d. Final RL Alignment</a></li></ul>
                </li>
                <li>
                    <a href="#3-inference-chain-of-thought-generation" aria-label="3. Inference: Chain-of-Thought Generation">3. Inference: Chain-of-Thought Generation</a></li>
                <li>
                    <a href="#4-distillation-into-smaller-models" aria-label="4. Distillation into Smaller Models">4. Distillation into Smaller Models</a></li></ul>
                </li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li></ul>
                </li>
                <li>
                    <a href="#implications" aria-label="Implications">Implications</a><ul>
                        
                <li>
                    <a href="#democratization-of-advanced-ai" aria-label="Democratization of Advanced AI:">Democratization of Advanced AI:</a></li>
                <li>
                    <a href="#cost-effective-scaling" aria-label="Cost-Effective Scaling:">Cost-Effective Scaling:</a></li>
                <li>
                    <a href="#validation-of-pure-rl-for-reasoning" aria-label="Validation of Pure RL for Reasoning:">Validation of Pure RL for Reasoning:</a></li>
                <li>
                    <a href="#influence-on-commercial-strategies" aria-label="Influence on Commercial Strategies:">Influence on Commercial Strategies:</a></li></ul>
                </li>
                <li>
                    <a href="#pitfalls" aria-label="Pitfalls">Pitfalls</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#ressources" aria-label="Ressources">Ressources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.</p>
<hr>
<h1 id="why-all-the-rumors">Why all the rumors?<a hidden class="anchor" aria-hidden="true" href="#why-all-the-rumors">#</a></h1>
<p>DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind. Its Open Source approach including a MIT license further amplifies its disruptive potential, enabling unrestricted commercial use, even for direct competitors, without costly R&amp;D.</p>
<p>Beyond accessibility, DeepSeek-R1 heats innovation by openly sharing its cost-effective training methodology, challenging the narrative that advanced AI requires exorbitant GPU investments. Operationally, it’s remarkably affordable: output tokens cost 30x less than OpenAI’s, and input tokens are 55x cheaper.</p>
<p>R1’s blend of performance, openness, innovation and affordability could signal that open-source can still play a pivotal role in the AI race.</p>
<hr>
<h1 id="in-s-nutshell">In s Nutshell<a hidden class="anchor" aria-hidden="true" href="#in-s-nutshell">#</a></h1>
<h2 id="key-training-process--innovations-in-deepseek-r1"><strong>Key Training Process &amp; Innovations in DeepSeek-R1</strong><a hidden class="anchor" aria-hidden="true" href="#key-training-process--innovations-in-deepseek-r1">#</a></h2>
<h3 id="1-two-core-models"><strong>1. Two Core Models</strong><a hidden class="anchor" aria-hidden="true" href="#1-two-core-models">#</a></h3>
<ul>
<li>
<p><strong>DeepSeek-R1-Zero</strong>:</p>
<ul>
<li><strong>Pure RL Training</strong>: Trained <em>directly</em> on the base model (DeepSeek-V3-Base) <strong>without supervised fine-tuning (SFT)</strong>.</li>
<li><strong>Algorithm</strong>: Uses <strong>GRPO</strong> (Group Relative Policy Optimization), a critic-free RL method that estimates baselines from group scores.</li>
<li><strong>Reward Design</strong>: Relies on <strong>rule-based rewards</strong> (accuracy + formatting) instead of neural reward models to avoid reward hacking.</li>
<li><strong>Emergent Behaviors</strong>: Self-verification, reflection, and extended reasoning (e.g., &ldquo;aha moments&rdquo;) emerged naturally during RL.</li>
<li><strong>Limitations</strong>: Poor readability, language mixing, and formatting inconsistencies.</li>
</ul>
</li>
<li>
<p><strong>DeepSeek-R1</strong>:</p>
<ul>
<li><strong>Cold-Start Data</strong>: Starts with <strong>thousands of curated CoT examples</strong> to fine-tune the base model, improving readability and stability.</li>
<li><strong>Multi-Stage Training</strong>:
<ol>
<li><strong>Cold-Start SFT</strong>: Initial fine-tuning for readable CoT generation.</li>
<li><strong>Reasoning-Oriented RL</strong>: Applies GRPO with added <strong>language consistency rewards</strong>.</li>
<li><strong>Rejection Sampling + SFT</strong>: Generates high-quality data (600k reasoning + 200k non-reasoning samples) for retraining.</li>
<li><strong>Final RL Alignment</strong>: Balances reasoning performance with human preferences (helpfulness/harmlessness).</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="2-key-innovations"><strong>2. Key Innovations</strong><a hidden class="anchor" aria-hidden="true" href="#2-key-innovations">#</a></h3>
<ol>
<li>
<p><strong>RL-First Approach</strong>:</p>
<ul>
<li>Proves reasoning capabilities can be <strong>incentivized purely through RL</strong> without SFT (novel for open research).</li>
<li>Achieves OpenAI-o1-0912-level performance (e.g., 71% → 86.7% on AIME with majority voting).</li>
</ul>
</li>
<li>
<p><strong>Cold-Start Strategy</strong>:</p>
<ul>
<li>Addresses R1-Zero’s limitations by seeding RL with human-prioritized data (readability, structured outputs).</li>
</ul>
</li>
<li>
<p><strong>Distillation Efficiency</strong>:</p>
<ul>
<li>Smaller models (1.5B–70B) distilled from DeepSeek-R1 outperform RL-trained counterparts (e.g., 72.6% vs. 47% on AIME for Qwen-32B).</li>
<li>Distilled models surpass GPT-4o/Claude-3.5-Sonnet in math/coding tasks despite smaller size.</li>
</ul>
</li>
<li>
<p><strong>Rule-Based Rewards</strong>:</p>
<ul>
<li>Avoids neural reward models, simplifying training and reducing hacking risks.</li>
</ul>
</li>
</ol>
<h3 id="3-critical-challenges--insights"><strong>3. Critical Challenges &amp; Insights</strong><a hidden class="anchor" aria-hidden="true" href="#3-critical-challenges--insights">#</a></h3>
<ul>
<li><strong>Failed Attempts</strong>:
<ul>
<li><strong>Process Reward Models (PRMs)</strong>: Struggled with fine-grained step validation and scalability.</li>
<li><strong>Monte Carlo Tree Search (MCTS)</strong>: Token-generation complexity made iterative improvement impractical.</li>
</ul>
</li>
<li><strong>Key Insight</strong>: Distillation is more cost-effective than RL for smaller models, but advancing SOTA requires large-scale RL on powerful base models.</li>
</ul>
<h3 id="4-performance-highlights"><strong>4. Performance Highlights</strong><a hidden class="anchor" aria-hidden="true" href="#4-performance-highlights">#</a></h3>
<ul>
<li><strong>DeepSeek-R1</strong>: Matches <strong>OpenAI-o1-1217</strong> on reasoning (79.8% pass@1 on AIME) and outperforms GPT-4o/Claude-3.5 in math/coding.</li>
<li><strong>Distilled Models</strong>:
<ul>
<li>7B model surpasses GPT-4o on MATH-500 (92.8% vs. 74.6%).</li>
<li>32B model outperforms QwQ-32B-Preview by 22.6% on AIME.</li>
</ul>
</li>
</ul>
<p><strong>Why It Stands Out</strong>:</p>
<ul>
<li>First open-source work validating pure RL for reasoning.</li>
<li>Combines scalability (GRPO), human-aligned cold-start data, and efficient distillation.</li>
<li>Open-sources models/data, enabling community-driven advancements.</li>
</ul>
<hr>
<h1 id="understanding-what-deepseek-did-in-more-detail">Understanding what DeepSeek did in more detail<a hidden class="anchor" aria-hidden="true" href="#understanding-what-deepseek-did-in-more-detail">#</a></h1>
<p>To gain a clearer insight into the core framework of DeepSeek-R1, let’s break down its foundational concepts:</p>
<p><strong>Reinforcement Learning (RL):</strong> This approach involves a model learning through a system of rewards and penalties tied to its actions, refining its performance over time via trial and error. In the realm of large language models (LLMs), RL can be implemented through techniques such as policy optimization (e.g., Proximal Policy Optimization or PPO), value-based methods (e.g., Q-learning), or combined approaches like actor-critic architectures. For instance, when presented with a prompt like “2 + 2 =”, the model might receive a reward of +1 for generating the correct answer “4” and a penalty of -1 for any incorrect response. In advanced LLMs, rewards are often derived from human feedback (RLHF) or automated evaluation systems like GRPO.</p>
<p><strong>Supervised Fine-Tuning (SFT):</strong> This process involves retraining a base model using a labeled dataset to enhance its performance on a specific task. For example, an LLM could be fine-tuned with a dataset of customer service queries and responses to improve its accuracy in addressing common support questions. This method is particularly effective when a substantial amount of labeled data is available.</p>
<p><strong>Cold Start Data:</strong> This refers to a small, minimally labeled dataset used to provide the model with a basic grasp of the task at hand. For instance, a chatbot might be fine-tuned using a simple dataset of frequently asked questions (FAQs) extracted from a website, helping it establish a foundational understanding. This approach is especially useful when labeled data is scarce.</p>
<p><strong>Multi-Stage Training:</strong> In this method, the model undergoes training in distinct phases, each targeting a specific improvement, such as accuracy or alignment with user expectations. For example, a model might first be trained on general text data and then further refined using reinforcement learning based on user feedback to enhance its conversational capabilities.</p>
<p><strong>Rejection Sampling:</strong> This technique involves generating multiple potential outputs from a model, but only retaining those that meet predefined criteria, such as quality or relevance. For example, after a reinforcement learning process, the model might produce several responses, but only the most useful ones are selected for retraining or further use. This ensures that only high-quality outputs contribute to the model’s ongoing improvement.</p>
<hr>
<h2 id="how-does-deepseek-work">How Does DeepSeek Work?<a hidden class="anchor" aria-hidden="true" href="#how-does-deepseek-work">#</a></h2>
<p>DeepSeek-R1 is built on a multi-stage training regime that combines a carefully engineered supervised fine-tuning (SFT) phase with pure reinforcement learning (RL) techniques—most notably, a novel Group Relative Policy Optimization (GRPO) framework. This section outlines the architecture, training pipeline, and mathematical formulation underpinning DeepSeek-R1.</p>
<h3 id="1-architectural-overview">1. Architectural Overview<a hidden class="anchor" aria-hidden="true" href="#1-architectural-overview">#</a></h3>
<p>At its core, DeepSeek-R1 is based on a powerful base model (DeepSeek-V3-Base) which is subsequently refined through several stages:</p>
<ul>
<li>
<p><strong>Base Model Initialization:</strong><br>
The process begins with a pre-trained large language model. This base model, having been trained on vast internet-scale data, provides the foundational language understanding and generative capabilities.</p>
</li>
<li>
<p><strong>Chain-of-Thought (CoT) Representation:</strong><br>
A unique aspect of DeepSeek-R1 is its explicit generation of chain-of-thought reasoning. During inference, the model produces both the reasoning steps (CoT) and the final answer. This transparency is achieved by encouraging multi-step reasoning during training.</p>
</li>
</ul>
<h3 id="2-multi-stage-training-pipeline">2. Multi-Stage Training Pipeline<a hidden class="anchor" aria-hidden="true" href="#2-multi-stage-training-pipeline">#</a></h3>
<p>DeepSeek-R1’s training is divided into several well-defined phases:</p>
<h4 id="a-cold-start-supervised-fine-tuning-sft">a. Cold-Start Supervised Fine-Tuning (SFT)<a hidden class="anchor" aria-hidden="true" href="#a-cold-start-supervised-fine-tuning-sft">#</a></h4>
<ul>
<li><strong>Objective:</strong><br>
To provide the model with an initial “readable” structure and coherent reasoning behavior.</li>
<li><strong>Method:</strong><br>
A relatively small but high-quality dataset of thousands of chain-of-thought examples (referred to as <em>cold-start data</em>) is used to fine-tune the base model. Although the dataset is orders of magnitude smaller than typical supervised corpora, its curation for clarity and structured output is crucial for overcoming issues like language mixing and formatting inconsistencies.</li>
</ul>
<h4 id="b-pure-reinforcement-learning-with-grpo">b. Pure Reinforcement Learning with GRPO<a hidden class="anchor" aria-hidden="true" href="#b-pure-reinforcement-learning-with-grpo">#</a></h4>
<ul>
<li>
<p><strong>Objective:</strong><br>
To enhance reasoning capabilities by learning directly from trial-and-error without any further labeled data.</p>
</li>
<li>
<p><strong>GRPO Overview:</strong><br>
Traditional RL approaches in language modeling often employ a critic network or neural reward models. Instead, DeepSeek-R1 uses GRPO—a critic-free method where rewards are determined via rule-based measures (e.g., coherence, formatting, and logical consistency) and then compared relative to group-average scores.</p>
</li>
<li>
<p><strong>Mathematical Formulation:</strong></p>
<p>Let the policy of the model be denoted as ( \pi_\theta(y|x) ) (with parameters (\theta)), and assume a reference policy ( \pi_{\text{ref}}(y|x) ) derived from the cold-start SFT stage. For a given prompt ( x ) and two candidate responses ( y_w ) (winning) and ( y_l ) (losing), define the relative log-likelihood difference as:</p>
<p>[
h_{\pi_\theta}(x, y_w, y_l) = \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}
]</p>
<p>The standard Direct Preference Optimization (DPO) loss (as used in earlier works) is given by:</p>
<p>[
L_{\text{DPO}}(\pi_\theta; D) = -\mathbb{E}<em>{(x, y_w, y_l) \sim D} \left[ \log \sigma\left(\beta \cdot h</em>{\pi_\theta}(x, y_w, y_l)\right) \right]
]</p>
<p>where (\sigma(\cdot)) is the sigmoid function and (\beta) is a scaling factor.</p>
<p>In GRPO, to account for multiple groups ( g \in {1, \ldots, K} ) (which may correspond to different task domains or user preferences), the objective is reformulated as a worst-case (minimax) problem:</p>
<p>[
L_{\text{GR}}(\pi_\theta) = \max_{g \in {1,\ldots,K}} ; L_{\text{DPO}}(\pi_\theta; D_g)
]</p>
<p>Alternatively, using a weighted combination over groups with weights (\alpha \in \Delta_K) (the (K)-simplex), we have:</p>
<p>[
\min_{\pi_\theta} ; \max_{\alpha \in \Delta_K} ; \sum_{g=1}^{K} \alpha_g ; \mathbb{E}<em>{(x_g, y_w, y_l) \sim D_g} \left[ -\log \sigma\left(\beta \cdot h</em>{\pi_\theta}(x_g, y_w, y_l)\right) \right]
]</p>
<p>This formulation ensures that groups with poorer performance (i.e., higher loss) receive higher weights during training, guiding the model to improve in those areas.</p>
</li>
<li>
<p><strong>Gradient Update:</strong><br>
The gradient update for the parameters (\theta) is derived from the weighted loss. If we denote the loss on a sample as:</p>
<p>[
l(\pi_\theta; (x_g, y_w, y_l)) = \log \sigma\left(\beta \cdot h_{\pi_\theta}(x_g, y_w, y_l)\right)
]</p>
<p>then the gradient update (ignoring normalization factors) is:</p>
<p>[
\nabla_\theta l(\pi_\theta; (x_g, y_w, y_l)) \propto \sigma\Big(r_\theta(x_g, y_l) - r_\theta(x_g, y_w)\Big) \left[ \nabla_\theta \log \pi_\theta(y_w|x_g) - \nabla_\theta \log \pi_\theta(y_l|x_g) \right]
]</p>
<p>where ( r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} ). This update explicitly increases the probability of the preferred response while decreasing that of the rejected one, with additional weighting from the group-specific factors (\alpha_g).</p>
</li>
</ul>
<h4 id="c-rejection-sampling-and-synthetic-data-generation">c. Rejection Sampling and Synthetic Data Generation<a hidden class="anchor" aria-hidden="true" href="#c-rejection-sampling-and-synthetic-data-generation">#</a></h4>
<ul>
<li><strong>Objective:</strong><br>
To refine the model’s outputs by selecting only high-quality samples.</li>
<li><strong>Method:</strong><br>
Once the RL process has nearly converged, the model generates multiple outputs for a given prompt. A rejection sampling mechanism filters these outputs using the same rule-based criteria (coherence, formatting, etc.), creating a synthetic dataset of high-quality reasoning examples. This dataset typically contains on the order of 600k reasoning samples and 200k non-reasoning samples, which are then used to further fine-tune the model via supervised training.</li>
</ul>
<h4 id="d-final-rl-alignment">d. Final RL Alignment<a hidden class="anchor" aria-hidden="true" href="#d-final-rl-alignment">#</a></h4>
<ul>
<li><strong>Objective:</strong><br>
To balance high-level reasoning with human-aligned attributes such as helpfulness and harmlessness.</li>
<li><strong>Method:</strong><br>
A final round of RL is applied over diverse prompts. This stage consolidates improvements from the previous stages while ensuring that the model’s behavior aligns with user expectations. The final objective remains similar to the GRPO loss, ensuring that no particular subgroup (or aspect of the task) is neglected.</li>
</ul>
<h3 id="3-inference-chain-of-thought-generation">3. Inference: Chain-of-Thought Generation<a hidden class="anchor" aria-hidden="true" href="#3-inference-chain-of-thought-generation">#</a></h3>
<p>During inference, DeepSeek-R1 generates a two-part output:</p>
<ul>
<li><strong>Reasoning Content:</strong><br>
The intermediate chain-of-thought (CoT) that details the step-by-step reasoning process.</li>
<li><strong>Final Answer:</strong><br>
The concise output generated after the reasoning steps.</li>
</ul>
<p>The dual output is a direct consequence of the multi-stage training pipeline. The training not only instills raw reasoning power but also structures the output to separate the thought process from the final answer—an innovation that facilitates transparency and error analysis.</p>
<h3 id="4-distillation-into-smaller-models">4. Distillation into Smaller Models<a hidden class="anchor" aria-hidden="true" href="#4-distillation-into-smaller-models">#</a></h3>
<p>An additional contribution of DeepSeek-R1 is the distillation of its reasoning capabilities into smaller models (ranging from 1.5B to 70B parameters). The distilled models inherit the chain-of-thought behavior and high reasoning performance, often outperforming models trained solely with RL. This distillation leverages the observation that reasoning patterns discovered by larger models can be effectively transferred to compact architectures without sacrificing performance.</p>
<hr>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>DeepSeek-R1 works through a meticulously designed multi-stage training process:</p>
<ol>
<li><strong>Cold-Start SFT</strong> to provide a solid, human-readable foundation.</li>
<li><strong>Pure RL using GRPO</strong> to boost reasoning capability by optimizing for worst-case group performance.</li>
<li><strong>Rejection Sampling and SFT</strong> to refine outputs by creating a high-quality synthetic dataset.</li>
<li><strong>Final RL Alignment</strong> to ensure the model adheres to human values and achieves a balanced performance.</li>
</ol>
<p>Mathematically, the model’s optimization is expressed via a robust version of the Direct Preference Optimization (DPO) loss, where group-specific losses are balanced through a minimax formulation.</p>
<hr>
<h1 id="implications">Implications<a hidden class="anchor" aria-hidden="true" href="#implications">#</a></h1>
<p>The release of DeepSeek-R1 was not just another model but it represents a shift in how advanced LLMs can be built and deployed:</p>
<h2 id="democratization-of-advanced-ai">Democratization of Advanced AI:<a hidden class="anchor" aria-hidden="true" href="#democratization-of-advanced-ai">#</a></h2>
<p>Against the overwhelming trend of closed-source models, DeepSeek-R1 is open-sourced and still reaches on-par performance with commercial models. This openness invites collaborative improvements and accelerates innovation in AI.</p>
<h2 id="cost-effective-scaling">Cost-Effective Scaling:<a hidden class="anchor" aria-hidden="true" href="#cost-effective-scaling">#</a></h2>
<p>DeepSeek-R1 dramatically reduces token costs (30× cheaper for outputs and 55× cheaper for inputs than comparable commercial models). This cost efficiency challenges the narrative that state-of-the-art reasoning demands vast GPU clusters and massive labeled datasets, initiating new innovation in AI training paradigms.</p>
<h2 id="validation-of-pure-rl-for-reasoning">Validation of Pure RL for Reasoning:<a hidden class="anchor" aria-hidden="true" href="#validation-of-pure-rl-for-reasoning">#</a></h2>
<p>The success of DeepSeek-R1-Zero proves that advanced reasoning can emerge solely through reinforcement learning. This breakthrough inspires further research into RL-first training paradigms, potentially reshaping how future LLMs are developed.</p>
<h2 id="influence-on-commercial-strategies">Influence on Commercial Strategies:<a hidden class="anchor" aria-hidden="true" href="#influence-on-commercial-strategies">#</a></h2>
<p>By openly publishing its training methods—including its efficient distillation and group-based optimization techniques—DeepSeek-R1 pressures commercial entities to adopt more transparent and community-friendly approaches. This could lead to a more competitive and innovative AI ecosystem.</p>
<hr>
<h1 id="pitfalls">Pitfalls<a hidden class="anchor" aria-hidden="true" href="#pitfalls">#</a></h1>
<p>While DeepSeek-R1 sets a new standard, several challenges remain:</p>
<ul>
<li>
<p><strong>Readability and Formatting:</strong><br>
The initial R1-Zero model exhibited issues such as poor readability and language mixing. Although the multi-stage training process addresses these concerns, some residual inconsistencies may persist in complex scenarios.</p>
</li>
<li>
<p><strong>Inference Latency:</strong><br>
DeepSeek-R1’s chain-of-thought generation is computationally intensive. Inference times are slower compared to models optimized solely for speed, which may limit its use in latency-critical applications.</p>
</li>
<li>
<p><strong>Limited Inference Flexibility:</strong><br>
The current API version does not support many adjustable parameters (e.g., temperature, top_p, etc.), making it harder to fine-tune output behavior for production environments.</p>
</li>
<li>
<p><strong>Risk of Overfitting to Rule-Based Rewards:</strong><br>
Relying on fixed, rule-based rewards simplifies training but may also constrain the model’s adaptability. In cases where nuanced human judgment is required, this approach might not capture every subtlety.</p>
</li>
<li>
<p><strong>Scalability of Pure RL:</strong><br>
Although pure RL has proven effective here, it typically requires longer training times and can be sensitive to reward design. The balance between cost-effectiveness and training complexity remains a delicate one.</p>
</li>
</ul>
<hr>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>In summary, DeepSeek-R1 shows that it’s possible to achieve advanced reasoning capabilities in large language models without the traditionally high costs. By using a modest yet carefully curated cold-start supervised phase combined with pure reinforcement learning via GRPO—and further refined with rejection sampling—the model delivers performance that can stand alongside commercial systems like those from OpenAI. Its open-source approach and transparent training process offer practical benefits for both researchers and practitioners.</p>
<p>That said, DeepSeek-R1 is not without its challenges. There remain issues with output readability, inference speed, and some limitations in fine-tuning flexibility. These factors indicate that there is still room for improvement as we continue to explore and refine these methods.</p>
<p>Overall, DeepSeek-R1 represents a thoughtful step forward in the development of cost-efficient, robust, and accessible AI. As the community builds on these ideas, we can expect further progress that will help broaden the range of available tools and techniques in the field.</p>
<hr>
<h1 id="ressources">Ressources<a hidden class="anchor" aria-hidden="true" href="#ressources">#</a></h1>
<p><a href="https://thelmbook.com/articles/#!./DeepSeek-R1.md">https://thelmbook.com/articles/#!./DeepSeek-R1.md</a></p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf</a></p>
<p><a href="https://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it">https://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it</a></p>
<p><a href="https://arxiv.org/pdf/2405.20304">https://arxiv.org/pdf/2405.20304</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.patrickschnass.de/tags/llm/">LLM</a></li>
      <li><a href="https://www.patrickschnass.de/tags/genai/">GenAI</a></li>
      <li><a href="https://www.patrickschnass.de/tags/architecture/">Architecture</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on twitter"
        href="https://twitter.com/intent/tweet/?text=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f&amp;hashtags=LLM%2cGenAI%2cArchitecture">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f&amp;title=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI&amp;summary=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI&amp;source=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f&title=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on whatsapp"
        href="https://api.whatsapp.com/send?text=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI%20-%20https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Why DeepSeek-R1 was a significant step for Open Source AI on telegram"
        href="https://telegram.me/share/url?text=Why%20DeepSeek-R1%20was%20a%20significant%20step%20for%20Open%20Source%20AI&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fdeepseek%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://www.patrickschnass.de/">patrickschnass.de</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
