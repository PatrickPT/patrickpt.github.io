<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Hands on with Retrieval Augmented Generation | Notes on AI</title>
<meta name="keywords" content="Hands On, AI Design, LLM, GenAI">
<meta name="description" content="TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&rsquo;s with long context windows.">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/hands-on-rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Hands on with Retrieval Augmented Generation" />
<meta property="og:description" content="TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&rsquo;s with long context windows." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/hands-on-rag/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-07T10:30:00+00:00" />
<meta property="article:modified_time" content="2023-10-07T10:30:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hands on with Retrieval Augmented Generation"/>
<meta name="twitter:description" content="TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&rsquo;s with long context windows."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Hands on with Retrieval Augmented Generation",
      "item": "https://patrickpt.github.io/posts/hands-on-rag/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hands on with Retrieval Augmented Generation",
  "name": "Hands on with Retrieval Augmented Generation",
  "description": "TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model\nHands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.\nRetrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM\u0026rsquo;s with long context windows.",
  "keywords": [
    "Hands On", "AI Design", "LLM", "GenAI"
  ],
  "articleBody": "TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model\nHands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.\nRetrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM’s with long context windows.\nPrerequisites All the code mentioned here can be found on github. The code can be run in a Docker container(even on a Raspberry Pi if you like). You need to add contextual data which you want to query and also use an API Key from OpenAI.\nDependencies The complete python code is containerized with docker and can be run via docker compose. It uses the following main dependencies:\nstreamlit as an easy to use an easy to implement Frontend. No need to set up Flask and debug through your CSS. Streamlit is open-source. llama_index which is used to build the retrieval engine. It is a simple, flexible data framework for connecting custom data sources to large language models. It is somehow similar to LangChain openai provides access to the OpenAI API Other packages are used to convert the context data. All dependencies can be found in the requirements.txt\nOverview The Knowledge Bot is a web-based chatbot that provides information and answers questions related to any data which is given as context based on Retrieval Augmented Generation Architecture. It utilizes the llama_index library for data indexing and OpenAI’s GPT-3.5-Turbo model for generating responses.\nThe chatbot is designed to assist users in finding information by answering questions based on indexed documents.\nFeatures Ask questions related to your indexed documents. Receive informative responses based on indexed data. Convenient web-based interface powered by Streamlit. Setup To run the Knowledge Bot locally with docker, follow these steps:\nClone this repository to your local machine:\ngit clone https://github.com/PatrickPT/RAG_LLM_example.git Create your OpenAI Key\ncd RAG_LLM_example mkdir .streamlit nano .streamlit/secrets.toml # Insert your API Key as openai_key = \"API Key\" and save Create your documents or change the input_dir parameter in config.yaml to your folder(which needs to be accessible from the docker container)\nmkdir data # Insert the contextual documents the LLM should use in that folder Change the config.yaml file accordingly to your prior changes\n-config: api: gpt-3.5-turbo info: This bot knows everything about PromptEngineering which is mentioned in the guides in https://www.promptingguide.ai/ input_dir: ./data name: Knowledge Bot system_prompt: You are an expert on Prompt Engineering and Retrieval Augmented Generation with Large Language Models. Assume that all questions are related to Prompt Engineering. Keep your answers technical and based on facts. Do not hallucinate features. Run docker compose\ndocker compose up -d Code This small project is a Streamlit-based web application that serves as a chatbot powered by the “llama_index” package and OpenAI’s GPT-3.5-Turbo model. It allows users to ask questions related to all documents which are stored in /data and provides informative responses.\nSeveral libraries, including streamlit, llama_index, openai, and others are imported.\nimport streamlit as st from llama_index import VectorStoreIndex, ServiceContext, Document from llama_index.llms import OpenAI import openai from llama_index import SimpleDirectoryReader import yaml The configuration is imported from config.yaml\nwith open(\"config.yaml\", \"r\") as yamlfile: config = yaml.load(yamlfile, Loader=yaml.FullLoader) # import configuration from yaml name = config[0]['config']['name'] info = config[0]['config']['info'] input_dir = config[0]['config']['input_dir'] system_prompt = config[0]['config']['system_prompt'] api = config[0]['config']['api'] The Streamlit app’s title, icon, layout, and sidebar state are configured.\n# Set Streamlit page configuration st.set_page_config( page_title=name, page_icon=\"🦙\", layout=\"centered\", initial_sidebar_state=\"auto\", menu_items=None ) OpenAI API key is set using a secret obtained from Streamlit secrets. The key is stored in /.streamlit/secrets.toml\n# Set OpenAI API key openai.api_key = st.secrets.openai_key Create the main interface: title and information message about the bot’s capabilities is configured.\n# Create main interface st.title(name) st.info(info, icon=\"📃\") A list called messages is initialized in Streamlit session state, which will be used to store the chat history.\n# Initialize the chat messages history if \"messages\" not in st.session_state.keys(): st.session_state.messages = [ {\"role\": \"assistant\", \"content\": \"Ask me a question\"} ] A function called load_data is built, that loads and indexes data from /data. This data is used for responding to user queries.\n# Function to load data @st.cache_resource(show_spinner=False) # data is cached in memory so limit the knowledge base according to your machine def load_data(): with st.spinner(text=\"Loading and indexing the provided data\"): reader = SimpleDirectoryReader(input_dir=input_dir, recursive=True) # read recursively all directories docs = reader.load_data() # load data and create docs service_context = ServiceContext.from_defaults(llm=OpenAI(model=api, temperature=0.5, system_prompt=system_prompt)) # add a permanent service prompt which is added index = VectorStoreIndex.from_documents(docs, service_context=service_context) # create your vector database return index Call the load_data function to load and index the data. Also a chat engine is initialized using the indexed data.\n# Load data and create the chat engine index = load_data() chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True) Check if the user has entered a question through the Streamlit chat input widget. If there is user input, it is appended to the chat history.\n# User input and chat history if prompt := st.chat_input(\"Your question\"): st.session_state.messages.append({\"role\": \"user\", \"content\": prompt}) Loop through the chat history and displays all previous messages in the chat interface.\n# Display chat history for message in st.session_state.messages: with st.chat_message(message[\"role\"]): st.write(message[\"content\"]) Checks if the last message in the chat history is not from the assistant (bot). If it’s not from the assistant, a response is generated using the chat engine and added to the chat history.\n# Generate a response if the last message is not from the assistant if st.session_state.messages[-1][\"role\"] != \"assistant\": with st.chat_message(\"assistant\"): with st.spinner(\"Thinking...\"): response = chat_engine.chat(prompt) st.write(response.response) message = {\"role\": \"assistant\", \"content\": response.response} st.session_state.messages.append(message) Resources Build a chatbot with custom data sources, powered by LlamaIndex\nStreamlit Secrets management\n",
  "wordCount" : "957",
  "inLanguage": "en",
  "datePublished": "2023-10-07T10:30:00Z",
  "dateModified": "2023-10-07T10:30:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/hands-on-rag/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes on AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="Notes on AI (Alt + H)">Notes on AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/recent/" title="Recent">
                    <span>Recent</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Hands on with Retrieval Augmented Generation
    </h1>
    <div class="post-meta"><span title='2023-10-07 10:30:00 +0000 UTC'>October 7, 2023</span>&nbsp;·&nbsp;5 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#hands-on-with-retrieval-augmented-generation" aria-label="Hands on with Retrieval Augmented Generation">Hands on with Retrieval Augmented Generation</a></li>
                <li>
                    <a href="#prerequisites" aria-label="Prerequisites">Prerequisites</a></li>
                <li>
                    <a href="#dependencies" aria-label="Dependencies">Dependencies</a></li>
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#features" aria-label="Features">Features</a></li>
                <li>
                    <a href="#setup" aria-label="Setup">Setup</a></li>
                <li>
                    <a href="#code" aria-label="Code">Code</a></li>
                <li>
                    <a href="#resources" aria-label="Resources">Resources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model</p>
<h1 id="hands-on-with-retrieval-augmented-generation">Hands on with Retrieval Augmented Generation<a hidden class="anchor" aria-hidden="true" href="#hands-on-with-retrieval-augmented-generation">#</a></h1>
<p>For a primer on Retrieval Augmented Generation please read my other post <a href="/posts/rag_intro">What is Retrieval Augmented Generation?</a>.</p>
<p>Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent <a href="https://arxiv.org/abs/2310.03025">study</a>) even outperform LLM&rsquo;s with long context windows.</p>
<h1 id="prerequisites">Prerequisites<a hidden class="anchor" aria-hidden="true" href="#prerequisites">#</a></h1>
<p>All the code mentioned here can be found on <a href="https://github.com/PatrickPT/RAG_LLM_example">github</a>. The code can be run in a Docker container(even on a Raspberry Pi if you like). You need to add contextual data which you want to query and also use an API Key from OpenAI.</p>
<h1 id="dependencies">Dependencies<a hidden class="anchor" aria-hidden="true" href="#dependencies">#</a></h1>
<p>The complete python code is containerized with docker and can be run via docker compose.
It uses the following main dependencies:</p>
<ul>
<li><a href="https://streamlit.io">streamlit</a> as an easy to use an easy to implement Frontend. No need to set up Flask and debug through your CSS. Streamlit is open-source.</li>
<li><a href="https://www.llamaindex.ai">llama_index</a> which is used to build the retrieval engine. It is a simple, flexible data framework for connecting custom data sources to large language models. It is somehow similar to <a href="https://www.langchain.com">LangChain</a></li>
<li><a href="https://github.com/openai/openai-python">openai</a> provides access to the OpenAI API</li>
</ul>
<p>Other packages are used to convert the context data. All dependencies can be found in the <a href="https://github.com/PatrickPT/RAG_LLM_example/blob/main/requirements.txt">requirements.txt</a></p>
<h1 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h1>
<p>The <strong>Knowledge Bot</strong> is a web-based chatbot that provides information and answers questions related to any data which is given as context based on Retrieval Augmented Generation Architecture. It utilizes the <code>llama_index</code> library for data indexing and OpenAI&rsquo;s GPT-3.5-Turbo model for generating responses.</p>
<p>The chatbot is designed to assist users in finding information by answering questions based on indexed documents.</p>
<h1 id="features">Features<a hidden class="anchor" aria-hidden="true" href="#features">#</a></h1>
<ul>
<li>Ask questions related to your indexed documents.</li>
<li>Receive informative responses based on indexed data.</li>
<li>Convenient web-based interface powered by Streamlit.</li>
</ul>
<h1 id="setup">Setup<a hidden class="anchor" aria-hidden="true" href="#setup">#</a></h1>
<p>To run the Knowledge Bot locally with docker, follow these steps:</p>
<ol>
<li>
<p>Clone this repository to your local machine:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/PatrickPT/RAG_LLM_example.git
</span></span></code></pre></div></li>
<li>
<p>Create your OpenAI Key</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd RAG_LLM_example
</span></span><span style="display:flex;"><span>mkdir .streamlit
</span></span><span style="display:flex;"><span>nano .streamlit/secrets.toml
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Insert your API Key as openai_key = &#34;API Key&#34; and save</span>
</span></span></code></pre></div></li>
<li>
<p>Create your documents or change the input_dir parameter in config.yaml to your folder(which needs to be accessible from the docker container)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir data
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Insert the contextual documents the LLM should use in that folder</span>
</span></span></code></pre></div></li>
<li>
<p>Change the config.yaml file accordingly to your prior changes</p>
<pre><code> -config:
     api: gpt-3.5-turbo
     info: This bot knows everything about PromptEngineering which is mentioned in the guides in https://www.promptingguide.ai/
     input_dir: ./data
     name: Knowledge Bot
     system_prompt: You are an expert on Prompt Engineering and Retrieval Augmented Generation with Large Language Models. Assume that all questions are related to Prompt Engineering. Keep your answers technical and based on facts. Do not
         hallucinate features.
</code></pre>
</li>
<li>
<p>Run docker compose</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker compose up -d
</span></span></code></pre></div></li>
</ol>
<h1 id="code">Code<a hidden class="anchor" aria-hidden="true" href="#code">#</a></h1>
<p>This small project is a Streamlit-based web application that serves as a chatbot powered by the &ldquo;llama_index&rdquo; package and OpenAI&rsquo;s GPT-3.5-Turbo model. It allows users to ask questions related to all documents which are stored in <code>/data</code> and provides informative responses.</p>
<p>Several libraries, including streamlit, llama_index, openai, and others are imported.</p>
<pre><code>import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader
import yaml
</code></pre>
<p>The configuration is imported from <code>config.yaml</code></p>
<pre><code>with open(&quot;config.yaml&quot;, &quot;r&quot;) as yamlfile:
    config = yaml.load(yamlfile, Loader=yaml.FullLoader)

# import configuration from yaml
name = config[0]['config']['name']
info = config[0]['config']['info']
input_dir = config[0]['config']['input_dir']
system_prompt = config[0]['config']['system_prompt']
api = config[0]['config']['api']
</code></pre>
<p>The Streamlit app&rsquo;s title, icon, layout, and sidebar state are configured.</p>
<pre><code># Set Streamlit page configuration
st.set_page_config(
    page_title=name,
    page_icon=&quot;🦙&quot;,
    layout=&quot;centered&quot;,
    initial_sidebar_state=&quot;auto&quot;,
    menu_items=None
)
</code></pre>
<p>OpenAI API key is set using a secret obtained from Streamlit secrets. The key is stored in <code>/.streamlit/secrets.toml</code></p>
<pre><code># Set OpenAI API key
openai.api_key = st.secrets.openai_key
</code></pre>
<p>Create the main interface:
title and information message about the bot&rsquo;s capabilities is configured.</p>
<pre><code># Create main interface
st.title(name)
st.info(info, icon=&quot;📃&quot;)
</code></pre>
<p>A list called messages is initialized in Streamlit session state, which will be used to store the chat history.</p>
<pre><code># Initialize the chat messages history
if &quot;messages&quot; not in st.session_state.keys():
    st.session_state.messages = [
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Ask me a question&quot;}
    ]
</code></pre>
<p>A function called load_data is built, that loads and indexes data from <code>/data</code>. This data is used for responding to user queries.</p>
<pre><code># Function to load data
@st.cache_resource(show_spinner=False) # data is cached in memory so limit the knowledge base according to your machine
def load_data():
    with st.spinner(text=&quot;Loading and indexing the provided data&quot;):
        reader = SimpleDirectoryReader(input_dir=input_dir, recursive=True) # read recursively all directories 
        docs = reader.load_data() # load data and create docs
        service_context = ServiceContext.from_defaults(llm=OpenAI(model=api, temperature=0.5, system_prompt=system_prompt)) # add a permanent service prompt which is added
        index = VectorStoreIndex.from_documents(docs, service_context=service_context) # create your vector database
        return index
</code></pre>
<p>Call the load_data function to load and index the data. Also a chat engine is initialized using the indexed data.</p>
<pre><code># Load data and create the chat engine
index = load_data()
chat_engine = index.as_chat_engine(chat_mode=&quot;condense_question&quot;, verbose=True)
</code></pre>
<p>Check if the user has entered a question through the Streamlit chat input widget. If there is user input, it is appended to the chat history.</p>
<pre><code># User input and chat history
if prompt := st.chat_input(&quot;Your question&quot;):
    st.session_state.messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})
</code></pre>
<p>Loop through the chat history and displays all previous messages in the chat interface.</p>
<pre><code># Display chat history
for message in st.session_state.messages:
    with st.chat_message(message[&quot;role&quot;]):
        st.write(message[&quot;content&quot;])
</code></pre>
<p>Checks if the last message in the chat history is not from the assistant (bot). If it&rsquo;s not from the assistant, a response is generated using the chat engine and added to the chat history.</p>
<pre><code># Generate a response if the last message is not from the assistant
if st.session_state.messages[-1][&quot;role&quot;] != &quot;assistant&quot;:
    with st.chat_message(&quot;assistant&quot;):
        with st.spinner(&quot;Thinking...&quot;):
            response = chat_engine.chat(prompt)
            st.write(response.response)
            message = {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response.response}
            st.session_state.messages.append(message)
</code></pre>
<h1 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h1>
<p><a href="https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/">Build a chatbot with custom data sources, powered by LlamaIndex</a></p>
<p><a href="https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app/secrets-management">Streamlit Secrets management</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/hands-on/">Hands On</a></li>
      <li><a href="https://patrickpt.github.io/tags/ai-design/">AI Design</a></li>
      <li><a href="https://patrickpt.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://patrickpt.github.io/tags/genai/">GenAI</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on twitter"
        href="https://twitter.com/intent/tweet/?text=Hands%20on%20with%20Retrieval%20Augmented%20Generation&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f&amp;hashtags=HandsOn%2cAIDesign%2cLLM%2cGenAI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f&amp;title=Hands%20on%20with%20Retrieval%20Augmented%20Generation&amp;summary=Hands%20on%20with%20Retrieval%20Augmented%20Generation&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f&title=Hands%20on%20with%20Retrieval%20Augmented%20Generation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on whatsapp"
        href="https://api.whatsapp.com/send?text=Hands%20on%20with%20Retrieval%20Augmented%20Generation%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Hands on with Retrieval Augmented Generation on telegram"
        href="https://telegram.me/share/url?text=Hands%20on%20with%20Retrieval%20Augmented%20Generation&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fhands-on-rag%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://patrickpt.github.io/">Notes on AI</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
