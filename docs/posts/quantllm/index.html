<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LLM Quantization in a nutshell | patrickpt.github.io</title>
<meta name="keywords" content="LLM, GenAI">
<meta name="description" content="TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.
Introduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.
Understanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels.">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/quantllm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="LLM Quantization in a nutshell" />
<meta property="og:description" content="TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.
Introduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.
Understanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/quantllm/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-28T09:30:57+00:00" />
<meta property="article:modified_time" content="2024-01-28T09:30:57+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="LLM Quantization in a nutshell"/>
<meta name="twitter:description" content="TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.
Introduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.
Understanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM Quantization in a nutshell",
      "item": "https://patrickpt.github.io/posts/quantllm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM Quantization in a nutshell",
  "name": "LLM Quantization in a nutshell",
  "description": "TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.\nIntroduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model\u0026rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.\nUnderstanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels.",
  "keywords": [
    "LLM", "GenAI"
  ],
  "articleBody": "TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.\nIntroduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model’s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.\nUnderstanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels. This is akin to reducing the bit depth in an audio file, where instead of infinite gradations, you have a limited number of steps. There are different strategies for quantization, such as post-training quantization, dynamic quantization, and quantization-aware training, each with its use cases and trade-offs.\nPost-Training Quantization This approach involves reducing the precision of the weights of a model that has already been fully trained, which can be quickly accomplished without further training. While this method is simple to execute, it may slightly reduce the model’s effectiveness due to a decrease in the precision of the weight values.\nQuantization-Aware Training Contrary to the post-training method, this technique incorporates the lower precision conversion of weights during the model’s training phase. This generally leads to better performance of the model but requires more computational resources. An example of a technique employed within this category is QLoRA.\nOptimization Techniques in Quantization Once the quantization procedure is applied, the size of the model is significantly reduced. However, selecting the appropriate bit width is crucial—too few bits, and you may lose critical information; too many, and you compromise on efficiency gains. It’s also important to choose the right quantization technique. Techniques such as rounding or truncation can affect the performance of the quantized model.\nA Technical Examination Diving Deeper into llama.cpp llama.cpp is a library written in C++ that allows users to perform inference on quantized LLaMA models. It provides tools to convert pre-trained models into a format suitable for inference on CPUs, particularly optimized for low-precision integer arithmetic, which dramatically reduces the computational load.\nQuantization in llama.cpp Using llama.cpp involves specific steps—starting with installation of the library, obtaining a compatible LLaMA model, and then running the provided conversion script to quantize the model. The script typically outputs a new model file, which can be loaded using llama.cpp APIs for inference.\nStore quantized models Quantized models are stored in the binary file format GGML or GGUF. “GG” refers to the initials of its originator (Georgi Gerganov). GGML is a C library created for machine learning, notable for its unique binary format that facilitated the easy distribution of LLMs. This binary format was distinctive to GGML and provided a way to execute LLMs on a CPU, enhancing accessibility and use. GGUF, represents an evolution from the original GGML format. It was developed to be more extensible and future-proof, while being lighter on RAM requirements. GGUF is compatible with the llama.cpp library.\nThe reduced memory footprint with GGML/GGUF and the ability to offload certain layers of computation onto a GPU with GGML/GGUF accelerates inference speeds while managing models that would otherwise be too large for standard VRAM.\nWhere to find models The Hugging Face Hub hosts a variety of pre-quantized models. You can find a collection of models employing diverse quantization techniques, offering choices that cater to specific requirements and scenarios.\nConclusion Advantages of Quantization Beyond the obvious benefits of reduced model size and computational resource requirements, there are other compelling reasons to quantize LLMs. For instance, lower memory bandwidth and power consumption are key factors for deployment on edge devices, and quantization can open up new possibilities for LLM applications in such constrained environments.\nChallenges and Considerations Despite the efficiencies introduced through quantization, one must be mindful of potential accuracy loss. This necessitates a robust evaluation pipeline to assess the performance impact on tasks relevant to the language model’s intended use. Furthermore, leveraging the right tooling and techniques to achieve optimal balance between size, efficiency, and accuracy is fundamental.\nPractical Expertise in Quantized LLMs Quantization of LLMs is a detailed and nuanced process, bringing together theoretical concepts of machine learning and practical aspects of software engineering. With advanced tools like llama.cpp, deployment of these efficient models is increasingly becoming more feasible across various hardware platforms. Mastery of quantization techniques is essential for anyone looking to efficiently deploy LLMs in real-world applications.\nThis synopsis provides an overview of a deeply technical discussion on the subject of LLM quantization, highlighting the principal components.\n",
  "wordCount" : "746",
  "inLanguage": "en",
  "datePublished": "2024-01-28T09:30:57Z",
  "dateModified": "2024-01-28T09:30:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/quantllm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickpt.github.io",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="patrickpt.github.io (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickpt.github.io</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      LLM Quantization in a nutshell
    </h1>
    <div class="post-meta"><span title='2024-01-28 09:30:57 +0000 UTC'>January 28, 2024</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#introduction-to-quantization" aria-label="Introduction to Quantization">Introduction to Quantization</a><ul>
                        
                <li>
                    <a href="#the-technical-foundation-of-llm-quantization" aria-label="The Technical Foundation of LLM Quantization">The Technical Foundation of LLM Quantization</a></li>
                <li>
                    <a href="#understanding-quantization" aria-label="Understanding Quantization">Understanding Quantization</a><ul>
                        
                <li>
                    <a href="#post-training-quantization" aria-label="Post-Training Quantization">Post-Training Quantization</a></li>
                <li>
                    <a href="#quantization-aware-training" aria-label="Quantization-Aware Training">Quantization-Aware Training</a></li></ul>
                </li>
                <li>
                    <a href="#optimization-techniques-in-quantization" aria-label="Optimization Techniques in Quantization">Optimization Techniques in Quantization</a></li></ul>
                </li>
                <li>
                    <a href="#a-technical-examination" aria-label="A Technical Examination">A Technical Examination</a><ul>
                        
                <li>
                    <a href="#diving-deeper-into-llamacpp" aria-label="Diving Deeper into llama.cpp">Diving Deeper into llama.cpp</a></li>
                <li>
                    <a href="#quantization-in-llamacpp" aria-label="Quantization in llama.cpp">Quantization in llama.cpp</a></li>
                <li>
                    <a href="#store-quantized-models" aria-label="Store quantized models">Store quantized models</a></li>
                <li>
                    <a href="#where-to-find-models" aria-label="Where to find models">Where to find models</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a><ul>
                        
                <li>
                    <a href="#advantages-of-quantization" aria-label="Advantages of Quantization">Advantages of Quantization</a></li>
                <li>
                    <a href="#challenges-and-considerations" aria-label="Challenges and Considerations">Challenges and Considerations</a></li>
                <li>
                    <a href="#practical-expertise-in-quantized-llms" aria-label="Practical Expertise in Quantized LLMs">Practical Expertise in Quantized LLMs</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.</p>
<h1 id="introduction-to-quantization">Introduction to Quantization<a hidden class="anchor" aria-hidden="true" href="#introduction-to-quantization">#</a></h1>
<h2 id="the-technical-foundation-of-llm-quantization">The Technical Foundation of LLM Quantization<a hidden class="anchor" aria-hidden="true" href="#the-technical-foundation-of-llm-quantization">#</a></h2>
<p>Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.</p>
<h2 id="understanding-quantization">Understanding Quantization<a hidden class="anchor" aria-hidden="true" href="#understanding-quantization">#</a></h2>
<p>Quantization works by mapping the continuous range of floating-point values to a discrete set of levels. This is akin to reducing the bit depth in an audio file, where instead of infinite gradations, you have a limited number of steps. There are different strategies for quantization, such as post-training quantization, dynamic quantization, and quantization-aware training, each with its use cases and trade-offs.</p>
<h3 id="post-training-quantization">Post-Training Quantization<a hidden class="anchor" aria-hidden="true" href="#post-training-quantization">#</a></h3>
<p>This approach involves reducing the precision of the weights of a model that has already been fully trained, which can be quickly accomplished without further training. While this method is simple to execute, it may slightly reduce the model&rsquo;s effectiveness due to a decrease in the precision of the weight values.</p>
<h3 id="quantization-aware-training">Quantization-Aware Training<a hidden class="anchor" aria-hidden="true" href="#quantization-aware-training">#</a></h3>
<p>Contrary to the post-training method, this technique incorporates the lower precision conversion of weights during the model&rsquo;s training phase. This generally leads to better performance of the model but requires more computational resources. An example of a technique employed within this category is QLoRA.</p>
<h2 id="optimization-techniques-in-quantization">Optimization Techniques in Quantization<a hidden class="anchor" aria-hidden="true" href="#optimization-techniques-in-quantization">#</a></h2>
<p>Once the quantization procedure is applied, the size of the model is significantly reduced. However, selecting the appropriate bit width is crucial—too few bits, and you may lose critical information; too many, and you compromise on efficiency gains. It’s also important to choose the right quantization technique. Techniques such as rounding or truncation can affect the performance of the quantized model.</p>
<h1 id="a-technical-examination">A Technical Examination<a hidden class="anchor" aria-hidden="true" href="#a-technical-examination">#</a></h1>
<h2 id="diving-deeper-into-llamacpp">Diving Deeper into llama.cpp<a hidden class="anchor" aria-hidden="true" href="#diving-deeper-into-llamacpp">#</a></h2>
<p>llama.cpp is a library written in C++ that allows users to perform inference on quantized LLaMA models. It provides tools to convert pre-trained models into a format suitable for inference on CPUs, particularly optimized for low-precision integer arithmetic, which dramatically reduces the computational load.</p>
<h2 id="quantization-in-llamacpp">Quantization in llama.cpp<a hidden class="anchor" aria-hidden="true" href="#quantization-in-llamacpp">#</a></h2>
<p>Using llama.cpp involves specific steps—starting with installation of the library, obtaining a compatible LLaMA model, and then running the provided conversion script to quantize the model. The script typically outputs a new model file, which can be loaded using llama.cpp APIs for inference.</p>
<h2 id="store-quantized-models">Store quantized models<a hidden class="anchor" aria-hidden="true" href="#store-quantized-models">#</a></h2>
<p>Quantized models are stored in the binary file format GGML or GGUF. &ldquo;GG&rdquo; refers to the initials of its originator (Georgi Gerganov).
GGML is a C library created for machine learning, notable for its unique binary format that facilitated the easy distribution of LLMs. This binary format was distinctive to GGML and provided a way to execute LLMs on a CPU, enhancing accessibility and use.
GGUF, represents an evolution from the original GGML format. It was developed to be more extensible and future-proof, while being lighter on RAM requirements.
GGUF is compatible with the llama.cpp library.</p>
<p>The reduced memory footprint with GGML/GGUF and the ability to offload certain layers of computation onto a GPU with GGML/GGUF accelerates inference speeds while managing models that would otherwise be too large for standard VRAM.</p>
<h2 id="where-to-find-models">Where to find models<a hidden class="anchor" aria-hidden="true" href="#where-to-find-models">#</a></h2>
<p>The <a href="https://huggingface.co/models">Hugging Face Hub</a> hosts a variety of pre-quantized models. You can find a collection of models employing diverse quantization techniques, offering choices that cater to specific requirements and scenarios.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<h2 id="advantages-of-quantization">Advantages of Quantization<a hidden class="anchor" aria-hidden="true" href="#advantages-of-quantization">#</a></h2>
<p>Beyond the obvious benefits of reduced model size and computational resource requirements, there are other compelling reasons to quantize LLMs. For instance, lower memory bandwidth and power consumption are key factors for deployment on edge devices, and quantization can open up new possibilities for LLM applications in such constrained environments.</p>
<h2 id="challenges-and-considerations">Challenges and Considerations<a hidden class="anchor" aria-hidden="true" href="#challenges-and-considerations">#</a></h2>
<p>Despite the efficiencies introduced through quantization, one must be mindful of potential accuracy loss. This necessitates a robust evaluation pipeline to assess the performance impact on tasks relevant to the language model&rsquo;s intended use. Furthermore, leveraging the right tooling and techniques to achieve optimal balance between size, efficiency, and accuracy is fundamental.</p>
<h2 id="practical-expertise-in-quantized-llms">Practical Expertise in Quantized LLMs<a hidden class="anchor" aria-hidden="true" href="#practical-expertise-in-quantized-llms">#</a></h2>
<p>Quantization of LLMs is a detailed and nuanced process, bringing together theoretical concepts of machine learning and practical aspects of software engineering. With advanced tools like llama.cpp, deployment of these efficient models is increasingly becoming more feasible across various hardware platforms. Mastery of quantization techniques is essential for anyone looking to efficiently deploy LLMs in real-world applications.</p>
<p>This synopsis provides an overview of a deeply technical discussion on the subject of LLM quantization, highlighting the principal components.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://patrickpt.github.io/tags/genai/">GenAI</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on twitter"
        href="https://twitter.com/intent/tweet/?text=LLM%20Quantization%20in%20a%20nutshell&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f&amp;hashtags=LLM%2cGenAI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f&amp;title=LLM%20Quantization%20in%20a%20nutshell&amp;summary=LLM%20Quantization%20in%20a%20nutshell&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f&title=LLM%20Quantization%20in%20a%20nutshell">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on whatsapp"
        href="https://api.whatsapp.com/send?text=LLM%20Quantization%20in%20a%20nutshell%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LLM Quantization in a nutshell on telegram"
        href="https://telegram.me/share/url?text=LLM%20Quantization%20in%20a%20nutshell&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fquantllm%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://patrickpt.github.io/">patrickpt.github.io</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
