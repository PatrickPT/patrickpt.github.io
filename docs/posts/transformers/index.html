<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Attention is all you need? | patrickschnass.de</title>
<meta name="keywords" content="Transformers, Foundations">
<meta name="description" content="Explore the Transformer Architecture.">
<meta name="author" content="">
<link rel="canonical" href="https://www.patrickschnass.de/posts/transformers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://www.patrickschnass.de/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.patrickschnass.de/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.patrickschnass.de/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.patrickschnass.de/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.patrickschnass.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Attention is all you need?" />
<meta property="og:description" content="Explore the Transformer Architecture." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.patrickschnass.de/posts/transformers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-30T11:50:57+00:00" />
<meta property="article:modified_time" content="2023-08-30T11:50:57+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Attention is all you need?"/>
<meta name="twitter:description" content="Explore the Transformer Architecture."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.patrickschnass.de/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Attention is all you need?",
      "item": "https://www.patrickschnass.de/posts/transformers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention is all you need?",
  "name": "Attention is all you need?",
  "description": "Explore the Transformer Architecture.",
  "keywords": [
    "Transformers", "Foundations"
  ],
  "articleBody": "Transformers: A Deep Dive into the Transformer Architecture Transformers are a revolutionary type of neural network architecture introduced in the seminal paper Attention is All You Need by Vaswani et al. (2017). They have dramatically advanced the field of Natural Language Processing (NLP) and beyond, powering tasks like language translation, text summarization, and even applications in computer vision.\nAn illustrative diagram of the Transformer model architecture.\nIntroduction Before the advent of Transformers, sequence-to-sequence tasks were predominantly handled by Recurrent Neural Networks (RNNs) and their variants (LSTMs and GRUs). While effective in many scenarios, RNNs process input tokens sequentially and struggle with long-range dependencies. Transformers overcome these limitations through the use of attention mechanisms that allow for parallel processing and a more flexible handling of context.\nKey Components of the Transformer 1. Self-Attention Mechanism At the core of the Transformer architecture lies the self-attention mechanism. This mechanism allows each token in the input to dynamically focus on other tokens, thereby capturing contextual relationships regardless of their distance in the sequence.\nThe self-attention computation is defined as:\n[ \\text{Attention}(Q, K, V) = \\text{softmax}!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V ]\n( Q ) (Query), ( K ) (Key), and ( V ) (Value): These matrices are derived from the input embeddings. ( d_k ): The dimension of the key vectors. Scaling Factor: The division by (\\sqrt{d_k}) prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. This formulation enables the model to compute a weighted sum of the values, where the weights are determined by the relevance of each token in the context of others.\n2. Multi-Head Attention Rather than applying a single attention operation, Transformers use multi-head attention. This technique involves splitting the embeddings into several subspaces, applying self-attention in parallel, and then concatenating the results. Each “head” can capture different aspects of the relationships between tokens.\nMulti-head attention enables the model to focus on various types of relationships concurrently.\n3. Positional Encoding Because Transformers process input sequences in parallel, they lack an inherent sense of token order. Positional encodings are added to the input embeddings to inject sequence information. A common method is to use sine and cosine functions at different frequencies:\n[ PE_{(pos, 2i)} = \\sin!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) ] [ PE_{(pos, 2i+1)} = \\cos!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) ]\nHere, ( pos ) indicates the token’s position and ( i ) is the dimension index. These encodings provide the necessary information about the order of tokens.\n4. Encoder-Decoder Structure The original Transformer architecture is divided into two main parts:\nEncoder: Processes the input sequence using layers that include multi-head self-attention and feed-forward networks, each augmented by residual connections and layer normalization.\nThe encoder stack processes the entire input simultaneously, capturing contextual relationships effectively.\nDecoder: Generates the output sequence. It employs masked self-attention (to prevent future tokens from influencing the current prediction) along with encoder-decoder attention, ensuring that the generated output aligns with the input context.\nMathematical Background and Implementation Details Mathematical Insights The mathematical formulations underpinning Transformers are central to their effectiveness:\nScaled Dot-Product: The division by (\\sqrt{d_k}) is essential. Without it, the dot product values could become excessively high for large ( d_k ), pushing the softmax into regions with very small gradients and thus hampering training. Linear Projections: The learned projections for ( Q ), ( K ), and ( V ) allow the model to extract diverse aspects of the input features. When these projections are split across multiple heads, the model learns to capture different patterns and dependencies simultaneously. Practical Implementation Building a Transformer involves several key steps:\nData Preparation:\nTokenization: Convert raw text into tokens. Embedding: Map tokens into continuous vector space. Positional Encoding: Add positional information to the embeddings. Constructing the Model Architecture:\nEncoder Layers: Stack layers that include multi-head self-attention and feed-forward networks, each wrapped with residual connections and layer normalization. Decoder Layers: Stack layers that perform masked self-attention, followed by encoder-decoder attention, and then feed-forward networks. Training the Model:\nLoss Function: Typically, a cross-entropy loss is used for tasks like language translation. Optimization: Modern frameworks like PyTorch and TensorFlow enable efficient training on GPUs/TPUs, capitalizing on the parallelizable nature of Transformers. Fine-Tuning:\nPre-trained Transformer models are often fine-tuned on specific downstream tasks, a process that has led to state-of-the-art performance in many NLP benchmarks. Conclusion Transformers have fundamentally transformed the landscape of machine learning by introducing a mechanism that can effectively capture long-range dependencies while enabling parallel computation. Their versatility and performance have not only pushed the boundaries in NLP but are also inspiring innovations in other fields like computer vision and reinforcement learning.\nAs research continues, the Transformer architecture is likely to be adapted for even broader applications.\nFor those interested in a deeper dive, the original paper Attention is All You Need provides comprehensive insights into the mathematical foundations and design choices that make Transformers so effective.\n",
  "wordCount" : "806",
  "inLanguage": "en",
  "datePublished": "2023-08-30T11:50:57Z",
  "dateModified": "2023-08-30T11:50:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.patrickschnass.de/posts/transformers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickschnass.de",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.patrickschnass.de/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.patrickschnass.de/" accesskey="h" title="patrickschnass.de (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickschnass.de</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.patrickschnass.de/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Attention is all you need?
    </h1>
    <div class="post-meta"><span title='2023-08-30 11:50:57 +0000 UTC'>August 30, 2023</span>&nbsp;·&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#transformers-a-deep-dive-into-the-transformer-architecture" aria-label="Transformers: A Deep Dive into the Transformer Architecture">Transformers: A Deep Dive into the Transformer Architecture</a><ul>
                        
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#key-components-of-the-transformer" aria-label="Key Components of the Transformer">Key Components of the Transformer</a><ul>
                        
                <li>
                    <a href="#1-self-attention-mechanism" aria-label="1. Self-Attention Mechanism">1. Self-Attention Mechanism</a></li>
                <li>
                    <a href="#2-multi-head-attention" aria-label="2. Multi-Head Attention">2. Multi-Head Attention</a></li>
                <li>
                    <a href="#3-positional-encoding" aria-label="3. Positional Encoding">3. Positional Encoding</a></li>
                <li>
                    <a href="#4-encoder-decoder-structure" aria-label="4. Encoder-Decoder Structure">4. Encoder-Decoder Structure</a></li></ul>
                </li>
                <li>
                    <a href="#mathematical-background-and-implementation-details" aria-label="Mathematical Background and Implementation Details">Mathematical Background and Implementation Details</a><ul>
                        
                <li>
                    <a href="#mathematical-insights" aria-label="Mathematical Insights">Mathematical Insights</a></li>
                <li>
                    <a href="#practical-implementation" aria-label="Practical Implementation">Practical Implementation</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="transformers-a-deep-dive-into-the-transformer-architecture">Transformers: A Deep Dive into the Transformer Architecture<a hidden class="anchor" aria-hidden="true" href="#transformers-a-deep-dive-into-the-transformer-architecture">#</a></h1>
<p>Transformers are a revolutionary type of neural network architecture introduced in the seminal paper <a href="https://arxiv.org/abs/1706.03762"><em>Attention is All You Need</em></a> by Vaswani et al. (2017). They have dramatically advanced the field of Natural Language Processing (NLP) and beyond, powering tasks like language translation, text summarization, and even applications in computer vision.</p>
<p><img loading="lazy" src="https://tse3.mm.bing.net/th?id=OIP.qmoSnrlv-dCqCor8Vqr1DQHaKb&amp;pid=Api" alt="Overview of Transformer Architecture"  />

<em>An illustrative diagram of the Transformer model architecture.</em></p>
<hr>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Before the advent of Transformers, sequence-to-sequence tasks were predominantly handled by Recurrent Neural Networks (RNNs) and their variants (LSTMs and GRUs). While effective in many scenarios, RNNs process input tokens sequentially and struggle with long-range dependencies. Transformers overcome these limitations through the use of attention mechanisms that allow for parallel processing and a more flexible handling of context.</p>
<hr>
<h2 id="key-components-of-the-transformer">Key Components of the Transformer<a hidden class="anchor" aria-hidden="true" href="#key-components-of-the-transformer">#</a></h2>
<h3 id="1-self-attention-mechanism">1. Self-Attention Mechanism<a hidden class="anchor" aria-hidden="true" href="#1-self-attention-mechanism">#</a></h3>
<p>At the core of the Transformer architecture lies the <strong>self-attention mechanism</strong>. This mechanism allows each token in the input to dynamically focus on other tokens, thereby capturing contextual relationships regardless of their distance in the sequence.</p>
<p>The self-attention computation is defined as:</p>
<p>[
\text{Attention}(Q, K, V) = \text{softmax}!\left(\frac{QK^T}{\sqrt{d_k}}\right)V
]</p>
<ul>
<li><strong>( Q ) (Query), ( K ) (Key), and ( V ) (Value):</strong> These matrices are derived from the input embeddings.</li>
<li><strong>( d_k ):</strong> The dimension of the key vectors.</li>
<li><strong>Scaling Factor:</strong> The division by (\sqrt{d_k}) prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients.</li>
</ul>
<p>This formulation enables the model to compute a weighted sum of the values, where the weights are determined by the relevance of each token in the context of others.</p>
<h3 id="2-multi-head-attention">2. Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#2-multi-head-attention">#</a></h3>
<p>Rather than applying a single attention operation, Transformers use <strong>multi-head attention</strong>. This technique involves splitting the embeddings into several subspaces, applying self-attention in parallel, and then concatenating the results. Each &ldquo;head&rdquo; can capture different aspects of the relationships between tokens.</p>
<p><img loading="lazy" src="https://tse4.mm.bing.net/th?id=OIP.IEzUbtUeb93u6R7lBjerRwHaFa&amp;pid=Api" alt="Multi-Head Attention"  />

<em>Multi-head attention enables the model to focus on various types of relationships concurrently.</em></p>
<h3 id="3-positional-encoding">3. Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#3-positional-encoding">#</a></h3>
<p>Because Transformers process input sequences in parallel, they lack an inherent sense of token order. <strong>Positional encodings</strong> are added to the input embeddings to inject sequence information. A common method is to use sine and cosine functions at different frequencies:</p>
<p>[
PE_{(pos, 2i)} = \sin!\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
]
[
PE_{(pos, 2i+1)} = \cos!\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
]</p>
<p>Here, ( pos ) indicates the token&rsquo;s position and ( i ) is the dimension index. These encodings provide the necessary information about the order of tokens.</p>
<h3 id="4-encoder-decoder-structure">4. Encoder-Decoder Structure<a hidden class="anchor" aria-hidden="true" href="#4-encoder-decoder-structure">#</a></h3>
<p>The original Transformer architecture is divided into two main parts:</p>
<ul>
<li>
<p><strong>Encoder:</strong> Processes the input sequence using layers that include multi-head self-attention and feed-forward networks, each augmented by residual connections and layer normalization.</p>
<p><img loading="lazy" src="https://tse3.mm.bing.net/th?id=OIP.qmoSnrlv-dCqCor8Vqr1DQHaKb&amp;pid=Api" alt="Encoder Architecture"  />

<em>The encoder stack processes the entire input simultaneously, capturing contextual relationships effectively.</em></p>
</li>
<li>
<p><strong>Decoder:</strong> Generates the output sequence. It employs masked self-attention (to prevent future tokens from influencing the current prediction) along with encoder-decoder attention, ensuring that the generated output aligns with the input context.</p>
</li>
</ul>
<hr>
<h2 id="mathematical-background-and-implementation-details">Mathematical Background and Implementation Details<a hidden class="anchor" aria-hidden="true" href="#mathematical-background-and-implementation-details">#</a></h2>
<h3 id="mathematical-insights">Mathematical Insights<a hidden class="anchor" aria-hidden="true" href="#mathematical-insights">#</a></h3>
<p>The mathematical formulations underpinning Transformers are central to their effectiveness:</p>
<ul>
<li><strong>Scaled Dot-Product:</strong> The division by (\sqrt{d_k}) is essential. Without it, the dot product values could become excessively high for large ( d_k ), pushing the softmax into regions with very small gradients and thus hampering training.</li>
<li><strong>Linear Projections:</strong> The learned projections for ( Q ), ( K ), and ( V ) allow the model to extract diverse aspects of the input features. When these projections are split across multiple heads, the model learns to capture different patterns and dependencies simultaneously.</li>
</ul>
<h3 id="practical-implementation">Practical Implementation<a hidden class="anchor" aria-hidden="true" href="#practical-implementation">#</a></h3>
<p>Building a Transformer involves several key steps:</p>
<ol>
<li>
<p><strong>Data Preparation:</strong></p>
<ul>
<li><strong>Tokenization:</strong> Convert raw text into tokens.</li>
<li><strong>Embedding:</strong> Map tokens into continuous vector space.</li>
<li><strong>Positional Encoding:</strong> Add positional information to the embeddings.</li>
</ul>
</li>
<li>
<p><strong>Constructing the Model Architecture:</strong></p>
<ul>
<li><strong>Encoder Layers:</strong> Stack layers that include multi-head self-attention and feed-forward networks, each wrapped with residual connections and layer normalization.</li>
<li><strong>Decoder Layers:</strong> Stack layers that perform masked self-attention, followed by encoder-decoder attention, and then feed-forward networks.</li>
</ul>
</li>
<li>
<p><strong>Training the Model:</strong></p>
<ul>
<li><strong>Loss Function:</strong> Typically, a cross-entropy loss is used for tasks like language translation.</li>
<li><strong>Optimization:</strong> Modern frameworks like PyTorch and TensorFlow enable efficient training on GPUs/TPUs, capitalizing on the parallelizable nature of Transformers.</li>
</ul>
</li>
<li>
<p><strong>Fine-Tuning:</strong></p>
<ul>
<li>Pre-trained Transformer models are often fine-tuned on specific downstream tasks, a process that has led to state-of-the-art performance in many NLP benchmarks.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Transformers have fundamentally transformed the landscape of machine learning by introducing a mechanism that can effectively capture long-range dependencies while enabling parallel computation. Their versatility and performance have not only pushed the boundaries in NLP but are also inspiring innovations in other fields like computer vision and reinforcement learning.</p>
<p><img loading="lazy" src="https://tse2.mm.bing.net/th?id=OIP.BcYJmgtncDsVjtH_IVL6cgHaEy&amp;pid=Api" alt="Future Directions for Transformers"  />

<em>As research continues, the Transformer architecture is likely to be adapted for even broader applications.</em></p>
<p>For those interested in a deeper dive, the original paper <a href="https://arxiv.org/abs/1706.03762"><em>Attention is All You Need</em></a> provides comprehensive insights into the mathematical foundations and design choices that make Transformers so effective.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.patrickschnass.de/tags/transformers/">Transformers</a></li>
      <li><a href="https://www.patrickschnass.de/tags/foundations/">Foundations</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on twitter"
        href="https://twitter.com/intent/tweet/?text=Attention%20is%20all%20you%20need%3f&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f&amp;hashtags=Transformers%2cFoundations">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f&amp;title=Attention%20is%20all%20you%20need%3f&amp;summary=Attention%20is%20all%20you%20need%3f&amp;source=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f&title=Attention%20is%20all%20you%20need%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on whatsapp"
        href="https://api.whatsapp.com/send?text=Attention%20is%20all%20you%20need%3f%20-%20https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Attention is all you need? on telegram"
        href="https://telegram.me/share/url?text=Attention%20is%20all%20you%20need%3f&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2ftransformers%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://www.patrickschnass.de/">patrickschnass.de</a></span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
