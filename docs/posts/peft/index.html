<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is Parameter Efficient Finetuning? |  </title>
<meta name="keywords" content="LLM, GenAI">
<meta name="description" content="TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/peft/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="What is Parameter Efficient Finetuning?" />
<meta property="og:description" content="TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/peft/" /><meta property="og:image" content="https://patrickpt.github.io/posts/2023_11_01_Parameter_Efficient_Finetuning/images/LoRA.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-01T18:30:57+00:00" />
<meta property="article:modified_time" content="2023-11-01T18:30:57+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://patrickpt.github.io/posts/2023_11_01_Parameter_Efficient_Finetuning/images/LoRA.jpg"/>

<meta name="twitter:title" content="What is Parameter Efficient Finetuning?"/>
<meta name="twitter:description" content="TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is Parameter Efficient Finetuning?",
      "item": "https://patrickpt.github.io/posts/peft/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is Parameter Efficient Finetuning?",
  "name": "What is Parameter Efficient Finetuning?",
  "description": "TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.\nWhy should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.",
  "keywords": [
    "LLM", "GenAI"
  ],
  "articleBody": "TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.\nWhy should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.\nParameter Efficient Fine-Tuning is a collection of techniques that aim to reduce computational and storage resources during the fine-tuning process. Rather than tuning all parameters, these methods strategically select and fine-tune a fraction of them. The rest of the parameters are frozen or updated with a lower precision format, saving memory and computational requirements. This technique also mitigates the risk of overfitting, enhances learning efficiency, and allows for more adaptability in applying large language models across a variety of tasks.\nUse Parameter-Efficient Fine-Tuning The process of parameter-efficient fine-tuning (PEFT) may exhibit variations depending on the specific implementation and the pre-trained model in use. Nevertheless, below are summarized all steps involved in PEFT:\nPre-training: Initially, a large-scale model undergoes pre-training on a substantial dataset, commonly for a generic task like image classification or language modeling. This phase equips the model with foundational knowledge and meaningful data representations. Task-specific dataset: Assemble or generate a dataset tailored to the particular task for which you intend to fine-tune the pre-trained model. This dataset must be labeled and faithfully represent the target task.\nParameter identification: Identify or estimate the significance and relevance of parameters within the pre-trained model for the target task. This step helps in discerning which parameters should be prioritized during the fine-tuning process. Techniques such as importance estimation, sensitivity analysis, or gradient-based methods can be employed for parameter assessment.\nSubset selection: Choose a subset of the pre-trained model’s parameters based on their importance or applicability to the target task. The selection process can involve setting specific criteria, like a threshold on importance scores or selecting the top-k most relevant parameters.\nFine-tuning: Initialize the chosen subset of parameters with values from the pre-trained model and lock the remaining parameters. Fine-tune the selected parameters by employing the task-specific dataset. This typically entails training the model on the target task data using optimization techniques like Stochastic Gradient Descent (SGD) or Adam.\nEvaluation: Assess the performance of the fine-tuned model on a validation set or by utilizing relevant evaluation metrics for the target task. This step serves to gauge the efficacy of PEFT in achieving the desired performance while reducing the number of parameters.\nIterative refinement (optional): Depending on performance and specific requirements, you may opt to iterate and refine the PEFT process. This can involve adjusting the criteria for parameter selection, exploring different subsets, or conducting additional fine-tuning epochs to further optimize the model’s performance. It’s crucial to note that the specific implementation details and techniques employed in PEFT can differ across research papers and real-world applications.\nPEFT Techniques Adapter Modules Adapter modules represent a specialized type of component that can be integrated into pre-trained language models to tailor their hidden representations during the fine-tuning process. These adapters are typically inserted following the multi-head attention and feed-forward layers within the transformer architecture, allowing for selective updates to the adapter parameters while keeping the remainder of the model parameters unchanged.\nIncorporating adapters is a straightforward procedure. The essential steps involve adding adapter modules to each transformer layer and positioning a classifier layer atop the pre-trained model. By updating the parameters of the adapters and the classifier head, one can enhance the pre-trained model’s performance on specific tasks without the need for a comprehensive model update. This approach not only saves time but also conserves computational resources while delivering notable results.\nBut how does fine-tuning using an adapter actually work? The adapter module itself consists of two feed-forward projection layers linked by a non-linear activation layer, and it incorporates a skip connection that bypasses the feed-forward layers.\nConsider an adapter placed immediately after the multi-head attention layer. In this case, the input to the adapter layer is the hidden representation denoted as h' derived from the multi-head attention layer. Within the adapter layer, h follows two distinct paths: one through the skip connection, which leaves the input unaltered, and the other through the feed-forward layers, facilitating the necessary modifications.\npicture from A Guide to PEFT\nTo begin, the first feed-forward layer initially projects h into a lower-dimensional space, which has fewer dimensions than h itself. Subsequently, the input traverses through a non-linear activation function, and the second feed-forward layer then reprojects it back to the same dimensionality as h. The outcomes generated by these two paths are combined through summation to yield the adapter module’s ultimate output.\nThe skip-connection dutifully maintains the original input h of the adapter, while the feed-forward path generates an incremental alteration, denoted as Δh, predicated on the original h. By introducing this incremental change, Δh, acquired from the feed-forward layer, to the original h from the previous layer, the adapter orchestrates a modification to the hidden representation calculated by the pre-trained model. This process empowers the adapter to adapt the pre-trained model’s hidden representation, thereby influencing its output for a specific task.\nLoRA Low-Rank Adaptation (LoRA) in the context of fine-tuning large language models offers an alternative approach for tailoring models to specific tasks or domains. Much like adapters, LoRA is a compact trainable submodule that seamlessly integrates into the transformer architecture. LoRA operation involves preserving the pre-trained model weights while introducing trainable rank decomposition matrices into each layer of the transformer architecture. This process significantly reduces the number of trainable parameters for downstream tasks. Remarkably, LoRA can achieve a reduction of up to 10,000 times in the number of trainable parameters and reduce GPU memory requirements by a factor of 3, all while maintaining or surpassing the performance of fine-tuned models across various tasks. LoRA also facilitates more efficient task-switching, lowers hardware constraints, and incurs no additional inference latency.\nSo, how does LoRA function? LoRA modules are inserted in parallel with the components of the pre-trained transformer model, specifically adjacent to the feed-forward layers. Each feed-forward layer includes two projection layers separated by a non-linear layer, wherein an input vector is transformed into an output vector with different dimensionality using an affine transformation. The LoRA layers are positioned alongside each of the two feed-forward layers.\npicture from A Guide to PEFT\nNow, let’s delve into the interaction between the feed-forward up-project layer and the adjacent LoRA module. The original parameters of the feed-forward layer take the output from the preceding layer, which is represented by dmodel, and project it into dFFW (where FFW stands for feed-forward). The adjacent LoRA module comprises two feed-forward layers. The first of these takes the same input as the feed-forward up-project layer and projects it into an r-dimensional vector, significantly smaller than dmodel. Subsequently, the second feed-forward layer transforms this vector into another vector with a dimensionality of dFFW. The two vectors are then combined through addition to create the final representation.\nAs discussed earlier, fine-tuning involves adjusting the hidden representation h computed by the original transformer model. In this context, the hidden representation generated by the feed-forward up-project layer of the original transformer corresponds to h. Simultaneously, the vector computed by the LoRA module represents the incremental change Δh applied to modify the original h. Consequently, the summation of the original representation and the incremental change yields the updated hidden representation h.\nBy incorporating LoRA modules alongside the feed-forward layers and a classifier head atop the pre-trained model, task-specific parameters for each individual task are kept to a minimum.\nQLoRA Compressing model parameters even more, QLoRA – or Quantized LoRA – takes the concepts of LoRA and applies a quantization technique. Instead of storing these new low-rank parameters in standard floating-point format, QLoRA stores them in a lower precision format, such as INT8 or INT4.\nThis method drastically reduces the number of bits required to store these parameters. As a result, QLoRA offers a significant reduction in the memory and computational requirements, vastly improving parameter efficiency during the fine-tuning process.\nPrefix Tuning Prefix-tuning offers a lightweight and cost-effective alternative to the conventional fine-tuning of large pre-trained language models for natural language generation tasks. Traditional fine-tuning entails the comprehensive updating and storage of all model parameters for each specific task, an endeavor that can be financially burdensome given the expansive scale of contemporary models. In contrast, prefix-tuning maintains the pre-trained language model parameters unchanged and focuses on optimizing a small, continuous, task-specific vector known as the “prefix.” In prefix-tuning, the prefix constitutes a set of independent parameters that undergo training alongside the language model. The primary aim of prefix-tuning is to discover a context that guides the language model to generate text that effectively addresses a particular task.\nThe prefix can be perceived as a sequence of “virtual tokens” to which subsequent tokens can attend. Remarkably, by updating a mere 0.1% of the model’s parameters, prefix-tuning manages to achieve performance comparable to traditional fine-tuning in full-data settings, outperforming it in scenarios with limited data, and exhibiting superior extrapolation to examples featuring topics not encountered during training.\npicture from A Guide to PEFT\nMuch like the aforementioned PEFT techniques, the ultimate objective of prefix-tuning is to attain h prime (h'). Prefix-tuning utilizes these prefixes to adapt the hidden representations derived from the original pre-trained language models. The modification is achieved by adding the incremental change Δh to the initial hidden representation h' resulting in the modified representation h prime (h').\nIn the realm of prefix-tuning, only the prefixes undergo updates, while the remainder of the model’s layers remain static and unchanged. This focused approach allows for enhanced efficiency and economy in addressing natural language generation tasks.\nPrompt Tuning Prompt tuning stands as a potent PEFT technique designed for the precise adaptation of pre-trained language models to specific downstream tasks. Unlike the conventional “model tuning” approach, where every parameter in the pre-trained model undergoes adjustments for each task, prompt tuning introduces the concept of learning soft prompts through backpropagation. These soft prompts can be fine-tuned for distinct tasks by incorporating labeled examples. The results are remarkable, as prompt tuning excels in outperforming few-shot learning methods like GPT-3, particularly gaining competitiveness as model sizes expand. It bolsters the robustness of domain transfer and opens the door to efficient prompt ensembling. Unlike model tuning, which necessitates creating task-specific copies of the entire pre-trained model for each task, prompt tuning simply requires storing a small task-specific prompt for each task. This approach simplifies the process of reusing a single frozen model across a multitude of downstream tasks.\npicture from A Guide to PEFT\nPrompt tuning serves as a simplified variant of prefix tuning. In this method, specific vectors are appended at the commencement of a sequence, specifically at the input layer. When presented with an input sentence, the embedding layer proceeds to convert each token into its corresponding word embedding, while the prefix embeddings are prepended to the sequence of token embeddings. Subsequently, the pre-trained transformer layers process the entire embedding sequence in a manner akin to how a transformer model treats a standard sequence. During the fine-tuning process, only the prefix embeddings undergo adjustments, while the remainder of the transformer model remains locked and unaltered.\nPrompt tuning offers numerous advantages compared to traditional fine-tuning approaches. It significantly enhances efficiency and mitigates computational demands. Furthermore, the exclusive fine-tuning of prefix embeddings minimizes the risk of overfitting to the training data, thereby producing models that are more resilient and capable of generalizing effectively.\nBenefits Cost Savings: PEFT significantly reduces computational and storage costs by fine-tuning a small number of additional model parameters while keeping most of the pre-trained LLM parameters frozen.\nMitigating Knowledge Loss: PEFT effectively addresses the issue of catastrophic forgetting that can occur during full fine-tuning of LLMs, as it updates only a limited set of parameters.\nEnhanced Performance in Low-Data Scenarios: PEFT methods have demonstrated superior performance in situations with limited data, and they exhibit better generalization to out-of-domain scenarios compared to full fine-tuning.\nPortability Advantage: PEFT approaches enable users to obtain compact checkpoints, typically a few megabytes in size, in contrast to the larger checkpoints produced by full fine-tuning. This makes it convenient to deploy and utilize the trained weights from PEFT for various tasks without the need to replace the entire model.\nComparable Performance to Full Fine-Tuning: PEFT allows achieving performance on par with full fine-tuning while utilizing only a small number of trainable parameters.\nConclusion The challenges associated with fine-tuning large language models, such as high memory requirements and computational burden, have prompted researchers to create innovative solutions like LoRa, QLoRa and other parameter efficient fine-tuning methods. By creating a balance between computational power and performance, these techniques make the power of large language models more accessible and applicable to an array of tasks. As natural language processing continues to advance, the importance of such computation-friendly and efficient methods will undoubtedly continue to grow.\nResources https://github.com/huggingface/peft [Repo]\nIntroduction to PEFT [Blogpost]\nA Guide to PEFT [Blogpost]\nA practical guide to PEFT [Blogpost]\n",
  "wordCount" : "2201",
  "inLanguage": "en",
  "datePublished": "2023-11-01T18:30:57Z",
  "dateModified": "2023-11-01T18:30:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/peft/"
  },
  "publisher": {
    "@type": "Organization",
    "name": " ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="  (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg>  </a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      What is Parameter Efficient Finetuning?
    </h1>
    <div class="post-meta"><span title='2023-11-01 18:30:57 +0000 UTC'>November 1, 2023</span>&nbsp;·&nbsp;11 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#why-should-i-care" aria-label="Why should i care?">Why should i care?</a></li>
                <li>
                    <a href="#use-parameter-efficient-fine-tuning" aria-label="Use Parameter-Efficient Fine-Tuning">Use Parameter-Efficient Fine-Tuning</a></li>
                <li>
                    <a href="#peft-techniques" aria-label="PEFT Techniques">PEFT Techniques</a><ul>
                        
                <li>
                    <a href="#adapter-modules" aria-label="Adapter Modules">Adapter Modules</a></li>
                <li>
                    <a href="#lora" aria-label="LoRA">LoRA</a></li>
                <li>
                    <a href="#qlora" aria-label="QLoRA">QLoRA</a></li>
                <li>
                    <a href="#prefix-tuning" aria-label="Prefix Tuning">Prefix Tuning</a></li>
                <li>
                    <a href="#prompt-tuning" aria-label="Prompt Tuning">Prompt Tuning</a></li></ul>
                </li>
                <li>
                    <a href="#benefits" aria-label="Benefits">Benefits</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#resources" aria-label="Resources">Resources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.</p>
<h1 id="why-should-i-care">Why should i care?<a hidden class="anchor" aria-hidden="true" href="#why-should-i-care">#</a></h1>
<p>Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.</p>
<p>Parameter Efficient Fine-Tuning is a collection of techniques that aim to reduce computational and storage resources during the fine-tuning process. Rather than tuning all parameters, these methods strategically select and fine-tune a fraction of them. The rest of the parameters are frozen or updated with a lower precision format, saving memory and computational requirements. This technique also mitigates the risk of overfitting, enhances learning efficiency, and allows for more adaptability in applying large language models across a variety of tasks.</p>
<h1 id="use-parameter-efficient-fine-tuning">Use Parameter-Efficient Fine-Tuning<a hidden class="anchor" aria-hidden="true" href="#use-parameter-efficient-fine-tuning">#</a></h1>
<p>The process of parameter-efficient fine-tuning (PEFT) may exhibit variations depending on the specific implementation and the pre-trained model in use. Nevertheless, below are summarized all steps involved in PEFT:</p>
<p><strong>Pre-training:</strong> Initially, a large-scale model undergoes pre-training on a substantial dataset, commonly for a generic task like image classification or language modeling. This phase equips the model with foundational knowledge and meaningful data representations.
Task-specific dataset: Assemble or generate a dataset tailored to the particular task for which you intend to fine-tune the pre-trained model. This dataset must be labeled and faithfully represent the target task.</p>
<p><strong>Parameter identification:</strong> Identify or estimate the significance and relevance of parameters within the pre-trained model for the target task. This step helps in discerning which parameters should be prioritized during the fine-tuning process. Techniques such as importance estimation, sensitivity analysis, or gradient-based methods can be employed for parameter assessment.</p>
<p><strong>Subset selection:</strong> Choose a subset of the pre-trained model&rsquo;s parameters based on their importance or applicability to the target task. The selection process can involve setting specific criteria, like a threshold on importance scores or selecting the top-k most relevant parameters.</p>
<p><strong>Fine-tuning:</strong> Initialize the chosen subset of parameters with values from the pre-trained model and lock the remaining parameters. Fine-tune the selected parameters by employing the task-specific dataset. This typically entails training the model on the target task data using optimization techniques like Stochastic Gradient Descent (SGD) or Adam.</p>
<p><strong>Evaluation:</strong> Assess the performance of the fine-tuned model on a validation set or by utilizing relevant evaluation metrics for the target task. This step serves to gauge the efficacy of PEFT in achieving the desired performance while reducing the number of parameters.</p>
<p><strong>Iterative refinement (optional):</strong> Depending on performance and specific requirements, you may opt to iterate and refine the PEFT process. This can involve adjusting the criteria for parameter selection, exploring different subsets, or conducting additional fine-tuning epochs to further optimize the model&rsquo;s performance.
It&rsquo;s crucial to note that the specific implementation details and techniques employed in PEFT can differ across research papers and real-world applications.</p>
<h1 id="peft-techniques">PEFT Techniques<a hidden class="anchor" aria-hidden="true" href="#peft-techniques">#</a></h1>
<h2 id="adapter-modules">Adapter Modules<a hidden class="anchor" aria-hidden="true" href="#adapter-modules">#</a></h2>
<p>Adapter modules represent a specialized type of component that can be integrated into pre-trained language models to tailor their hidden representations during the fine-tuning process. These adapters are typically inserted following the multi-head attention and feed-forward layers within the transformer architecture, allowing for selective updates to the adapter parameters while keeping the remainder of the model parameters unchanged.</p>
<p>Incorporating adapters is a straightforward procedure. The essential steps involve adding adapter modules to each transformer layer and positioning a classifier layer atop the pre-trained model. By updating the parameters of the adapters and the classifier head, one can enhance the pre-trained model&rsquo;s performance on specific tasks without the need for a comprehensive model update. This approach not only saves time but also conserves computational resources while delivering notable results.</p>
<p>But how does fine-tuning using an adapter actually work? The adapter module itself consists of two feed-forward projection layers linked by a non-linear activation layer, and it incorporates a skip connection that bypasses the feed-forward layers.</p>
<p>Consider an adapter placed immediately after the multi-head attention layer. In this case, the input to the adapter layer is the hidden representation denoted as <code>h'</code> derived from the multi-head attention layer. Within the adapter layer, <code>h</code> follows two distinct paths: one through the skip connection, which leaves the input unaltered, and the other through the feed-forward layers, facilitating the necessary modifications.</p>
<p><img loading="lazy" src="/posts/2023_11_01_Parameter_Efficient_Finetuning/images/Adapter.jpg" alt="Fine-Tuning Adapters"  />

<em>picture from <a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">A Guide to PEFT</a></em></p>
<p>To begin, the first feed-forward layer initially projects <code>h</code> into a lower-dimensional space, which has fewer dimensions than <code>h</code> itself. Subsequently, the input traverses through a non-linear activation function, and the second feed-forward layer then reprojects it back to the same dimensionality as <code>h</code>. The outcomes generated by these two paths are combined through summation to yield the adapter module&rsquo;s ultimate output.</p>
<p>The skip-connection dutifully maintains the original input <code>h</code> of the adapter, while the feed-forward path generates an incremental alteration, denoted as <code>Δh</code>, predicated on the original <code>h</code>. By introducing this incremental change, <code>Δh</code>, acquired from the feed-forward layer, to the original <code>h</code> from the previous layer, the adapter orchestrates a modification to the hidden representation calculated by the pre-trained model. This process empowers the adapter to adapt the pre-trained model&rsquo;s hidden representation, thereby influencing its output for a specific task.</p>
<h2 id="lora">LoRA<a hidden class="anchor" aria-hidden="true" href="#lora">#</a></h2>
<p>Low-Rank Adaptation (LoRA) in the context of fine-tuning large language models offers an alternative approach for tailoring models to specific tasks or domains. Much like adapters, LoRA is a compact trainable submodule that seamlessly integrates into the transformer architecture. LoRA operation involves preserving the pre-trained model weights while introducing trainable rank decomposition matrices into each layer of the transformer architecture. This process significantly reduces the number of trainable parameters for downstream tasks. Remarkably, LoRA can achieve a reduction of up to 10,000 times in the number of trainable parameters and reduce GPU memory requirements by a factor of 3, all while maintaining or surpassing the performance of fine-tuned models across various tasks. LoRA also facilitates more efficient task-switching, lowers hardware constraints, and incurs no additional inference latency.</p>
<p>So, how does LoRA function? LoRA modules are inserted in parallel with the components of the pre-trained transformer model, specifically adjacent to the feed-forward layers. Each feed-forward layer includes two projection layers separated by a non-linear layer, wherein an input vector is transformed into an output vector with different dimensionality using an affine transformation. The LoRA layers are positioned alongside each of the two feed-forward layers.</p>
<p><img loading="lazy" src="/posts/2023_11_01_Parameter_Efficient_Finetuning/images/LoRA.jpg" alt="Low-Rank Adaptation"  />

<em>picture from <a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">A Guide to PEFT</a></em></p>
<p>Now, let&rsquo;s delve into the interaction between the feed-forward up-project layer and the adjacent LoRA module. The original parameters of the feed-forward layer take the output from the preceding layer, which is represented by <code>dmodel</code>, and project it into <code>dFFW</code> (where <code>FFW</code> stands for feed-forward). The adjacent LoRA module comprises two feed-forward layers. The first of these takes the same input as the feed-forward up-project layer and projects it into an <code>r</code>-dimensional vector, significantly smaller than <code>dmodel</code>. Subsequently, the second feed-forward layer transforms this vector into another vector with a dimensionality of <code>dFFW</code>. The two vectors are then combined through addition to create the final representation.</p>
<p>As discussed earlier, fine-tuning involves adjusting the hidden representation <code>h</code> computed by the original transformer model. In this context, the hidden representation generated by the feed-forward up-project layer of the original transformer corresponds to <code>h</code>. Simultaneously, the vector computed by the LoRA module represents the incremental change <code>Δh</code> applied to modify the original <code>h</code>. Consequently, the summation of the original representation and the incremental change yields the updated hidden representation <code>h</code>.</p>
<p>By incorporating LoRA modules alongside the feed-forward layers and a classifier head atop the pre-trained model, task-specific parameters for each individual task are kept to a minimum.</p>
<h2 id="qlora">QLoRA<a hidden class="anchor" aria-hidden="true" href="#qlora">#</a></h2>
<p>Compressing model parameters even more, QLoRA – or Quantized LoRA – takes the concepts of LoRA and applies a quantization technique. Instead of storing these new low-rank parameters in standard floating-point format, QLoRA stores them in a lower precision format, such as INT8 or INT4.</p>
<p>This method drastically reduces the number of bits required to store these parameters. As a result, QLoRA offers a significant reduction in the memory and computational requirements, vastly improving parameter efficiency during the fine-tuning process.</p>
<h2 id="prefix-tuning">Prefix Tuning<a hidden class="anchor" aria-hidden="true" href="#prefix-tuning">#</a></h2>
<p>Prefix-tuning offers a lightweight and cost-effective alternative to the conventional fine-tuning of large pre-trained language models for natural language generation tasks. Traditional fine-tuning entails the comprehensive updating and storage of all model parameters for each specific task, an endeavor that can be financially burdensome given the expansive scale of contemporary models. In contrast, prefix-tuning maintains the pre-trained language model parameters unchanged and focuses on optimizing a small, continuous, task-specific vector known as the &ldquo;prefix.&rdquo; In prefix-tuning, the prefix constitutes a set of independent parameters that undergo training alongside the language model. The primary aim of prefix-tuning is to discover a context that guides the language model to generate text that effectively addresses a particular task.</p>
<p>The prefix can be perceived as a sequence of &ldquo;virtual tokens&rdquo; to which subsequent tokens can attend. Remarkably, by updating a mere 0.1% of the model&rsquo;s parameters, prefix-tuning manages to achieve performance comparable to traditional fine-tuning in full-data settings, outperforming it in scenarios with limited data, and exhibiting superior extrapolation to examples featuring topics not encountered during training.</p>
<p><img loading="lazy" src="/posts/2023_11_01_Parameter_Efficient_Finetuning/images/Prefix-tuning.jpg" alt="Prefix Tuning"  />

<em>picture from <a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">A Guide to PEFT</a></em></p>
<p>Much like the aforementioned PEFT techniques, the ultimate objective of prefix-tuning is to attain <code>h</code> prime (<code>h'</code>). Prefix-tuning utilizes these prefixes to adapt the hidden representations derived from the original pre-trained language models. The modification is achieved by adding the incremental change <code>Δh</code> to the initial hidden representation <code>h'</code> resulting in the modified representation <code>h</code> prime (<code>h'</code>).</p>
<p>In the realm of prefix-tuning, only the prefixes undergo updates, while the remainder of the model&rsquo;s layers remain static and unchanged. This focused approach allows for enhanced efficiency and economy in addressing natural language generation tasks.</p>
<h2 id="prompt-tuning">Prompt Tuning<a hidden class="anchor" aria-hidden="true" href="#prompt-tuning">#</a></h2>
<p>Prompt tuning stands as a potent PEFT technique designed for the precise adaptation of pre-trained language models to specific downstream tasks. Unlike the conventional &ldquo;model tuning&rdquo; approach, where every parameter in the pre-trained model undergoes adjustments for each task, prompt tuning introduces the concept of learning soft prompts through backpropagation. These soft prompts can be fine-tuned for distinct tasks by incorporating labeled examples. The results are remarkable, as prompt tuning excels in outperforming few-shot learning methods like GPT-3, particularly gaining competitiveness as model sizes expand. It bolsters the robustness of domain transfer and opens the door to efficient prompt ensembling. Unlike model tuning, which necessitates creating task-specific copies of the entire pre-trained model for each task, prompt tuning simply requires storing a small task-specific prompt for each task. This approach simplifies the process of reusing a single frozen model across a multitude of downstream tasks.</p>
<p><img loading="lazy" src="/posts/2023_11_01_Parameter_Efficient_Finetuning/images/Prompt-tuning.jpeg" alt="Prompt Tuning"  />

<em>picture from <a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">A Guide to PEFT</a></em></p>
<p>Prompt tuning serves as a simplified variant of prefix tuning. In this method, specific vectors are appended at the commencement of a sequence, specifically at the input layer. When presented with an input sentence, the embedding layer proceeds to convert each token into its corresponding word embedding, while the prefix embeddings are prepended to the sequence of token embeddings. Subsequently, the pre-trained transformer layers process the entire embedding sequence in a manner akin to how a transformer model treats a standard sequence. During the fine-tuning process, only the prefix embeddings undergo adjustments, while the remainder of the transformer model remains locked and unaltered.</p>
<p>Prompt tuning offers numerous advantages compared to traditional fine-tuning approaches. It significantly enhances efficiency and mitigates computational demands. Furthermore, the exclusive fine-tuning of prefix embeddings minimizes the risk of overfitting to the training data, thereby producing models that are more resilient and capable of generalizing effectively.</p>
<h1 id="benefits">Benefits<a hidden class="anchor" aria-hidden="true" href="#benefits">#</a></h1>
<p><strong>Cost Savings:</strong> PEFT significantly reduces computational and storage costs by fine-tuning a small number of additional model parameters while keeping most of the pre-trained LLM parameters frozen.</p>
<p><strong>Mitigating Knowledge Loss:</strong> PEFT effectively addresses the issue of catastrophic forgetting that can occur during full fine-tuning of LLMs, as it updates only a limited set of parameters.</p>
<p><strong>Enhanced Performance in Low-Data Scenarios:</strong> PEFT methods have demonstrated superior performance in situations with limited data, and they exhibit better generalization to out-of-domain scenarios compared to full fine-tuning.</p>
<p><strong>Portability Advantage:</strong> PEFT approaches enable users to obtain compact checkpoints, typically a few megabytes in size, in contrast to the larger checkpoints produced by full fine-tuning. This makes it convenient to deploy and utilize the trained weights from PEFT for various tasks without the need to replace the entire model.</p>
<p><strong>Comparable Performance to Full Fine-Tuning:</strong> PEFT allows achieving performance on par with full fine-tuning while utilizing only a small number of trainable parameters.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>The challenges associated with fine-tuning large language models, such as high memory requirements and computational burden, have prompted researchers to create innovative solutions like LoRa, QLoRa and other parameter efficient fine-tuning methods. By creating a balance between computational power and performance, these techniques make the power of large language models more accessible and applicable to an array of tasks. As natural language processing continues to advance, the importance of such computation-friendly and efficient methods will undoubtedly continue to grow.</p>
<h1 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h1>
<p><a href="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a> [Repo]</p>
<p><a href="https://towardsdatascience.com/parameter-efficient-fine-tuning-peft-for-llms-a-comprehensive-introduction-e52d03117f95">Introduction to PEFT</a> [Blogpost]</p>
<p><a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">A Guide to PEFT</a> [Blogpost]</p>
<p><a href="https://markovate.com/blog/parameter-efficient-fine-tuning-peft-of-llms-a-practical-guide/">A practical guide to PEFT</a> [Blogpost]</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/llm/">LLM</a></li>
      <li><a href="https://patrickpt.github.io/tags/genai/">GenAI</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on twitter"
        href="https://twitter.com/intent/tweet/?text=What%20is%20Parameter%20Efficient%20Finetuning%3f&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f&amp;hashtags=LLM%2cGenAI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f&amp;title=What%20is%20Parameter%20Efficient%20Finetuning%3f&amp;summary=What%20is%20Parameter%20Efficient%20Finetuning%3f&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f&title=What%20is%20Parameter%20Efficient%20Finetuning%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on whatsapp"
        href="https://api.whatsapp.com/send?text=What%20is%20Parameter%20Efficient%20Finetuning%3f%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Parameter Efficient Finetuning? on telegram"
        href="https://telegram.me/share/url?text=What%20is%20Parameter%20Efficient%20Finetuning%3f&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2fpeft%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://patrickpt.github.io/"> </a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
