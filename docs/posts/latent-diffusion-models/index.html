<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Latent Diffusion Models: What is all the fuzz about? | Notes on AI</title>
<meta name="keywords" content="Math, stable-diffusion, latent-diffusion-models, Notes">
<meta name="description" content="TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.
The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.
Illustrations by Jay Alammar
Sohl-Dickstein et al., 2015">
<meta name="author" content="">
<link rel="canonical" href="https://patrickpt.github.io/posts/latent-diffusion-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://patrickpt.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://patrickpt.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://patrickpt.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://patrickpt.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://patrickpt.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<meta property="og:title" content="Latent Diffusion Models: What is all the fuzz about?" />
<meta property="og:description" content="TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.
The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.
Illustrations by Jay Alammar
Sohl-Dickstein et al., 2015" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://patrickpt.github.io/posts/latent-diffusion-models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-20T14:50:18+01:00" />
<meta property="article:modified_time" content="2023-01-20T14:50:18+01:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Latent Diffusion Models: What is all the fuzz about?"/>
<meta name="twitter:description" content="TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.
The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.
Illustrations by Jay Alammar
Sohl-Dickstein et al., 2015"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://patrickpt.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Latent Diffusion Models: What is all the fuzz about?",
      "item": "https://patrickpt.github.io/posts/latent-diffusion-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Latent Diffusion Models: What is all the fuzz about?",
  "name": "Latent Diffusion Models: What is all the fuzz about?",
  "description": "TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.\nThe following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.\nIllustrations by Jay Alammar\nSohl-Dickstein et al., 2015",
  "keywords": [
    "Math", "stable-diffusion", "latent-diffusion-models", "Notes"
  ],
  "articleBody": "TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.\nThe following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.\nIllustrations by Jay Alammar\nSohl-Dickstein et al., 2015\nHo et al. 2020\nRombach \u0026 Blattmann, et al. 2022\nLilian Weng on Diffusion Models\nSergios Karagiannakos on Diffusion Models\nHugging Face on Annotated Diffusion Model\nIf you recently introduced yourself to strangers and told them about your job in AI, there is a good chance they asked you about the current hype-train around the next generation of generative AI models and related data products like Dall-E, Stable Diffusion or Midjourney.\nLatent Diffusion Models like the ones above had some significant media attention. While no one outside AI community bats an eye if Deepmind creates an algorithm, that beats the (almost ancient) and important Strassen-Algorithm by some percent in computation complexity(which is a tremendous progress), nearly everyone is excited to create made up pictures of cats doing crazy stuff through a simple interface.\nDall-E 2 created picture by author - “A cat surfing a wave in comic style during sunset”\nWhile those models already made a name for themselves by winning art competitions, are adopted by companies into their related data products(Canva.com, Shutterstock.com) and start-ups creating those products raising billions in venture capital you may ask yourself:\nWhat is all the fuzz about? What is behind the hype? What are Latent Diffusion Models? What is the math behind them? Do they impact my life? What is the best way to leverage their power? Let me briefly introduce you to Diffusion Models and Latent Diffusion Models and explain the math.\nIf you are interested in a Hands-On you can find that in my other post:\nHands on Latent Diffusion Models\nIf you want an awesome visual introduction with diagrams i strongly advise to visit the blog by Jay Alammar.\nWhat is this post about: A brand-new category of cutting-edge generative models called diffusion models generate a variety of high-resolution images. They have already received a great deal of attention as a result of OpenAI, Nvidia, and Google’s success in training massive models. We’ll take a deeper look into Denoising Diffusion Probabilistic Models (also known as DDPMs, diffusion models, score-based generative models or simply autoencoders) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include GLIDE and DALL-E 2 by OpenAI, Latent Diffusion by the University of Heidelberg and ImageGen by Google Brain.\nIntuition on Diffusion Models You may ask “What is the intuition behind diffusion models?” Let’s break it down with a short example to make it clear: You are a painter hired by Vatican with the task to repaint the fresco at the ceiling of sixtinian chapel.\nThe requirement is to recreate the fresco with pictures of cats. The requirement is based on the old fresco and the vatican wants to have the same scenes as currently there but witch cats. So you start remembering the fresco and start bringing up a base coat and the old fresco soon becomes a big grey noise.\nMost likely you will be overwhelmed with creating a big fresco and immediately think of structuring your work into smaller chunks. Then you may outline the structures you want to paint based on your memory on the old picture and your vision on the new one.\nMaybe you start with one object like the arm of Adam, from the creation of Adam and then focus on the hand. Gradually you add more details and finally are happy with the scene, decide to finish it and tackle the next part. Still later you may change things after you decided that it fits better with the overall fresco.\nFinally you are building a masterpiece lasting centuries until someone thinks dogs are nicer than cats(so never).\nThe same idea comes with Diffusion: Gradually add noise and create the best representation of the input vision in many small steps. Breaking up the image sampling allows the models to correct itself over those small steps iteratively and produces a good sample.\nUnfortunately nothing is cost-neutral. Like it will cost a painter like Michealangelo almost 4 years to finish the fresco in Sixtinian chapel, the iterative process makes the Models slow at sampling(At least compared to GANs).\nWhat are Diffusion Models? The idea of diffusion for generative modeling was introduced in (Sohl-Dickstein et al., 2015). However, it took until (Song et al., 2019) (at Stanford University), and then (Ho et al., 2020) (at Google Brain) who independently improved the approach. DDPM which we are focussing on originally was introduced in a paper by (Ho et al., 2020).\nAt a high level, they work by first representing the input signal as a set of latent variables, which are then transformed through a series of probabilistic transformations to produce the output signal. The transformation process is designed to smooth out the noise in the input signal and reconstruct a cleaner version of the signal.\nTransformation consist of 2 processes.\nIn a bit more detail for images, the set-up consists of 2 processes:\na fixed (or predefined) forward diffusion process \\(q\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise a learned reverse denoising diffusion process \\(p_\\theta\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. Diffusion Models are basically generative models: Overview of the different types of generative models\nI want to see the math Diffusion In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. E.g. Brownian motion\nWikipedia says:\nA Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\nA continuous-time Markov chain (CTMC) is a continuous stochastic process in which, for each state, the process will change state according to an exponential random variable and then move to a different state as specified by the probabilities of a stochastic matrix\nDiffusion consists of 2 processes:\na fixed (or predefined) forward diffusion process $q$ of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise a learned reverse denoising diffusion process $p_\\theta$, where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: Ho et al. 2020) and Lilian Weng\nForward Diffusion Given a data point from a real data distribution $x_0 \\sim q(x)$ we define a forward diffusion in which we add small Gaussian noise stepwise for $T$ steps producing noisy samples $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$\nStep sizes are controlled by a variance schedule $0 \u003c \\beta_1 \u003c \\beta_2 \u003c … \u003c \\beta_T \u003c 1$.\nIt is defined as\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I})$ with $\\sqrt{1 - \\beta_t} x_{t-1}$ as decay towards origin and\n$\\beta_t \\mathbf{I}$ as the addition of small noise.\nRephrased: A normal distribution (also called Gaussian distribution) is defined by 2 parameters:\na mean $\\mu$ and a variance $\\sigma^2 \\geq 0$. Basically, each new (slightly noisier) image at time step $t$ is drawn from a conditional Gaussian distribution with\n$\\mathbf{\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}$ and $\\sigma^2_t = \\beta_t$, which we can do by sampling $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$ and then setting $\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\mathbf{\\epsilon}$.\nGiven a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an isotropic Gaussian distribution at $t=T$ via a gradual process.\nIsotropic means the probability density is equal (iso) in every direction (tropic). In gaussians this can be achieved with a $\\sigma^2 I$ covariance matrix.\nOne property of the diffusion process is, that you can sample $x_t$ at any time step $t$ using reparameterization trick. Let $\\alpha_{t} = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n$$ \\begin{aligned} \\mathbf{x}_t \u0026= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \u0026 \\text{ ;where } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\ \u0026= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \u0026 \\text{ ;where } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\ \u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2} \u0026 \\text{ ;where } \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\text{ merges two Gaussians.} \\\\ \u0026= \\dots \\\\ \u0026= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\\\ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) \u0026= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I}) \\end{aligned} $$\nSo the sampling of noise and creation of $x_t$ is done in one step only and can be sampled at any timestep.\nReverse Diffusion $q(x_{t-1} \\vert x_t)$ which denotes the Reverse Process is intractable since statistical estimates of it require computations involving the entire dataset and therefore we need to learn a model $p_0$ to approximate these conditional probabilities in order to run the reverse diffusion process.\nWe need to learn a model $p_0$ to approximate these conditional probabilities\nSince $q(x_{t-1} \\vert x_t)$ will also be Gaussian, for small enough $\\beta_t$, we can choose $p_0$ to be Gaussian and just parameterize the mean and variance as the Reverse Diffusion:\n$\\quad p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N} (\\mathbf{x}_{t-1}; {\\mu}_\\theta(\\mathbf{x}_t, t), {\\Sigma}_\\theta(\\mathbf{x}_t, t)) $\nwith $\\mu_\\theta(x_{t},t)$ as the mean and $\\Sigma_\\theta (x_{t},t)$ as the variance\nconditioned on the noise level $t$ as the to be learned functions of drift and covariance of the Gaussians(by a Neural Net).\nAs the target image is already defined the problem can be described as a supervised learning problem.\nAn example of training a diffusion model for modeling a 2D swiss roll data. (Image source: Sohl-Dickstein et al., 2015)\nHence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to keep the variance fixed, and let the neural network only learn (represent) the mean $\\mu_\\theta$ of this conditional probability distribution.\nOptimization of the Loss Function To derive an objective function to learn the mean of the backward process, the authors observe that the combination of $q$ and $p_\\theta$ can be seen as a variational auto-encoder (VAE) (Kingma et al., 2013).\nA Diffusion Model can be trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood\n$- \\log p_\\theta(\\mathbf{x}_0)$.\nAfter transformation(Lilian Weng for reference), we can write the evidence lower bound (ELBO) as follows:\n$$ \\begin{aligned} - \\log p_\\theta(\\mathbf{x}_0) \u0026\\leq - \\log p_\\theta(\\mathbf{x}_0)+ D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\vert p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) ) \\end{aligned} $$\nIntuition on the optimization: For a function $f(x)$, which can’t be computed(like e.g. the above negative log-likelihood) and have also a function $g(x)$, which we can compute and fullfills the condition $g(x) \u003c= f(x)$. If we then maximize $g(x)$ we can be certain that $f(x)$ will also increase.\nFor optimization we use Kullback-Leibler (KL) Divergences. The KL Divergence is a statistical distance measure of how much one probability distribution $P$ differs from a reference distribution $Q$.\nWe are interested in formulating the Loss function in terms of KL divergences because the transition distributions in our Markov chain are Gaussians, and the KL divergence between Gaussians has a closed form. For a closer look please look here\nIf we rewrite the above Loss function and apply the bayesian rule the upper term can be summarized to a joint probability and will be trainsformed to the Variational Lower Bound:\n$$ \\begin{aligned} \u0026= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{\\mathbf{x}_{1:T}\\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\ \u0026= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \\text{Let }L_\\text{VLB} \u0026= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\geq - \\mathbb{E}_{q(\\mathbf{x}_0)} \\log p_\\theta(\\mathbf{x}_0) \\end{aligned} $$\nComplete calculation can be found here together with a really nice explanation here\nThe objective can be further rewritten to be a combination of several KL-divergence and entropy terms(Detailed process in Appendix B in Sohl-Dickstein et al., 2015)\n$$ \\begin{aligned} L_\\text{VLB} \u0026= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \u0026= \\dots \\\\ \u0026= \\mathbb{E}_q [\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} + \\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ] \\end{aligned} $$\nReshaped:\n$$ \\begin{aligned} L_\\text{VLB} \u0026= L_T + L_{T-1} + \\dots + L_0 \\\\ \\text{where } L_T \u0026= D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T)) \\\\ L_t \u0026= D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1 \\\\ L_0 \u0026= - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1) \\end{aligned} $$\nEvery KL term in $L_\\text{VLB}$ except for $L_0$ compares two Gaussian distributions and therefore they can be computed in closed form. $L_T$ is constant and can be ignored during training because $q$ has no learnable parameters and $x_T$ is a Gaussian noise. $L_t$ formulates the difference between the desired denoising steps and the approximated ones.\nIt is evident that through the ELBO, maximizing the likelihood boils down to learning the denoising steps $L_t$.\nWe would like to train $\\boldsymbol{\\mu}_\\theta$ to predict $\\tilde{\\boldsymbol{\\mu}}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)$. Because $\\mathbf{x}_t$ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict $\\boldsymbol{\\epsilon}_t$ from the input $\\mathbf{x}_t$ at time step $t$:\n$$ \\begin{aligned} \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \u0026= \\color{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)} \\\\ \\text{Thus }\\mathbf{x}_{t-1} \u0026= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) \\end{aligned} $$\nThe loss term $L_t$ is parameterized to minimize the difference from $\\tilde\\mu$ :\n$$ \\begin{aligned} L_t \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 | \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) |^2_2} | \\color{blue}{\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} |^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 |\\boldsymbol{\\Sigma}_\\theta |^2_2} | \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big)} |^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) | \\boldsymbol{\\Sigma}_\\theta |^2_2} |\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)|^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) | \\boldsymbol{\\Sigma}_\\theta |^2_2} |\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)|^2 \\Big] \\end{aligned} $$\nThe final objective function $L_t$ then looks as follows (for a random time step $t$ given $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ ) as shown by Ho et al. (2020)\n$$ | \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) |^2 = | \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t) } \\mathbf{\\epsilon}, t) |^2.$$\nHere, $\\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\\mathbf{\\epsilon}$ is the pure noise sampled at time step $t$, and $\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\nHere, $\\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\\mathbf{\\epsilon}$ is the pure noise sampled at time step $t$, and $\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\nThe training algorithm now looks as follows:\nIn other words:\nwe take a random sample $\\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\\mathbf{x}_0)$ we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step) we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ (using the nice property defined above) the neural network is trained to predict this noise based on the corrupted image $\\mathbf{x}_t$ (i.e. noise applied on $\\mathbf{x}_0$ based on known schedule $\\beta_t$ ) Neural Nets The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?\nWhat is typically used here is very similar to that of an Autoencoder, which you may remember from typical “intro to deep learning” tutorials. Autoencoders have a so-called “bottleneck” layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the “bottleneck”, and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\nIn terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).\nThe math on Latent Diffusion It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as can be up to one or a few thousand steps. One data point from Song et al. 2020: “For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.”\nLatent diffusion model (LDM; Rombach \u0026 Blattmann, et al. 2022) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.\nIt is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression.\nLDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.\nThe perceptual compression process relies on an autoencoder model.\nAn encoder $\\mathcal{E}$ is used to compress the input image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times 3}$ to a smaller 2D latent vector $\\mathbf{z} = \\mathcal{E}(\\mathbf{x}) \\in \\mathbb{R}^{h \\times w \\times c}$ , where the downsampling rate $f=H/h=W/w=2^m, m \\in \\mathbb{N}$.\nThen an decoder $\\mathcal{D}$ reconstructs the images from the latent vector, $\\tilde{\\mathbf{x}} = \\mathcal{D}(\\mathbf{z})$.\nThe paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.\nKL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to VAE. VQ-reg: Uses a vector quantization layer within the decoder, like VQVAE but the quantization layer is absorbed by the decoder. The diffusion and denoising processes happen on the latent vector $\\mathbf{z}$. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image).\nThe design is equivalent to fuse representation of different modality into the model with cross-attention mechanism.\nEach type of conditioning information is paired with a domain-specific encoder $\\tau_\\theta$ to project the conditioning input $y$ to an intermediate representation that can be mapped into cross-attention component, $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_\\tau}$:\n$$ \\begin{aligned} \u0026\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\Big(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d}}\\Big) \\cdot \\mathbf{V} \\\\ \u0026\\text{where }\\mathbf{Q} = \\mathbf{W}^{(i)}_Q \\cdot \\varphi_i(\\mathbf{z}_i),; \\mathbf{K} = \\mathbf{W}^{(i)}_K \\cdot \\tau_\\theta(y),; \\mathbf{V} = \\mathbf{W}^{(i)}_V \\cdot \\tau_\\theta(y) \\\\ \u0026\\text{and } \\mathbf{W}^{(i)}_Q \\in \\mathbb{R}^{d \\times d^i_\\epsilon},; \\mathbf{W}^{(i)}_K, \\mathbf{W}^{(i)}_V \\in \\mathbb{R}^{d \\times d_\\tau},; \\varphi_i(\\mathbf{z}_i) \\in \\mathbb{R}^{N \\times d^i_\\epsilon},; \\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_\\tau} \\end{aligned} $$\nPicture from J. Rafid Siddiqui\nAnd what is the result? Ressources The Annotated Diffusion Model\nHow does Stable Diffusion work? – Latent Diffusion Models EXPLAINED [Video]\nStable Diffusion - What, Why, How? [Video]\nStable Diffusion videos from fast.ai [Video]\nIllustrations by Jay Alammar Sohl-Dickstein et al., 2015\nRombach \u0026 Blattmann, et al. 2022\nHo et al. 2020\nLilian Weng on Diffusion Models\nSergios Karagiannakos on Diffusion Models\nHugging Face on Annotated Diffusion Model\nAssembly AI on Diffusion\nJ. Rafid Siddiqui on Latent Diffusion\n",
  "wordCount" : "3465",
  "inLanguage": "en",
  "datePublished": "2023-01-20T14:50:18+01:00",
  "dateModified": "2023-01-20T14:50:18+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://patrickpt.github.io/posts/latent-diffusion-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Notes on AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://patrickpt.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://patrickpt.github.io/" accesskey="h" title="Notes on AI (Alt + H)">Notes on AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://patrickpt.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/recent/" title="Recent">
                    <span>Recent</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://patrickpt.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Latent Diffusion Models: What is all the fuzz about?
    </h1>
    <div class="post-meta"><span title='2023-01-20 14:50:18 +0100 CET'>January 20, 2023</span>&nbsp;·&nbsp;17 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#what-is-this-post-about" aria-label="What is this post about:">What is this post about:</a></li>
                <li>
                    <a href="#intuition-on-diffusion-models" aria-label="Intuition on Diffusion Models">Intuition on Diffusion Models</a></li>
                <li>
                    <a href="#what-are-diffusion-models" aria-label="What are Diffusion Models?">What are Diffusion Models?</a></li>
                <li>
                    <a href="#i-want-to-see-the-math" aria-label="I want to see the math">I want to see the math</a><ul>
                        
                <li>
                    <a href="#diffusion" aria-label="Diffusion">Diffusion</a></li>
                <li>
                    <a href="#forward-diffusion" aria-label="Forward Diffusion">Forward Diffusion</a></li>
                <li>
                    <a href="#reverse-diffusion" aria-label="Reverse Diffusion">Reverse Diffusion</a></li>
                <li>
                    <a href="#optimization-of-the-loss-function" aria-label="Optimization of the Loss Function">Optimization of the Loss Function</a></li></ul>
                </li>
                <li>
                    <a href="#neural-nets" aria-label="Neural Nets">Neural Nets</a></li>
                <li>
                    <a href="#the-math-on-latent-diffusion" aria-label="The math on Latent Diffusion">The math on Latent Diffusion</a></li>
                <li>
                    <a href="#and-what-is-the-result" aria-label="And what is the result?">And what is the result?</a></li>
                <li>
                    <a href="#ressources" aria-label="Ressources">Ressources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>The following learning notes try to give some intuition on how <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a> works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.</p>
<p><strong>The following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.</strong></p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/">Illustrations by Jay Alammar</a></p>
<p><a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a></p>
<p><a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a></p>
<p><a href="https://arxiv.org/abs/2112.10752">Rombach &amp; Blattmann, et al. 2022</a></p>
<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng on Diffusion Models</a></p>
<p><a href="https://theaisummer.com/diffusion-models/">Sergios Karagiannakos on Diffusion Models</a></p>
<p><a href="https://huggingface.co/blog/annotated-diffusion">Hugging Face on Annotated Diffusion Model</a></p>
<p>If you recently introduced yourself to strangers and told them about your job in AI, there is a good chance they asked you about the current hype-train around the next generation of generative AI models and related data products like Dall-E, Stable Diffusion or Midjourney.</p>
<p>Latent Diffusion Models like the ones above had some significant media attention. While no one outside AI community bats an eye if Deepmind creates an algorithm, that beats the (almost ancient) and important Strassen-Algorithm by some percent in computation complexity(which is a tremendous progress), nearly everyone is excited to create made up pictures of cats doing crazy stuff through a simple interface.</p>
<p><a href="/posts/2023_01_11_latent_diffusion_models/images/cat.png"><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/cat.png" alt="cat"  />
</a>
<em>Dall-E 2 created picture by author - &ldquo;A cat surfing a wave in comic style during sunset&rdquo;</em></p>
<p>While those models already made a name for themselves by winning <a href="https://news.artnet.com/art-world/colorado-artists-mad-ai-art-competition-2168495">art competitions</a>, are adopted by companies into their related data products(Canva.com, Shutterstock.com) and start-ups creating those products raising billions in <a href="https://www.bloomberg.com/news/articles/2022-10-17/digital-media-firm-stability-ai-raises-funds-at-1-billion-value">venture capital</a> you may ask yourself:</p>
<ul>
<li>What is all the fuzz about? </li>
<li>What is behind the hype? What are Latent Diffusion Models? </li>
<li>What is the math behind them? </li>
<li>Do they impact my life? What is the best way to leverage their power?  </li>
</ul>
<p>Let me briefly introduce you to Diffusion Models and Latent Diffusion Models and explain the math.</p>
<p>If you are interested in a Hands-On you can find that in my other post:</p>
<p><a href="/posts/hands-on-latent-diffusion-models">Hands on Latent Diffusion Models</a></p>
<p>If you want an awesome visual introduction with diagrams i strongly advise to visit the <a href="https://jalammar.github.io/illustrated-stable-diffusion/">blog by Jay Alammar</a>.</p>
<h1 id="what-is-this-post-about">What is this post about:<a hidden class="anchor" aria-hidden="true" href="#what-is-this-post-about">#</a></h1>
<p>A brand-new category of cutting-edge generative models called diffusion models generate a variety of high-resolution images.
They have already received a great deal of attention as a result of OpenAI, Nvidia, and Google&rsquo;s success in training massive models.
We&rsquo;ll take a deeper look into <strong>Denoising Diffusion Probabilistic Models</strong> (also known as DDPMs, diffusion models, score-based generative models or simply <a href="https://benanne.github.io/2022/01/31/diffusion.html">autoencoders</a>) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include <a href="https://arxiv.org/abs/2112.10741">GLIDE</a> and <a href="https://openai.com/dall-e-2/">DALL-E 2</a> by OpenAI, <a href="https://github.com/CompVis/latent-diffusion">Latent Diffusion</a> by the University of Heidelberg and <a href="https://imagen.research.google/">ImageGen</a> by Google Brain.</p>
<h1 id="intuition-on-diffusion-models">Intuition on Diffusion Models<a hidden class="anchor" aria-hidden="true" href="#intuition-on-diffusion-models">#</a></h1>
<p>You may ask &ldquo;What is the intuition behind diffusion models?&rdquo;
Let&rsquo;s break it down with a short example to make it clear:
You are a painter hired by Vatican with the task to repaint the fresco at the ceiling of <a href="https://de.wikipedia.org/wiki/Sixtinische_Kapelle#/media/Datei:CAPPELLA_SISTINA_Ceiling.jpg">sixtinian chapel</a>.</p>
<p>The requirement is to recreate the fresco with pictures of cats. The requirement is based on the old fresco and the vatican wants to have the same scenes as currently there but witch cats.
So you start remembering the fresco and start bringing up a base coat and the old fresco soon becomes a big grey noise.</p>
<p>Most likely you will be overwhelmed with creating a big fresco and immediately think of structuring your work into smaller chunks.
Then you may outline the structures you want to paint based on your memory on the old picture and your vision on the new one.</p>
<p>Maybe you start with one object like the arm of Adam, from <a href="https://en.wikipedia.org/wiki/The_Creation_of_Adam">the creation of Adam</a> and then focus on the hand. Gradually you add more details and finally are happy with the scene, decide to finish it and tackle the next part. Still later you may change things after you decided that it fits better with the overall fresco.</p>
<p>Finally you are building a masterpiece lasting centuries until someone thinks dogs are nicer than cats(so never).</p>
<p>The same idea comes with Diffusion: Gradually add noise and create the best representation of the input vision in many small steps. Breaking up the image sampling allows the models to correct itself over those small steps iteratively and produces a good sample.</p>
<p>Unfortunately nothing is cost-neutral. Like it will cost a painter like Michealangelo almost 4 years to finish the fresco in Sixtinian chapel, the iterative process makes the Models slow at sampling(At least compared to GANs).</p>
<h1 id="what-are-diffusion-models">What are Diffusion Models?<a hidden class="anchor" aria-hidden="true" href="#what-are-diffusion-models">#</a></h1>
<p>The idea of diffusion for generative modeling was introduced in (<a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>). However, it took until (<a href="https://arxiv.org/abs/1907.05600">Song et al., 2019</a>) (at Stanford University), and then (<a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>) (at Google Brain) who independently improved the approach.
DDPM  which we are focussing on originally was introduced in a paper by (<a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>).</p>
<p>At a high level, they work by first representing the input signal as a set of latent variables, which are then transformed through a series of probabilistic transformations to produce the output signal. The transformation process is designed to smooth out the noise in the input signal and reconstruct a cleaner version of the signal.</p>
<p>Transformation consist of 2 processes.</p>
<p>In a bit more detail for images, the set-up consists of 2 processes:</p>
<ul>
<li>a fixed (or predefined) forward diffusion process \(q\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise</li>
<li>a learned reverse denoising diffusion process \(p_\theta\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.</li>
</ul>
<p>Diffusion Models are basically generative models:
<a href="/posts/2023_01_11_latent_diffusion_models/images/types_gans.png"><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/types_gans.png" alt="types_gans"  />
</a>
<em>Overview of the different types of generative <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/HowdoestheDiffusionprocesswork?">models</a></em></p>
<h1 id="i-want-to-see-the-math">I want to see the math<a hidden class="anchor" aria-hidden="true" href="#i-want-to-see-the-math">#</a></h1>
<h2 id="diffusion">Diffusion<a hidden class="anchor" aria-hidden="true" href="#diffusion">#</a></h2>
<!-- raw HTML omitted -->
<p>In probability theory and statistics, diffusion processes are a class of continuous-time <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov</a> process with almost surely continuous sample paths. E.g. <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a></p>
<p>Wikipedia says:</p>
<ul>
<li>
<p>A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p>
</li>
<li>
<p>A continuous-time Markov chain (CTMC) is a continuous stochastic process in which, for each state, the process will change state according to an exponential random variable and then move to a different state as specified by the probabilities of a stochastic matrix</p>
</li>
</ul>
<p>Diffusion consists of 2 processes:</p>
<ul>
<li>a fixed (or predefined) forward diffusion process $q$ of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise</li>
<li>a learned reverse denoising diffusion process $p_\theta$, where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.</li>
</ul>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/DDPM_pres.png" alt="DDPM"  />

<em>The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: Ho et al. 2020) and <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng</a></em></p>
<h2 id="forward-diffusion">Forward Diffusion<a hidden class="anchor" aria-hidden="true" href="#forward-diffusion">#</a></h2>
<p>Given a data point from a real data distribution $x_0 \sim q(x)$ we define a
<strong>forward diffusion</strong> in which we add small Gaussian noise stepwise for $T$ steps
producing noisy samples $\mathbf{x}_1, \dots, \mathbf{x}_T$</p>
<p>Step sizes are controlled by a variance schedule $0 &lt; \beta_1 &lt; \beta_2 &lt; &hellip; &lt; \beta_T &lt; 1$.</p>
<p>It is defined as</p>
<p>$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t \mathbf{I})$
with
$\sqrt{1 - \beta_t} x_{t-1}$ as decay towards origin and</p>
<p>$\beta_t \mathbf{I}$ as the addition of small noise.</p>
<p>Rephrased:
A normal distribution (also called Gaussian distribution) is defined by 2 parameters:</p>
<ul>
<li>a mean $\mu$ and</li>
<li>a variance $\sigma^2 \geq 0$.</li>
</ul>
<p>Basically, each new (slightly noisier) image at time step $t$ is drawn from a <strong>conditional Gaussian distribution</strong> with</p>
<ul>
<li>$\mathbf{\mu}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}$ and</li>
<li>$\sigma^2_t = \beta_t$,</li>
</ul>
<p>which we can do by sampling $\mathbf{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ and then setting $\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} +  \sqrt{\beta_t} \mathbf{\epsilon}$.</p>
<p>Given a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an <a href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic">isotropic Gaussian distribution</a> at $t=T$ via a gradual process.</p>
<p>Isotropic means the probability density is equal (iso) in every direction (tropic). In gaussians this can be achieved with a $\sigma^2 I$ covariance matrix.</p>
<p>One property of the diffusion process is, that you can sample $x_t$ at any time step $t$ using <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick">reparameterization trick</a>.
Let $\alpha_{t} = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$</p>
<p>$$
\begin{aligned}
\mathbf{x}_t
&amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp; \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp;= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} &amp; \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} &amp; \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians.} \\
&amp;= \dots \\
&amp;= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}\\
q(\mathbf{x}_t \vert \mathbf{x}_0)
&amp;= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{aligned}
$$</p>
<p>So the sampling of noise and creation of $x_t$ is done in one step only and can be sampled at any timestep.</p>
<h2 id="reverse-diffusion">Reverse Diffusion<a hidden class="anchor" aria-hidden="true" href="#reverse-diffusion">#</a></h2>
<p>$q(x_{t-1} \vert x_t)$ which denotes the Reverse Process is intractable since statistical estimates of it require computations involving the entire dataset and therefore we need to learn a model $p_0$ to approximate these conditional probabilities in order to run the reverse diffusion process.</p>
<p>We need to learn a model $p_0$ to approximate these conditional probabilities</p>
<p>Since $q(x_{t-1} \vert x_t)$ will also be Gaussian, for small enough $\beta_t$, we can choose $p_0$ to be Gaussian and just parameterize the mean and variance as the <strong>Reverse Diffusion</strong>:</p>
<p>$\quad p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N} (\mathbf{x}_{t-1}; {\mu}_\theta(\mathbf{x}_t, t), {\Sigma}_\theta(\mathbf{x}_t, t))
$</p>
<p>with
$\mu_\theta(x_{t},t)$ as the mean and
$\Sigma_\theta (x_{t},t)$ as the variance</p>
<p>conditioned on the noise level $t$ as the to be learned functions of drift and covariance of the Gaussians(by a Neural Net).</p>
<p>As the target image is already defined the problem can be described as a supervised learning problem.</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/diffusion-example_pres.png" alt=""  />
</p>
<p><em>An example of training a diffusion model for modeling a 2D swiss roll data. (Image source: Sohl-Dickstein et al., 2015)</em></p>
<p>Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to <strong>keep the variance fixed, and let the neural network only learn (represent) the mean $\mu_\theta$ of this conditional probability distribution</strong>.</p>
<h2 id="optimization-of-the-loss-function">Optimization of the Loss Function<a hidden class="anchor" aria-hidden="true" href="#optimization-of-the-loss-function">#</a></h2>
<p>To derive an objective function to learn the mean of the backward process, the authors observe that the combination of $q$ and $p_\theta$ can be seen as a variational auto-encoder (VAE) <a href="https://arxiv.org/abs/1312.6114">(Kingma et al., 2013)</a>.</p>
<p>A Diffusion Model can be trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood</p>
<p>$- \log p_\theta(\mathbf{x}_0)$.</p>
<p>After transformation(<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng</a> for reference), we can write the evidence lower bound (ELBO) as follows:</p>
<p>$$
\begin{aligned}
- \log p_\theta(\mathbf{x}_0)
&amp;\leq - \log p_\theta(\mathbf{x}_0)+ D_\text{KL}(q(\mathbf{x}_{1:T}\vert\mathbf{x}_0) \vert p_\theta(\mathbf{x}_{1:T}\vert\mathbf{x}_0) )
\end{aligned}
$$</p>
<p>Intuition on the optimization:
For a function $f(x)$, which can&rsquo;t be computed(like e.g. the above negative log-likelihood) and have also a function $g(x)$, which we can compute and fullfills the condition $g(x) &lt;= f(x)$. If we then maximize $g(x)$ we can be certain that $f(x)$ will also increase.</p>
<p>For optimization we use <strong>Kullback-Leibler (KL) Divergences</strong>.
The KL Divergence is a statistical distance measure of how much one probability distribution $P$ differs from a reference distribution $Q$.</p>
<p>We are interested in formulating the Loss function in terms of KL divergences because the transition distributions in our Markov chain are Gaussians, and the KL divergence between Gaussians has a closed form.
For a closer look please look <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">here</a></p>
<p>If we rewrite the above Loss function and apply the bayesian rule the upper term can be summarized to a joint probability and will be trainsformed to the Variational Lower Bound:</p>
<p>$$
\begin{aligned}
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_{\mathbf{x}_{1:T}\sim q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T}) / p_\theta(\mathbf{x}_0)} \Big] \\
&amp;= -\log p_\theta(\mathbf{x}_0) + \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} + \log p_\theta(\mathbf{x}_0) \Big] \\
&amp;= \mathbb{E}_q \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
\text{Let }L_\text{VLB}
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log \frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \geq - \mathbb{E}_{q(\mathbf{x}_0)} \log p_\theta(\mathbf{x}_0)
\end{aligned}
$$</p>
<p>Complete calculation can be found <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">here</a> together with a really nice explanation <a href="https://www.youtube.com/watch?v=HoKDTa5jHvg&amp;t=905s">here</a></p>
<p>The objective can be further rewritten to be a combination of several KL-divergence and entropy terms(Detailed process in Appendix B in <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>)</p>
<p>$$
\begin{aligned}
L_\text{VLB}
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\
&amp;= \dots \\
&amp;= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]
\end{aligned}
$$</p>
<p>Reshaped:</p>
<p>$$
\begin{aligned}
L_\text{VLB} &amp;= L_T + L_{T-1} + \dots + L_0 \\
\text{where } L_T &amp;= D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T)) \\
L_t &amp;= D_\text{KL}(q(\mathbf{x}_t \vert \mathbf{x}_{t+1}, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_t \vert\mathbf{x}_{t+1})) \text{ for }1 \leq t \leq T-1 \\
L_0 &amp;= - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)
\end{aligned}
$$</p>
<p>Every KL term in $L_\text{VLB}$ except for $L_0$ compares two Gaussian distributions and therefore they can be computed in closed form. $L_T$ is constant and can be ignored during training because $q$ has no learnable parameters and $x_T$ is a Gaussian noise. $L_t$ formulates the difference between the desired denoising steps and the approximated ones.</p>
<p>It is evident that through the ELBO, maximizing the likelihood boils down to learning the denoising steps $L_t$.</p>
<p>We would like to train $\boldsymbol{\mu}_\theta$ to predict $\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)$.
Because $\mathbf{x}_t$ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict $\boldsymbol{\epsilon}_t$ from the input  $\mathbf{x}_t$ at time step $t$:</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &amp;= \color{red}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\
\text{Thus }\mathbf{x}_{t-1} &amp;= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\end{aligned}
$$</p>
<p>The loss term $L_t$ is parameterized to minimize the difference from $\tilde\mu$ :</p>
<p>$$
\begin{aligned}
L_t
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2 | \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) |^2_2} | \color{blue}{\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)} - \color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} |^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2  |\boldsymbol{\Sigma}_\theta |^2_2} | \color{blue}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)} - \color{green}{\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, t) \Big)} |^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) | \boldsymbol{\Sigma}_\theta |^2_2} |\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) | \boldsymbol{\Sigma}_\theta |^2_2} |\boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)|^2 \Big]
\end{aligned}
$$</p>
<p>The final objective function $L_t$ then looks as follows (for a random time step $t$ given $\mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ ) <a href="https://arxiv.org/abs/2006.11239">as shown by Ho et al. (2020)</a></p>
<p>$$ | \mathbf{\epsilon} - \mathbf{\epsilon}_\theta(\mathbf{x}_t, t) |^2 = | \mathbf{\epsilon} - \mathbf{\epsilon}_\theta( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{(1- \bar{\alpha}_t)  } \mathbf{\epsilon}, t) |^2.$$</p>
<p>Here, $\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\mathbf{\epsilon}$ is the pure noise sampled at time step $t$, and $\mathbf{\epsilon}_\theta (\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.</p>
<p>Here, $\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\mathbf{\epsilon}$ is the pure noise sampled at time step $t$, and $\mathbf{\epsilon}_\theta (\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.</p>
<p>The training algorithm now looks as follows:</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/training_pres.png" alt=""  />
</p>
<p>In other words:</p>
<ul>
<li>we take a random sample $\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\mathbf{x}_0)$</li>
<li>we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step)</li>
<li>we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ (using the nice property defined above)</li>
<li>the neural network is trained to predict this noise based on the corrupted image $\mathbf{x}_t$ (i.e. noise applied on $\mathbf{x}_0$ based on known schedule $\beta_t$ )</li>
</ul>
<h1 id="neural-nets">Neural Nets<a hidden class="anchor" aria-hidden="true" href="#neural-nets">#</a></h1>
<p>The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?</p>
<p>What is typically used here is very similar to that of an Autoencoder, which you may remember from typical &ldquo;intro to deep learning&rdquo; tutorials. Autoencoders have a so-called &ldquo;bottleneck&rdquo; layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the &ldquo;bottleneck&rdquo;, and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.</p>
<p>In terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/unet_architecture.jpg" alt=""  />
</p>
<h1 id="the-math-on-latent-diffusion">The math on Latent Diffusion<a hidden class="anchor" aria-hidden="true" href="#the-math-on-latent-diffusion">#</a></h1>
<p>It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as
can be up to one or a few thousand steps. One data point from Song et al. 2020: “For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.”</p>
<p>Latent diffusion model (LDM; Rombach &amp; Blattmann, et al. 2022) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/rombach-latent-space.png" alt=""  />
</p>
<p>It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression.</p>
<p>LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.</p>
<p>The perceptual compression process relies on an autoencoder model.</p>
<p>An encoder $\mathcal{E}$ is used to compress the input image $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$ to a smaller 2D latent vector $\mathbf{z} = \mathcal{E}(\mathbf{x}) \in \mathbb{R}^{h \times w \times c}$ , where the downsampling rate $f=H/h=W/w=2^m, m \in \mathbb{N}$.</p>
<p>Then an decoder $\mathcal{D}$ reconstructs the images from the latent vector, $\tilde{\mathbf{x}} = \mathcal{D}(\mathbf{z})$.</p>
<p>The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.</p>
<ul>
<li>KL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">VAE</a>.</li>
<li>VQ-reg: Uses a vector quantization layer within the decoder, like <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#vq-vae-and-vq-vae-2">VQVAE</a> but the quantization layer is absorbed by the decoder.</li>
</ul>
<p>The diffusion and denoising processes happen on the latent vector $\mathbf{z}$. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image).</p>
<p>The design is equivalent to fuse representation of different modality into the model with cross-attention mechanism.</p>
<p>Each type of conditioning information is paired with a domain-specific encoder $\tau_\theta$ to project the conditioning input $y$ to an intermediate representation that can be mapped into cross-attention component, $\tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}$:</p>
<p>$$
\begin{aligned}
&amp;\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\Big(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\Big) \cdot \mathbf{V} \\
&amp;\text{where }\mathbf{Q} = \mathbf{W}^{(i)}_Q \cdot \varphi_i(\mathbf{z}_i),; \mathbf{K} = \mathbf{W}^{(i)}_K \cdot \tau_\theta(y),; \mathbf{V} = \mathbf{W}^{(i)}_V \cdot \tau_\theta(y) \\
&amp;\text{and } \mathbf{W}^{(i)}_Q \in \mathbb{R}^{d \times d^i_\epsilon},; \mathbf{W}^{(i)}_K, \mathbf{W}^{(i)}_V \in \mathbb{R}^{d \times d_\tau},; \varphi_i(\mathbf{z}_i) \in \mathbb{R}^{N \times d^i_\epsilon},; \tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}
\end{aligned}
$$</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/rombach-latent-space-comments.jpg" alt=""  />

<em>Picture from <a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46">J. Rafid Siddiqui</a></em></p>
<h1 id="and-what-is-the-result">And what is the result?<a hidden class="anchor" aria-hidden="true" href="#and-what-is-the-result">#</a></h1>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/cat_math.jpeg" alt=""  />
</p>
<p><img loading="lazy" src="/posts/2023_01_11_latent_diffusion_models/images/cat_math2.jpeg" alt=""  />
</p>
<h1 id="ressources">Ressources<a hidden class="anchor" aria-hidden="true" href="#ressources">#</a></h1>
<p><a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</a></p>
<p><a href="https://www.youtube.com/watch?v=J87hffSMB60">How does Stable Diffusion work? – Latent Diffusion Models EXPLAINED</a> [Video]</p>
<p><a href="https://www.youtube.com/watch?v=ltLNYA3lWAQ">Stable Diffusion - What, Why, How?</a> [Video]</p>
<p><a href="https://www.youtube.com/watch?v=_7rMfsA24Ls&amp;ab_channel=JeremyHoward">Stable Diffusion videos from fast.ai</a> [Video]</p>
<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/">Illustrations by Jay Alammar</a>
<a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a></p>
<p><a href="https://arxiv.org/abs/2112.10752">Rombach &amp; Blattmann, et al. 2022</a></p>
<p><a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a></p>
<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng on Diffusion Models</a></p>
<p><a href="https://theaisummer.com/diffusion-models/">Sergios Karagiannakos on Diffusion Models</a></p>
<p><a href="https://huggingface.co/blog/annotated-diffusion">Hugging Face on Annotated Diffusion Model</a></p>
<p><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Assembly AI on Diffusion</a></p>
<p><a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46">J. Rafid Siddiqui on Latent Diffusion</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://patrickpt.github.io/tags/math/">Math</a></li>
      <li><a href="https://patrickpt.github.io/tags/stable-diffusion/">stable-diffusion</a></li>
      <li><a href="https://patrickpt.github.io/tags/latent-diffusion-models/">latent-diffusion-models</a></li>
      <li><a href="https://patrickpt.github.io/tags/notes/">Notes</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on twitter"
        href="https://twitter.com/intent/tweet/?text=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f&amp;hashtags=Math%2cstable-diffusion%2clatent-diffusion-models%2cNotes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f&amp;title=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f&amp;summary=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f&amp;source=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f&title=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on whatsapp"
        href="https://api.whatsapp.com/send?text=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f%20-%20https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Latent Diffusion Models: What is all the fuzz about? on telegram"
        href="https://telegram.me/share/url?text=Latent%20Diffusion%20Models%3a%20What%20is%20all%20the%20fuzz%c2%a0about%3f&amp;url=https%3a%2f%2fpatrickpt.github.io%2fposts%2flatent-diffusion-models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://patrickpt.github.io/">Notes on AI</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
