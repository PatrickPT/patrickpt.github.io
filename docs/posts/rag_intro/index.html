<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is RAG? | patrickschnass.de</title>
<meta name="keywords" content="Model Architecture, AI Design, LLM, GenAI">
<meta name="description" content="Retrieval Augmented Generation is an Architecture used for NLP tasks.">
<meta name="author" content="">
<link rel="canonical" href="https://www.patrickschnass.de/posts/rag_intro/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://www.patrickschnass.de/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.patrickschnass.de/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.patrickschnass.de/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.patrickschnass.de/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.patrickschnass.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="What is RAG?" />
<meta property="og:description" content="Retrieval Augmented Generation is an Architecture used for NLP tasks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.patrickschnass.de/posts/rag_intro/" /><meta property="og:image" content="https://www.patrickschnass.de/posts/2023_09_29_Retrieval_Augmented_Generation/images/rag_high_level.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-29T18:30:57+00:00" />
<meta property="article:modified_time" content="2023-09-29T18:30:57+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://www.patrickschnass.de/posts/2023_09_29_Retrieval_Augmented_Generation/images/rag_high_level.jpg"/>

<meta name="twitter:title" content="What is RAG?"/>
<meta name="twitter:description" content="Retrieval Augmented Generation is an Architecture used for NLP tasks."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.patrickschnass.de/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is RAG?",
      "item": "https://www.patrickschnass.de/posts/rag_intro/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is RAG?",
  "name": "What is RAG?",
  "description": "Retrieval Augmented Generation is an Architecture used for NLP tasks.",
  "keywords": [
    "Model Architecture", "AI Design", "LLM", "GenAI"
  ],
  "articleBody": "TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.\nWhy should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.\npicture from Neo4J\nBut fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:\nAcquiring the massive computational infrastructure required to train LLMs effectively demands significant financial investment. Even if you rely on Cloud Providers instead of on-premise the cost to train is a factor which cannot be neglected currently. Assembling high-quality, domain-specific datasets for fine-tuning can be time-consuming and expensive, as it often involves labor-intensive data collection and annotation efforts. Other downsides are:\nHallucinations: If you think about it consequently fine-tuning will not guarantee that the model behaves the way you would like it to behave. We are able to steer how the model is “learning” when fine-tuning but we cannot control the outcome. The probability that we receive hallucinations is way higher if we fine-tune in contrast to if we just directly provide additional context in the prompt which is matching to our question. Retraining and Knowledge Cut-offs: It would require constant compute-intensive retraining for even small changes. As you would not do that every day every LLM has a training end date, post which it is unaware of events or information. Context Window: Each Large Language Model (LLM) functions within a contextual window, which essentially defines the maximum volume of information it can simultaneously accommodate. When external data sources provides information surpassing this window’s capacity, it needs to be segmented into smaller portions that align with the model’s contextual limitations. Use Retrieval Augmented Generation Research on NLP was there before the hype around GPT’s and specifically on OpenAIs service ChatGPT(based on GPT3.5) started in 2022. In 2020 Lewis et al. from Meta AI published a paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\nRetrieval-augmented generation (RAG) is an AI architecture for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLMs internal representation of information.\npicture from Neo4J\n“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”\nIt is a fusion of two powerful NLP techniques: retrieval-based models and generative models. Retrieval-based models excel at finding relevant information from a vast pool of knowledge, often in the form of pre-existing text or documents. On the other hand, generative models are proficient in generating human-like text based on the given input.\nIncorporating it into an LLM architecture enables the model to combine the best of both worlds.\nIt can retrieve contextually relevant information from a vast knowledge base and use that information to generate coherent and contextually appropriate responses. This approach leads to more accurate and context-aware interactions with the language model.\npicture from Lewis et al. (2021)\nThe architecture of an LLM incorporating retrieval augmented generation typically consists of two main components:\nRetrieval Module The retrieval module is the heart of the architecture and responsible for searching and retrieving relevant information from a large knowledge base. This knowledge base can be a collection of texts, documents, or even web pages. Retrieval can be broken down into two stages:\nIndexing You will index the knowledge base which you want to give as context to the model. To index your data you can simply use libraries like LangChain llama_index or transformers which do the work for you. The data is loaded, chunked and tokenized. Data will be fetched from your various sources, segmented into bite-sized chunks to optimize them for embedding and search and afterwards tokenized to create embeddings. The embeddings are stored as high dimensional vectors in vector databases and build the foundation for RAG.\nLet’s look into the implementation of RAG in llama_index, a famous orchestration network to further understand the indexing stage:\npicture and text from llama_index\nData Connectors: A data connector (i.e. Reader) ingest data from different data sources and data formats into a simple Document representation (text and simple metadata).\nDocuments / Nodes: A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. It’s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\nData Indexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a format that’s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the VectorStoreIndex\nQuerying The data from the vector databases can be used for Semantic Search. Meaning that when a query is processed(into an embedding) the retriever can search for the most appropriate matching data in the vector database and give it to the generator as context. The retrieved data is combined with the original prompt, creating an enhanced or augmented prompt. This augmented prompt provides additional context. This is even more relevant for domain specific data which may not be part of the corpus used for training the model.\nAgain let’s look how this is implemented llama_index, to also understand the querying stage:\npicture and text from llama_index\nRetrievers: A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.\nNode Postprocessors: A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.\nResponse Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\nIn summary: We start by searching for relevant documents or excerpts within an extensive dataset. For this a dense retrieval mechanism that leverages embeddings to represent both the query and the documents is used. These embeddings are then utilized to calculate similarity scores, leading to the retrieval of the top-ranked documents.\nGeneration Module The generation module is based on a generative language model like GPT-3.5. It takes as input the retrieved information and the user’s query or context and generates a coherent response.\nExample An example architecture using llama_index would look like the following:\npicture from streamlit\nThe code for this example can be found on github. Also you can find a dedicated blogpost on my blog Hands on with Retrieval Augmented Generation.\nBenefits of using RAG Improved Precision: Through the utilization of external data sources, the RAG LLM can produce responses that are not solely derived from its training dataset but are also enriched by the most pertinent and current information accessible within the retrieval corpus.\nAddressing Knowledge Gaps: RAG effectively confronts the inherent knowledge limitations of LLM, whether stemming from the model’s training cut-off or the absence of domain-specific data in its training corpus.\nAdaptability: RAG can seamlessly integrate with an array of external data sources, encompassing proprietary databases within organizations and publicly accessible internet data. This adaptability makes it suitable for a broad spectrum of applications and industries.\nMitigating Hallucinations: A challenge associated with LLMs is the potential occurrence of “hallucinations,”. By incorporating real-time data context, RAG decreases the probability of such outputs.\nScalability: An advantage of RAG LLMs lies in its scalability. Through the separation of the retrieval and generation processes, the model can manage extensive datasets, making it well-suited for real-world scenarios characterized by abundant data.\nPitfalls Of course there is a downside with retrieval systems. They rely on semantic search. Semantic Search has a simple assumption which leads to problems:\nQuestion and answer have a high semantic similarity.\nThis assumption seems easy and straightforward but it is oversimplifying human language. Semantic Search only looks for explicit matches not for negation or implicit matches.\nA simple example Think of following example:\ninitial sentence:\n“I like Bananas”\nvs\n“I don’t like Bananas”\n“I like every fruit but bananas”\n“I love this divine yellow fruit. It is curved like a smile and I feel like a monkey when I eat it.”\nI think it is clear where i am pointing at. The sentences are already ordered from highest to lowest similarity.\nOf course there is potential to overcome this with additional heuristics but for sure we need to understand that every architecture we built is implicitly build on assumptions that influence the outcome.\nConclusion Retrieval augmented generation is a versatile hack.\nIn the early days of LLM adoption - where we are still at - it is a good technique to rapidly deliver production grade systems which are more efficient and less prone for hallucinations.\nDue to the implicit assumptions and with it bias humans are bringing into the architecture in the future there will be better ways to deliver a good outcome. But for now it is highly relevant to consider Retrieval Augmented Generation for building LLM powered solutions.\nResources Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\nIBM: What is retrieval-augmented generation?\nRetrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models\nRetrieval meets Long Context Large Language Models\nA Deep Dive into Retrieval-Augmented Generation in LLM\nPitfalls of Semantic Search\n",
  "wordCount" : "1596",
  "inLanguage": "en",
  "datePublished": "2023-09-29T18:30:57Z",
  "dateModified": "2023-09-29T18:30:57Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.patrickschnass.de/posts/rag_intro/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickschnass.de",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.patrickschnass.de/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.patrickschnass.de/" accesskey="h" title="patrickschnass.de (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickschnass.de</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.patrickschnass.de/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      What is RAG?
    </h1>
    <div class="post-meta"><span title='2023-09-29 18:30:57 +0000 UTC'>September 29, 2023</span>&nbsp;·&nbsp;8 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#why-should-i-care" aria-label="Why should i care?">Why should i care?</a></li>
                <li>
                    <a href="#use-retrieval-augmented-generation" aria-label="Use Retrieval Augmented Generation">Use Retrieval Augmented Generation</a></li>
                <li>
                    <a href="#retrieval-module" aria-label="Retrieval Module">Retrieval Module</a><ul>
                        
                <li>
                    <a href="#indexing" aria-label="Indexing">Indexing</a></li>
                <li>
                    <a href="#querying" aria-label="Querying">Querying</a></li></ul>
                </li>
                <li>
                    <a href="#generation-module" aria-label="Generation Module">Generation Module</a></li>
                <li>
                    <a href="#example" aria-label="Example">Example</a></li>
                <li>
                    <a href="#benefits-of-using-rag" aria-label="Benefits of using RAG">Benefits of using RAG</a></li>
                <li>
                    <a href="#pitfalls" aria-label="Pitfalls">Pitfalls</a><ul>
                        
                <li>
                    <a href="#a-simple-example" aria-label="A simple example">A simple example</a></li></ul>
                </li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#resources" aria-label="Resources">Resources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.</p>
<h1 id="why-should-i-care">Why should i care?<a hidden class="anchor" aria-hidden="true" href="#why-should-i-care">#</a></h1>
<p>Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/fine-tuning.png" alt=""  />

<em>picture from <a href="https://neo4j.com/developer-blog/fine-tuning-retrieval-augmented-generation/">Neo4J</a></em></p>
<p>But fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:</p>
<ul>
<li>Acquiring the massive computational infrastructure required to train LLMs effectively demands significant financial investment. Even if you rely on Cloud Providers instead of on-premise the cost to train is a factor which cannot be neglected currently.</li>
<li>Assembling high-quality, domain-specific datasets for fine-tuning can be time-consuming and expensive, as it often involves labor-intensive data collection and annotation efforts.</li>
</ul>
<p>Other downsides are:</p>
<ul>
<li><strong>Hallucinations:</strong> If you think about it consequently fine-tuning will not guarantee that the model behaves the way you would like it to behave. We are able to steer how the model is &ldquo;learning&rdquo; when fine-tuning but we cannot control the outcome. The probability that we receive hallucinations is way higher if we fine-tune in contrast to if we just directly provide additional context in the prompt which is matching to our question.</li>
<li><strong>Retraining and Knowledge Cut-offs:</strong> It would require constant compute-intensive retraining for even small changes. As you would not do that every day every LLM has a training end date, post which it is unaware of events or information.</li>
<li><strong>Context Window:</strong> Each Large Language Model (LLM) functions within a contextual window, which essentially defines the maximum volume of information it can simultaneously accommodate. When external data sources provides information surpassing this window&rsquo;s capacity, it needs to be segmented into smaller portions that align with the model&rsquo;s contextual limitations.</li>
</ul>
<h1 id="use-retrieval-augmented-generation">Use Retrieval Augmented Generation<a hidden class="anchor" aria-hidden="true" href="#use-retrieval-augmented-generation">#</a></h1>
<p>Research on NLP was there before the hype around GPT&rsquo;s and specifically on OpenAIs service ChatGPT(based on GPT3.5) started in 2022.
In 2020 Lewis et al. from Meta AI published a paper on <a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>.</p>
<p>Retrieval-augmented generation (RAG) is an AI architecture for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLMs internal representation of information.</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/overview.png" alt=""  />

<em>picture from <a href="https://neo4j.com/developer-blog/fine-tuning-retrieval-augmented-generation/">Neo4J</a></em></p>
<blockquote>
<p><em>“It’s the difference between an open-book and a closed-book exam,”</em> Lastras said. <em>“In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”</em></p>
</blockquote>
<p>It is a fusion of two powerful NLP techniques: retrieval-based models and generative models. Retrieval-based models excel at finding relevant information from a vast pool of knowledge, often in the form of pre-existing text or documents. On the other hand, generative models are proficient in generating human-like text based on the given input.</p>
<p>Incorporating it into an LLM architecture enables the model to combine the best of both worlds.</p>
<ul>
<li>It can retrieve contextually relevant information from a vast knowledge base and</li>
<li>use that information to generate coherent and contextually appropriate responses.</li>
</ul>
<p>This approach leads to more accurate and context-aware interactions with the language model.</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/rag_lewis.png" alt=""  />

<em>picture from <a href="https://arxiv.org/pdf/2005.11401.pdf">Lewis et al. (2021)</a></em></p>
<p>The architecture of an LLM incorporating retrieval augmented generation typically consists of two main components:</p>
<h1 id="retrieval-module">Retrieval Module<a hidden class="anchor" aria-hidden="true" href="#retrieval-module">#</a></h1>
<p>The retrieval module is the heart of the architecture and responsible for searching and retrieving relevant information from a large knowledge base. This knowledge base can be a collection of texts, documents, or even web pages. Retrieval can be broken down into two stages:</p>
<h2 id="indexing">Indexing<a hidden class="anchor" aria-hidden="true" href="#indexing">#</a></h2>
<p>You will index the knowledge base which you want to give as context to the model. To index your data you can simply use libraries like LangChain llama_index or transformers which do the work for you. The data is loaded, chunked and tokenized. Data will be fetched from your various sources, segmented into bite-sized chunks to optimize them for embedding and search and afterwards tokenized to create embeddings.
The embeddings are stored as high dimensional vectors in vector databases and build the foundation for RAG.</p>
<p>Let&rsquo;s look into the implementation of RAG in <a href="https://github.com/run-llama/llama_index">llama_index</a>, a famous orchestration network to further understand the indexing stage:</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/indexing.jpg" alt=""  />

<em>picture and text from <a href="https://gpt-index.readthedocs.io/en/latest/getting_started/concepts.html">llama_index</a></em></p>
<blockquote>
<p><strong>Data Connectors:</strong> A data connector (i.e. Reader) ingest data from different data sources and data formats into a simple Document representation (text and simple metadata).</p>
</blockquote>
<blockquote>
<p><strong>Documents / Nodes:</strong> A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. It’s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.</p>
</blockquote>
<blockquote>
<p><strong>Data Indexes:</strong> Once you’ve ingested your data, LlamaIndex will help you index the data into a format that’s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the VectorStoreIndex</p>
</blockquote>
<h2 id="querying">Querying<a hidden class="anchor" aria-hidden="true" href="#querying">#</a></h2>
<p>The data from the vector databases can be used for Semantic Search. Meaning that when a query is processed(into an embedding) the retriever can search for the most appropriate matching data in the vector database and give it to the generator as context. The retrieved data is combined with the original prompt, creating an enhanced or augmented prompt. This augmented prompt provides additional context. This is even more relevant for domain specific data which may not be part of the corpus used for training the model.</p>
<p>Again let&rsquo;s look how this is implemented llama_index, to also understand the querying stage:</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/querying.jpg" alt=""  />

<em>picture and text from <a href="https://gpt-index.readthedocs.io/en/latest/getting_started/concepts.html">llama_index</a></em></p>
<blockquote>
<p><strong>Retrievers:</strong> A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.</p>
</blockquote>
<blockquote>
<p><strong>Node Postprocessors:</strong> A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.</p>
</blockquote>
<blockquote>
<p><strong>Response Synthesizers:</strong> A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.</p>
</blockquote>
<p>In summary:
We start by searching for relevant documents or excerpts within an extensive dataset. For this a dense retrieval mechanism that leverages embeddings to represent both the query and the documents is used. These embeddings are then utilized to calculate similarity scores, leading to the retrieval of the top-ranked documents.</p>
<h1 id="generation-module">Generation Module<a hidden class="anchor" aria-hidden="true" href="#generation-module">#</a></h1>
<p>The generation module is based on a generative language model like GPT-3.5. It takes as input the retrieved information and the user&rsquo;s query or context and generates a coherent response.</p>
<h1 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h1>
<p>An example architecture using llama_index would look like the following:</p>
<p><img loading="lazy" src="/posts/2023_09_29_Retrieval_Augmented_Generation/images/rag-with-llamaindex.png" alt=""  />

<em>picture from <a href="(https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/)">streamlit</a></em></p>
<p>The code for this example can be found on <a href="https://github.com/PatrickPT/RAG_LLM_example">github</a>.
Also you can find a dedicated blogpost on my blog <a href="/posts/hands-on-rag">Hands on with Retrieval Augmented Generation</a>.</p>
<h1 id="benefits-of-using-rag">Benefits of using RAG<a hidden class="anchor" aria-hidden="true" href="#benefits-of-using-rag">#</a></h1>
<p><strong>Improved Precision:</strong>
Through the utilization of external data sources, the RAG LLM can produce responses that are not solely derived from its training dataset but are also enriched by the most pertinent and current information accessible within the retrieval corpus.</p>
<p><strong>Addressing Knowledge Gaps:</strong>
RAG effectively confronts the inherent knowledge limitations of LLM, whether stemming from the model&rsquo;s training cut-off or the absence of domain-specific data in its training corpus.</p>
<p><strong>Adaptability:</strong>
RAG can seamlessly integrate with an array of external data sources, encompassing proprietary databases within organizations and publicly accessible internet data. This adaptability makes it suitable for a broad spectrum of applications and industries.</p>
<p><strong>Mitigating Hallucinations:</strong>
A challenge associated with LLMs is the potential occurrence of &ldquo;hallucinations,&rdquo;. By incorporating real-time data context, RAG decreases the probability of such outputs.</p>
<p><strong>Scalability:</strong>
An advantage of RAG LLMs lies in its scalability. Through the separation of the retrieval and generation processes, the model can manage extensive datasets, making it well-suited for real-world scenarios characterized by abundant data.</p>
<h1 id="pitfalls">Pitfalls<a hidden class="anchor" aria-hidden="true" href="#pitfalls">#</a></h1>
<p>Of course there is a downside with retrieval systems. They rely on semantic search.
Semantic Search has a simple assumption which leads to problems:</p>
<blockquote>
<p><em>Question and answer have a high semantic similarity.</em></p>
</blockquote>
<p>This assumption seems easy and straightforward but it is oversimplifying human language.
Semantic Search only looks for explicit matches not for negation or implicit matches.</p>
<h2 id="a-simple-example">A simple example<a hidden class="anchor" aria-hidden="true" href="#a-simple-example">#</a></h2>
<p>Think of following example:</p>
<p>initial sentence:</p>
<blockquote>
<p><em>&ldquo;I like Bananas&rdquo;</em></p>
</blockquote>
<p>vs</p>
<blockquote>
<p><em>&ldquo;I don&rsquo;t like Bananas&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;I like every fruit but bananas&rdquo;</em></p>
</blockquote>
<blockquote>
<p><em>&ldquo;I love this divine yellow fruit. It is curved like a smile and I feel like a monkey when I eat it.&rdquo;</em></p>
</blockquote>
<p>I think it is clear where i am pointing at. The sentences are already ordered from highest to lowest similarity.</p>
<p>Of course there is potential to overcome this with additional heuristics but for sure we need to understand that every architecture we built is implicitly build on assumptions that influence the outcome.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>Retrieval augmented generation is a versatile hack.</p>
<p>In the early days of LLM adoption - where we are still at - it is a good technique to <strong>rapidly deliver production grade systems</strong> which are <strong>more efficient</strong> and <strong>less prone for hallucinations</strong>.</p>
<p>Due to the implicit assumptions and with it <strong>bias</strong> humans are bringing into the architecture in the future there will be better ways to deliver a good outcome.
But <strong>for now it is highly relevant to consider Retrieval Augmented Generation for building LLM powered solutions</strong>.</p>
<h1 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h1>
<p><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p>
<p><a href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG">IBM: What is retrieval-augmented generation?</a></p>
<p><a href="https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models</a></p>
<p><a href="https://arxiv.org/abs/2310.03025">Retrieval meets Long Context Large Language Models</a></p>
<p><a href="https://www.unite.ai/a-deep-dive-into-retrieval-augmented-generation-in-llm/">A Deep Dive into Retrieval-Augmented Generation in LLM</a></p>
<p><a href="https://www.linkedin.com/pulse/pitfalls-semantic-search-reza-bonyadi%3FtrackingId=ktoarNI%252F3V2WaJyucfotvg%253D%253D/?trackingId=ktoarNI%2F3V2WaJyucfotvg%3D%3D">Pitfalls of Semantic Search</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.patrickschnass.de/tags/model-architecture/">Model Architecture</a></li>
      <li><a href="https://www.patrickschnass.de/tags/ai-design/">AI Design</a></li>
      <li><a href="https://www.patrickschnass.de/tags/llm/">LLM</a></li>
      <li><a href="https://www.patrickschnass.de/tags/genai/">GenAI</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on twitter"
        href="https://twitter.com/intent/tweet/?text=What%20is%20RAG%3f&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f&amp;hashtags=ModelArchitecture%2cAIDesign%2cLLM%2cGenAI">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f&amp;title=What%20is%20RAG%3f&amp;summary=What%20is%20RAG%3f&amp;source=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f&title=What%20is%20RAG%3f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on whatsapp"
        href="https://api.whatsapp.com/send?text=What%20is%20RAG%3f%20-%20https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is RAG? on telegram"
        href="https://telegram.me/share/url?text=What%20is%20RAG%3f&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2frag_intro%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://www.patrickschnass.de/">patrickschnass.de</a></span>
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
