<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>How to use a Reinforment Learning Agent to play Super Mario | patrickschnass.de</title>
<meta name="keywords" content="Fun, Q-Learning, Reinforcement Learning">
<meta name="description" content="TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.
Train your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.">
<meta name="author" content="">
<link rel="canonical" href="https://www.patrickschnass.de/posts/super_mario/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://www.patrickschnass.de/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.patrickschnass.de/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.patrickschnass.de/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.patrickschnass.de/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.patrickschnass.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-XKDTNB9VR0"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-XKDTNB9VR0', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="How to use a Reinforment Learning Agent to play Super Mario" />
<meta property="og:description" content="TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.
Train your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.patrickschnass.de/posts/super_mario/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-31T11:58:22+02:00" />
<meta property="article:modified_time" content="2023-03-31T11:58:22+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How to use a Reinforment Learning Agent to play Super Mario"/>
<meta name="twitter:description" content="TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.
Train your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.patrickschnass.de/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "How to use a Reinforment Learning Agent to play Super Mario",
      "item": "https://www.patrickschnass.de/posts/super_mario/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "How to use a Reinforment Learning Agent to play Super Mario",
  "name": "How to use a Reinforment Learning Agent to play Super Mario",
  "description": "TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.\nTrain your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.",
  "keywords": [
    "Fun", "Q-Learning", "Reinforcement Learning"
  ],
  "articleBody": "TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.\nTrain your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.\nThinking about the current possibilities and being lazy i thought about how to write software that could do the job for me and that this would be even more satisfying than playing it myself. I have knonw about the magnitude of recent emulators for all kinds of plattforms and wondered if some of those already contain an API for automating any training.\nAfter some search there it is: A GameBoy Emulator written in Python which can be fully controlled from a Python Script: PyBoy (I can’t highlight how big my nerdy enthusiasm for the fact of an emulator in Python is)\nSo the place is set up and we have everything we need:\nA GameBoy Emulator which allows us to interact via python A Rom of the game you already own (Legally speaking Security Backups are allowed) Time and Motivation to start The capabilities of PyBoy Installation Assuming that you already have a working Python environment the instalation is super easy. Just pip install pyboy and you are ready to go.\nStart Now if you are just interested in playing a game it is also super easy as you can just start pyboy from your terminal: $ pyboy path/to/your/file.rom\nPyBoy is loadable as an object in Python. This means, it can be initialized from another script, and be controlled and probed by the scrip:\nfrom pyboy import PyBoy pyboy = PyBoy('ROMs/gamerom.gb') while not pyboy.tick(): pass pyboy.stop() API When the emulator is running you can access PyBoy’s API.\nfrom pyboy import WindowEvent pyboy.send_input(WindowEvent.PRESS_ARROW_DOWN) pyboy.tick() # Process one frame to let the game register the input pyboy.send_input(WindowEvent.RELEASE_ARROW_DOWN) pil_image = pyboy.screen_image() pil_image.save('screenshot.png') Now you have everything you need and find all relevant commands in the PyBoy Documentation.\nThe Setup for your Reinforcement Learning Algorithm Concepts of RL Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The goal of RL is to find an optimal policy that maximizes a long-term reward signal. The agent takes actions in the environment and receives feedback in the form of rewards or penalties. The agent then updates its policy based on the feedback received, in order to maximize the total reward it receives over time.\nOur setup is following the typical concepts of RL\nEnvironment The world that an agent interacts with and learns from.\nAction $a$ : How the Agent responds to the Environment. The set of all possible Actions is called action-space.\nState $s$ : The current characteristic of the Environment. The set of all possible States the Environment can be in is called state-space.\nReward $r$ : Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called Return.\nOptimal Action-Value function $Q^*(s,a)$ : Gives the expected return if you start in state $s$, take an arbitrary action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the “quality” of the action in a state. We try to approximate this function.\nSpecification of Q Learning Q-Learning is a popular reinforcement learning algorithm that learns the optimal action-value function, also known as the Q-function, for a given environment. The Q-function represents the expected long-term reward for taking a particular action in a given state. The Q-learning algorithm updates the Q-function based on the rewards received and the estimated Q-values for the next state. The agent then uses the updated Q-function to select the next action.\nThe Q-learning algorithm is based on the Bellman equation, which is a recursive equation that expresses the optimal Q-value in terms of the expected reward for the current action and the expected Q-value for the next state. The Q-learning algorithm uses a greedy approach to select actions, meaning that it always chooses the action with the highest estimated Q-value.\nQ-learning stores the results between iterations in a Q-table, which is essentially a lookup table that contains the expected reward values for every state-action pair in the environment. The Q-table is initialized with arbitrary values and is updated over time as the agent interacts with the environment.\nAt each iteration, the agent observes the current state of the environment, selects an action based on the Q-values in the Q-table, performs the action, and observes the reward and the next state. The agent then updates the Q-value for the state-action pair based on the Bellman equation, which expresses the optimal Q-value for a state-action pair as the sum of the immediate reward and the discounted expected future reward.\nThe Q-value update equation is as follows:\nQ(s,a) = Q(s,a) + alpha * (r + gamma * max(Q(s’,a’)) - Q(s,a))\nwhere:\nQ(s,a) is the current Q-value for the state-action pair (s,a) alpha is the learning rate, which determines how much the Q-value is updated based on the new information r is the immediate reward received for taking the action a in state s gamma is the discount factor, which determines how much weight is given to future rewards max(Q(s’,a’)) is the maximum Q-value for the next state s’ and all possible actions a’ that can be taken from s' After the Q-value update, the agent moves to the next state and repeats the process until it reaches the terminal state.\nAs the agent continues to interact with the environment, the Q-table is gradually updated with more accurate estimates of the optimal Q-values. The agent uses the Q-table to select the optimal action in each state, based on the highest Q-value in the table for that state. By learning from experience and updating the Q-values over time, Q-learning allows the agent to make better decisions and maximize the cumulative reward it receives from the environment.\nIt’s a me Mario PyBoy is giving an inspiration on how to set up the Agent for Super Mario.\nEnvironment The environment is in this case the world itself: The Floor, Pipes, Blocks the Background and of course the enemies. To capture the evironment means to capture the complete observation space in every single frame.\nAction The actions chosen could be all actions from the actual Window Event: List[WindowEvent] but for the game it would make absolutely no sense to test the Start Buttone or Select Button. Therefore we focus on LEFT,RIGHT and A.\nbaseActions = [WindowEvent.PRESS_ARROW_RIGHT, WindowEvent.PRESS_BUTTON_A, WindowEvent.PRESS_ARROW_LEFT] State Here it is defined by GameState.\nReward The reward is defined by the progress in the game: deaths, time, level and movement.\nclock = current_mario.time_left - prevGameState.time_left movement = current_mario.real_x_pos - prevGameState.real_x_pos death = -15*(current_mario.lives_left - prevGameState.lives_left) levelReward = 15*max((current_mario.world[0] - prevGameState.world[0]), (current_mario.world[1] - prevGameState.world[1])) # +15 if either new level or new world reward = clock + death + movement + levelReward Let the games begin I built my own simplified agent on following example and had a ton of fun here\nRessources PyBoy Repo\nPyBoy Documentation\nPlay SNES Super Mario with RL\nExample RL for PyBoy\n",
  "wordCount" : "1238",
  "inLanguage": "en",
  "datePublished": "2023-03-31T11:58:22+02:00",
  "dateModified": "2023-03-31T11:58:22+02:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.patrickschnass.de/posts/super_mario/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "patrickschnass.de",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.patrickschnass.de/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.patrickschnass.de/" accesskey="h" title="patrickschnass.de (Alt + H)">
                    <svg xmlns="http://www.w3.org/2000/svg" width="25" height="33" viewBox="0 0 692 925" version="1.1"><path d="M 305.500 152.598 C 277.952 155.364, 264.040 157.585, 246.500 162.020 C 176.736 179.660, 114.320 220.431, 69.715 277.500 C 45.341 308.683, 24.587 349.480, 13.500 388 C 0.984 431.487, -2.156 479.998, 4.569 525.963 C 11.060 570.329, 28.756 616.870, 53.441 654.500 C 107.120 736.327, 191.378 788.597, 289 800.631 C 309.235 803.126, 346.723 802.885, 367.236 800.128 C 427.820 791.987, 479.398 770.351, 527.851 732.754 C 547.994 717.124, 575.146 688.480, 591.603 665.500 C 615.790 631.726, 634.893 588.574, 643.841 547.500 C 664.823 451.194, 641.660 351.304, 580.528 274.458 C 570.339 261.650, 542.233 233.480, 529.996 223.812 C 480.219 184.483, 424.814 161.515, 361.500 153.962 C 352.205 152.853, 312.574 151.888, 305.500 152.598 M 202.500 225.519 C 197.484 228.038, 193.204 233.699, 192.033 239.361 C 191.297 242.925, 191.052 316.541, 191.235 479.500 L 191.500 714.500 193.591 718.500 C 194.891 720.988, 197.506 723.634, 200.511 725.500 L 205.341 728.500 253.421 728.500 C 279.864 728.500, 302.703 728.163, 304.173 727.750 C 308.187 726.624, 313.058 722.083, 315.120 717.541 C 316.843 713.750, 316.957 709.322, 316.978 645.876 L 317 578.252 346.250 577.722 C 386.057 577.002, 408.993 573.357, 435.500 563.538 C 481.889 546.355, 515.407 516.366, 535.052 474.470 C 554.614 432.751, 558.304 377.382, 544.380 334.500 C 541.078 324.331, 532.261 306.687, 525.640 297 C 518.321 286.290, 499.444 267.803, 488 260.135 C 457.983 240.024, 420.028 228.466, 372.813 225.060 C 364.363 224.450, 325.716 224.003, 281.813 224.006 C 215.830 224.011, 205.094 224.216, 202.500 225.519 M 228 476.500 L 228 692 253.980 692 L 279.959 692 280.230 623.750 C 280.495 556.741, 280.538 555.427, 282.589 551.500 C 283.738 549.300, 286.589 546.150, 288.924 544.500 L 293.170 541.500 333.835 540.870 C 377.104 540.200, 385.356 539.449, 404.742 534.412 C 458.388 520.474, 494.460 488.338, 508.885 441.636 C 513.883 425.454, 515.447 413.641, 515.458 392 C 515.468 369.891, 514.439 362.271, 509.317 346.500 C 495.700 304.579, 461.330 277.584, 407.500 266.533 C 383.389 261.584, 371.738 261, 297.028 261 L 228 261 228 476.500 M 291.481 298.336 C 287.256 300.258, 282.281 306.244, 281.021 310.922 C 280.298 313.607, 280.045 343.297, 280.229 403.759 L 280.500 492.647 282.910 496.164 C 284.235 498.098, 286.901 500.765, 288.835 502.090 C 292.346 504.498, 292.379 504.500, 324.925 504.811 C 343.375 504.987, 361.402 504.669, 366.500 504.078 C 412.900 498.701, 445.063 471.671, 456.738 428.242 C 459.135 419.328, 459.363 416.800, 459.429 398.500 C 459.486 382.404, 459.145 377.036, 457.682 371 C 450.353 340.762, 431.297 318.855, 401.922 306.901 C 394.184 303.752, 386.667 301.638, 375 299.328 C 364.132 297.177, 295.815 296.365, 291.481 298.336 M 317 400.968 L 317 468.177 342.250 467.754 C 364.573 467.380, 368.427 467.071, 375.499 465.087 C 403.633 457.192, 419.220 438.577, 423.041 408.312 C 424.982 392.934, 422.187 375.758, 415.788 363.754 C 411.848 356.362, 402.013 347.113, 393.814 343.091 C 390.341 341.387, 383 338.812, 377.500 337.369 C 368.336 334.963, 365.390 334.703, 342.250 334.252 L 317 333.760 317 400.968" stroke="none" fill="currentColor" fill-rule="evenodd"/></svg> patrickschnass.de</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.patrickschnass.de/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.patrickschnass.de/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      How to use a Reinforment Learning Agent to play Super Mario
    </h1>
    <div class="post-meta"><span title='2023-03-31 11:58:22 +0200 CEST'>March 31, 2023</span>&nbsp;·&nbsp;6 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tldr" aria-label="TL;DR">TL;DR</a></li>
                <li>
                    <a href="#train-your-own-rl-agent-to-play-super-mario" aria-label="Train your own RL Agent to play Super Mario">Train your own RL Agent to play Super Mario</a></li>
                <li>
                    <a href="#the-capabilities-of-pyboy" aria-label="The capabilities of PyBoy">The capabilities of PyBoy</a><ul>
                        
                <li>
                    <a href="#installation" aria-label="Installation">Installation</a></li>
                <li>
                    <a href="#start" aria-label="Start">Start</a></li>
                <li>
                    <a href="#api" aria-label="API">API</a></li></ul>
                </li>
                <li>
                    <a href="#the-setup-for-your-reinforcement-learning-algorithm" aria-label="The Setup for your Reinforcement Learning Algorithm">The Setup for your Reinforcement Learning Algorithm</a><ul>
                        
                <li>
                    <a href="#concepts-of-rl" aria-label="Concepts of RL">Concepts of RL</a></li>
                <li>
                    <a href="#specification-of-q-learning" aria-label="Specification of Q Learning">Specification of Q Learning</a></li>
                <li>
                    <a href="#its-a-me-mario" aria-label="It&amp;rsquo;s a me Mario">It&rsquo;s a me Mario</a></li></ul>
                </li>
                <li>
                    <a href="#let-the-games-begin" aria-label="Let the games begin">Let the games begin</a></li>
                <li>
                    <a href="#ressources" aria-label="Ressources">Ressources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tldr">TL;DR<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.</p>
<h1 id="train-your-own-rl-agent-to-play-super-mario">Train your own RL Agent to play Super Mario<a hidden class="anchor" aria-hidden="true" href="#train-your-own-rl-agent-to-play-super-mario">#</a></h1>
<p><img loading="lazy" src="/posts/2023_03_31_train_super_mario/images/Super_Mario_Land.jpeg" alt="Super Mario Land"  />
</p>
<p>Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago.
Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.</p>
<p>Thinking about the current possibilities and being lazy i thought about how to write software that could do the job for me and that this would be even more satisfying than playing it myself. I have knonw about the magnitude of recent emulators for all kinds of plattforms and wondered if some of those already contain an API for automating any training.</p>
<p>After some search there it is:
A GameBoy Emulator written in Python which can be fully controlled from a Python Script: <a href="https://github.com/Baekalfen/PyBoy">PyBoy</a>
(<em>I can&rsquo;t highlight how big my nerdy enthusiasm for the fact of an emulator in Python is</em>)</p>
<p>So the place is set up and we have everything we need:</p>
<ul>
<li>A GameBoy Emulator which allows us to interact via python</li>
<li>A Rom of the game you already own (Legally speaking Security Backups are allowed)</li>
<li>Time and Motivation to start</li>
</ul>
<h1 id="the-capabilities-of-pyboy">The capabilities of PyBoy<a hidden class="anchor" aria-hidden="true" href="#the-capabilities-of-pyboy">#</a></h1>
<h2 id="installation">Installation<a hidden class="anchor" aria-hidden="true" href="#installation">#</a></h2>
<p>Assuming that you already have a working Python environment the instalation is super easy.
Just <code>pip install pyboy</code> and you are ready to go.</p>
<h2 id="start">Start<a hidden class="anchor" aria-hidden="true" href="#start">#</a></h2>
<p>Now if you are just interested in playing a game it is also super easy as you can just start pyboy from your terminal:
<code>$ pyboy path/to/your/file.rom</code></p>
<p>PyBoy is loadable as an object in Python. This means, it can be initialized from another script, and be controlled and probed by the scrip:</p>
<pre><code>from pyboy import PyBoy
pyboy = PyBoy('ROMs/gamerom.gb')
while not pyboy.tick():
    pass
pyboy.stop()
</code></pre>
<h2 id="api">API<a hidden class="anchor" aria-hidden="true" href="#api">#</a></h2>
<p>When the emulator is running you can access <a href="https://docs.pyboy.dk/index.html">PyBoy&rsquo;s API</a>.</p>
<pre><code>from pyboy import WindowEvent

pyboy.send_input(WindowEvent.PRESS_ARROW_DOWN)
pyboy.tick() # Process one frame to let the game register the input
pyboy.send_input(WindowEvent.RELEASE_ARROW_DOWN)

pil_image = pyboy.screen_image()
pil_image.save('screenshot.png')
</code></pre>
<p>Now you have everything you need and find all relevant commands in the <a href="https://docs.pyboy.dk/index.html">PyBoy Documentation</a>.</p>
<h1 id="the-setup-for-your-reinforcement-learning-algorithm">The Setup for your Reinforcement Learning Algorithm<a hidden class="anchor" aria-hidden="true" href="#the-setup-for-your-reinforcement-learning-algorithm">#</a></h1>
<h2 id="concepts-of-rl">Concepts of RL<a hidden class="anchor" aria-hidden="true" href="#concepts-of-rl">#</a></h2>
<p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The goal of RL is to find an optimal policy that maximizes a long-term reward signal. The agent takes actions in the environment and receives feedback in the form of rewards or penalties. The agent then updates its policy based on the feedback received, in order to maximize the total reward it receives over time.</p>
<p>Our setup is following the typical <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">concepts of RL</a></p>
<p><strong>Environment</strong> The world that an agent interacts with and learns from.</p>
<p><strong>Action</strong> $a$ : How the Agent responds to the Environment. The
set of all possible Actions is called <em>action-space</em>.</p>
<p><strong>State</strong> $s$ : The current characteristic of the Environment. The
set of all possible States the Environment can be in is called
<em>state-space</em>.</p>
<p><strong>Reward</strong> $r$ : Reward is the key feedback from Environment to
Agent. It is what drives the Agent to learn and to change its future
action. An aggregation of rewards over multiple time steps is called
<strong>Return</strong>.</p>
<p><strong>Optimal Action-Value function</strong> $Q^*(s,a)$ : Gives the expected
return if you start in state $s$, take an arbitrary action
$a$, and then for each future time step take the action that
maximizes returns. $Q$ can be said to stand for the “quality” of
the action in a state. We try to approximate this function.</p>
<h2 id="specification-of-q-learning">Specification of Q Learning<a hidden class="anchor" aria-hidden="true" href="#specification-of-q-learning">#</a></h2>
<p>Q-Learning is a popular reinforcement learning algorithm that learns the optimal action-value function, also known as the Q-function, for a given environment. The Q-function represents the expected long-term reward for taking a particular action in a given state. The Q-learning algorithm updates the Q-function based on the rewards received and the estimated Q-values for the next state. The agent then uses the updated Q-function to select the next action.</p>
<p>The Q-learning algorithm is based on the Bellman equation, which is a recursive equation that expresses the optimal Q-value in terms of the expected reward for the current action and the expected Q-value for the next state. The Q-learning algorithm uses a greedy approach to select actions, meaning that it always chooses the action with the highest estimated Q-value.</p>
<p>Q-learning stores the results between iterations in a Q-table, which is essentially a lookup table that contains the expected reward values for every state-action pair in the environment. The Q-table is initialized with arbitrary values and is updated over time as the agent interacts with the environment.</p>
<p>At each iteration, the agent observes the current state of the environment, selects an action based on the Q-values in the Q-table, performs the action, and observes the reward and the next state. The agent then updates the Q-value for the state-action pair based on the Bellman equation, which expresses the optimal Q-value for a state-action pair as the sum of the immediate reward and the discounted expected future reward.</p>
<p>The Q-value update equation is as follows:</p>
<p>Q(s,a) = Q(s,a) + alpha * (r + gamma * max(Q(s&rsquo;,a&rsquo;)) - Q(s,a))</p>
<p>where:</p>
<p>Q(s,a) is the current Q-value for the state-action pair (s,a)
alpha is the learning rate, which determines how much the Q-value is updated based on the new information
r is the immediate reward received for taking the action a in state s
gamma is the discount factor, which determines how much weight is given to future rewards
max(Q(s&rsquo;,a&rsquo;)) is the maximum Q-value for the next state s&rsquo; and all possible actions a&rsquo; that can be taken from s'
After the Q-value update, the agent moves to the next state and repeats the process until it reaches the terminal state.</p>
<p>As the agent continues to interact with the environment, the Q-table is gradually updated with more accurate estimates of the optimal Q-values. The agent uses the Q-table to select the optimal action in each state, based on the highest Q-value in the table for that state. By learning from experience and updating the Q-values over time, Q-learning allows the agent to make better decisions and maximize the cumulative reward it receives from the environment.</p>
<h2 id="its-a-me-mario">It&rsquo;s a me Mario<a hidden class="anchor" aria-hidden="true" href="#its-a-me-mario">#</a></h2>
<p>PyBoy is giving an <a href="https://github.com/Baekalfen/PyBoy/wiki/Scripts,-AI-and-Bots">inspiration</a> on how to set up the Agent for Super Mario.</p>
<p><strong>Environment</strong> The environment is in this case the world itself: The Floor, Pipes, Blocks the Background and of course the enemies. To capture the evironment means to capture the complete observation space in every single frame.</p>
<p><strong>Action</strong> The actions chosen could be all actions from the actual Window Event: <code>List[WindowEvent]</code> but for the game it would make absolutely no sense to test the Start Buttone or Select Button. Therefore we focus on LEFT,RIGHT and A.</p>
<pre><code>baseActions = [WindowEvent.PRESS_ARROW_RIGHT,
                        WindowEvent.PRESS_BUTTON_A, WindowEvent.PRESS_ARROW_LEFT]
</code></pre>
<p><strong>State</strong> Here it is defined by <code>GameState</code>.</p>
<p><strong>Reward</strong> The reward is defined by the progress in the game: deaths, time, level and movement.</p>
<pre><code>clock = current_mario.time_left - prevGameState.time_left
movement = current_mario.real_x_pos - prevGameState.real_x_pos
death = -15*(current_mario.lives_left - prevGameState.lives_left)
levelReward = 15*max((current_mario.world[0] - prevGameState.world[0]), (current_mario.world[1] - prevGameState.world[1])) # +15 if either new level or new world

reward = clock + death + movement + levelReward
</code></pre>
<h1 id="let-the-games-begin">Let the games begin<a hidden class="anchor" aria-hidden="true" href="#let-the-games-begin">#</a></h1>
<p>I built my own simplified agent on following example and had a ton of fun <a href="https://github.com/lixado/PyBoy-RL">here</a></p>
<h1 id="ressources">Ressources<a hidden class="anchor" aria-hidden="true" href="#ressources">#</a></h1>
<p><a href="https://github.com/Baekalfen/PyBoy">PyBoy Repo</a></p>
<p><a href="https://docs.pyboy.dk/index.html">PyBoy Documentation</a></p>
<p><a href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">Play SNES Super Mario with RL</a></p>
<p><a href="https://github.com/lixado/PyBoy-RL">Example RL for PyBoy</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.patrickschnass.de/tags/fun/">Fun</a></li>
      <li><a href="https://www.patrickschnass.de/tags/q-learning/">Q-Learning</a></li>
      <li><a href="https://www.patrickschnass.de/tags/reinforcement-learning/">Reinforcement Learning</a></li>
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on twitter"
        href="https://twitter.com/intent/tweet/?text=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f&amp;hashtags=Fun%2cQ-Learning%2cReinforcementLearning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f&amp;title=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario&amp;summary=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario&amp;source=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f&title=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on whatsapp"
        href="https://api.whatsapp.com/send?text=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario%20-%20https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share How to use a Reinforment Learning Agent to play Super Mario on telegram"
        href="https://telegram.me/share/url?text=How%20to%20use%20a%20Reinforment%20Learning%20Agent%20to%20play%20Super%20Mario&amp;url=https%3a%2f%2fwww.patrickschnass.de%2fposts%2fsuper_mario%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://www.patrickschnass.de/">patrickschnass.de</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
