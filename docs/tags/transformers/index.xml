<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Transformers on Notes on AI</title>
    <link>https://patrickpt.github.io/tags/transformers/</link>
    <description>Recent content in Transformers on Notes on AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 09 Jan 2023 11:50:57 +0000</lastBuildDate><atom:link href="https://patrickpt.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction to Transformers</title>
      <link>https://patrickpt.github.io/posts/2023_01_09_introduction_to_transformers/2023_01_09_introduction_to_transformers/</link>
      <pubDate>Mon, 09 Jan 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/2023_01_09_introduction_to_transformers/2023_01_09_introduction_to_transformers/</guid>
      <description>What are Transformers Transformer is a type of neural network architecture that was introduced in the paper &amp;ldquo;Attention is All You Need&amp;rdquo; by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification.
One of the key innovations of the Transformer architecture is the use of attention mechanisms. In a traditional neural network, each input is processed independently, without any information about the relationships between the inputs.</description>
    </item>
    
  </channel>
</rss>
