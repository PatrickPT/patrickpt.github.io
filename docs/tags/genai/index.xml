<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GenAI on Notes on AI</title>
    <link>https://patrickpt.github.io/tags/genai/</link>
    <description>Recent content in GenAI on Notes on AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 28 Jan 2024 09:30:57 +0000</lastBuildDate><atom:link href="https://patrickpt.github.io/tags/genai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Quantization in a nutshell</title>
      <link>https://patrickpt.github.io/posts/quantllm/</link>
      <pubDate>Sun, 28 Jan 2024 09:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/quantllm/</guid>
      <description>TL;DR This blogpost summarizes the buts and bolts of LLM quantization with llama.cpp.
Introduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the context of machine learning, refers to the process of reducing the precision of a model&amp;rsquo;s parameters, typically converting floating-point numbers to lower-bit representations. This has profound implications for model deployment, particularly in rendering sizable LLMs more accessible.
Understanding Quantization Quantization works by mapping the continuous range of floating-point values to a discrete set of levels.</description>
    </item>
    
    <item>
      <title>LLM Quantization in a nutshell</title>
      <link>https://patrickpt.github.io/posts/quantllm/</link>
      <pubDate>Sun, 28 Jan 2024 09:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/quantllm/</guid>
      <description>TL;DR This blog post explains the Byte Latent Transformer (BLT), a tokenizer-free architecture for NLP tasks. BLT processes raw byte data dynamically, making it a scalable, efficient, and robust alternative to traditional token-based models.
Why Should I Care? Traditional LLMs rely on tokenizationâ€”a preprocessing step that compresses text into a fixed vocabulary. While effective, tokenization introduces several challenges:
High Costs: Fine-tuning LLMs with domain-specific data demands extensive computational resources, often requiring significant financial investments.</description>
    </item>
    
    <item>
      <title>What is Parameter Efficient Finetuning?</title>
      <link>https://patrickpt.github.io/posts/peft/</link>
      <pubDate>Wed, 01 Nov 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/peft/</guid>
      <description>TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.
Why should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.</description>
    </item>
    
    <item>
      <title>Hands on with Retrieval Augmented Generation</title>
      <link>https://patrickpt.github.io/posts/hands-on-rag/</link>
      <pubDate>Sat, 07 Oct 2023 10:30:00 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/hands-on-rag/</guid>
      <description>TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model
Hands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.
Retrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM&amp;rsquo;s with long context windows.</description>
    </item>
    
    <item>
      <title>What is Retrieval Augmented Generation?</title>
      <link>https://patrickpt.github.io/posts/rag_intro/</link>
      <pubDate>Fri, 29 Sep 2023 18:30:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/rag_intro/</guid>
      <description>TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.
Why should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.
picture from Neo4J
But fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:</description>
    </item>
    
  </channel>
</rss>
