<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Foundations on patrickpt.github.io</title>
    <link>https://patrickpt.github.io/tags/foundations/</link>
    <description>Recent content in Foundations on patrickpt.github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 30 Aug 2023 11:50:57 +0000</lastBuildDate><atom:link href="https://patrickpt.github.io/tags/foundations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What are Transformers?</title>
      <link>https://patrickpt.github.io/posts/transformers/</link>
      <pubDate>Wed, 30 Aug 2023 11:50:57 +0000</pubDate>
      
      <guid>https://patrickpt.github.io/posts/transformers/</guid>
      <description>What are Transformers Transformers are a type of neural network architecture that was introduced in the paper &amp;ldquo;Attention is All You Need&amp;rdquo; by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification. Furthermore it is the foundation for Language Models and their application.
The key innovation of the Transformer architecture is the use of attention mechanisms.</description>
    </item>
    
  </channel>
</rss>
