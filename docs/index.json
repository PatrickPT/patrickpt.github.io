[{"content":"TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.\nWhy all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind. Its Open Source approach including a MIT license further amplifies its disruptive potential, enabling unrestricted commercial use, even for direct competitors, without costly R\u0026amp;D.\nBeyond accessibility, DeepSeek-R1 heats innovation by openly sharing its cost-effective training methodology, challenging the narrative that advanced AI requires exorbitant GPU investments. Operationally, it’s remarkably affordable: output tokens cost 30x less than OpenAI’s, and input tokens are 55x cheaper.\nR1’s blend of performance, openness, innovation and affordability could signal that open-source can still play a pivotal role in the AI race.\nIn s Nutshell Key Training Process \u0026amp; Innovations in DeepSeek-R1 1. Two Core Models DeepSeek-R1-Zero:\nPure RL Training: Trained directly on the base model (DeepSeek-V3-Base) without supervised fine-tuning (SFT). Algorithm: Uses GRPO (Group Relative Policy Optimization), a critic-free RL method that estimates baselines from group scores. Reward Design: Relies on rule-based rewards (accuracy + formatting) instead of neural reward models to avoid reward hacking. Emergent Behaviors: Self-verification, reflection, and extended reasoning (e.g., \u0026ldquo;aha moments\u0026rdquo;) emerged naturally during RL. Limitations: Poor readability, language mixing, and formatting inconsistencies. DeepSeek-R1:\nCold-Start Data: Starts with thousands of curated CoT examples to fine-tune the base model, improving readability and stability. Multi-Stage Training: Cold-Start SFT: Initial fine-tuning for readable CoT generation. Reasoning-Oriented RL: Applies GRPO with added language consistency rewards. Rejection Sampling + SFT: Generates high-quality data (600k reasoning + 200k non-reasoning samples) for retraining. Final RL Alignment: Balances reasoning performance with human preferences (helpfulness/harmlessness). 2. Key Innovations RL-First Approach:\nProves reasoning capabilities can be incentivized purely through RL without SFT (novel for open research). Achieves OpenAI-o1-0912-level performance (e.g., 71% → 86.7% on AIME with majority voting). Cold-Start Strategy:\nAddresses R1-Zero’s limitations by seeding RL with human-prioritized data (readability, structured outputs). Distillation Efficiency:\nSmaller models (1.5B–70B) distilled from DeepSeek-R1 outperform RL-trained counterparts (e.g., 72.6% vs. 47% on AIME for Qwen-32B). Distilled models surpass GPT-4o/Claude-3.5-Sonnet in math/coding tasks despite smaller size. Rule-Based Rewards:\nAvoids neural reward models, simplifying training and reducing hacking risks. 3. Critical Challenges \u0026amp; Insights Failed Attempts: Process Reward Models (PRMs): Struggled with fine-grained step validation and scalability. Monte Carlo Tree Search (MCTS): Token-generation complexity made iterative improvement impractical. Key Insight: Distillation is more cost-effective than RL for smaller models, but advancing SOTA requires large-scale RL on powerful base models. 4. Performance Highlights DeepSeek-R1: Matches OpenAI-o1-1217 on reasoning (79.8% pass@1 on AIME) and outperforms GPT-4o/Claude-3.5 in math/coding. Distilled Models: 7B model surpasses GPT-4o on MATH-500 (92.8% vs. 74.6%). 32B model outperforms QwQ-32B-Preview by 22.6% on AIME. Why It Stands Out:\nFirst open-source work validating pure RL for reasoning. Combines scalability (GRPO), human-aligned cold-start data, and efficient distillation. Open-sources models/data, enabling community-driven advancements. Understanding what DeepSeek did in more detail To gain a clearer insight into the core framework of DeepSeek-R1, let’s break down its foundational concepts:\nReinforcement Learning (RL): This approach involves a model learning through a system of rewards and penalties tied to its actions, refining its performance over time via trial and error. In the realm of large language models (LLMs), RL can be implemented through techniques such as policy optimization (e.g., Proximal Policy Optimization or PPO), value-based methods (e.g., Q-learning), or combined approaches like actor-critic architectures. For instance, when presented with a prompt like “2 + 2 =”, the model might receive a reward of +1 for generating the correct answer “4” and a penalty of -1 for any incorrect response. In advanced LLMs, rewards are often derived from human feedback (RLHF) or automated evaluation systems like GRPO.\nSupervised Fine-Tuning (SFT): This process involves retraining a base model using a labeled dataset to enhance its performance on a specific task. For example, an LLM could be fine-tuned with a dataset of customer service queries and responses to improve its accuracy in addressing common support questions. This method is particularly effective when a substantial amount of labeled data is available.\nCold Start Data: This refers to a small, minimally labeled dataset used to provide the model with a basic grasp of the task at hand. For instance, a chatbot might be fine-tuned using a simple dataset of frequently asked questions (FAQs) extracted from a website, helping it establish a foundational understanding. This approach is especially useful when labeled data is scarce.\nMulti-Stage Training: In this method, the model undergoes training in distinct phases, each targeting a specific improvement, such as accuracy or alignment with user expectations. For example, a model might first be trained on general text data and then further refined using reinforcement learning based on user feedback to enhance its conversational capabilities.\nRejection Sampling: This technique involves generating multiple potential outputs from a model, but only retaining those that meet predefined criteria, such as quality or relevance. For example, after a reinforcement learning process, the model might produce several responses, but only the most useful ones are selected for retraining or further use. This ensures that only high-quality outputs contribute to the model’s ongoing improvement.\nHow Does DeepSeek Work? DeepSeek-R1 is built on a multi-stage training regime that combines a carefully engineered supervised fine-tuning (SFT) phase with pure reinforcement learning (RL) techniques—most notably, a novel Group Relative Policy Optimization (GRPO) framework. This section outlines the architecture, training pipeline, and mathematical formulation underpinning DeepSeek-R1.\n1. Architectural Overview At its core, DeepSeek-R1 is based on a powerful base model (DeepSeek-V3-Base) which is subsequently refined through several stages:\nBase Model Initialization:\nThe process begins with a pre-trained large language model. This base model, having been trained on vast internet-scale data, provides the foundational language understanding and generative capabilities.\nChain-of-Thought (CoT) Representation:\nA unique aspect of DeepSeek-R1 is its explicit generation of chain-of-thought reasoning. During inference, the model produces both the reasoning steps (CoT) and the final answer. This transparency is achieved by encouraging multi-step reasoning during training.\n2. Multi-Stage Training Pipeline DeepSeek-R1’s training is divided into several well-defined phases:\na. Cold-Start Supervised Fine-Tuning (SFT) Objective:\nTo provide the model with an initial “readable” structure and coherent reasoning behavior. Method:\nA relatively small but high-quality dataset of thousands of chain-of-thought examples (referred to as cold-start data) is used to fine-tune the base model. Although the dataset is orders of magnitude smaller than typical supervised corpora, its curation for clarity and structured output is crucial for overcoming issues like language mixing and formatting inconsistencies. b. Pure Reinforcement Learning with GRPO Objective:\nTo enhance reasoning capabilities by learning directly from trial-and-error without any further labeled data.\nGRPO Overview:\nTraditional RL approaches in language modeling often employ a critic network or neural reward models. Instead, DeepSeek-R1 uses GRPO—a critic-free method where rewards are determined via rule-based measures (e.g., coherence, formatting, and logical consistency) and then compared relative to group-average scores.\nMathematical Formulation:\nLet the policy of the model be denoted as ( \\pi_\\theta(y|x) ) (with parameters (\\theta)), and assume a reference policy ( \\pi_{\\text{ref}}(y|x) ) derived from the cold-start SFT stage. For a given prompt ( x ) and two candidate responses ( y_w ) (winning) and ( y_l ) (losing), define the relative log-likelihood difference as:\n[ h_{\\pi_\\theta}(x, y_w, y_l) = \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} ]\nThe standard Direct Preference Optimization (DPO) loss (as used in earlier works) is given by:\n[ L_{\\text{DPO}}(\\pi_\\theta; D) = -\\mathbb{E}{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma\\left(\\beta \\cdot h{\\pi_\\theta}(x, y_w, y_l)\\right) \\right] ]\nwhere (\\sigma(\\cdot)) is the sigmoid function and (\\beta) is a scaling factor.\nIn GRPO, to account for multiple groups ( g \\in {1, \\ldots, K} ) (which may correspond to different task domains or user preferences), the objective is reformulated as a worst-case (minimax) problem:\n[ L_{\\text{GR}}(\\pi_\\theta) = \\max_{g \\in {1,\\ldots,K}} ; L_{\\text{DPO}}(\\pi_\\theta; D_g) ]\nAlternatively, using a weighted combination over groups with weights (\\alpha \\in \\Delta_K) (the (K)-simplex), we have:\n[ \\min_{\\pi_\\theta} ; \\max_{\\alpha \\in \\Delta_K} ; \\sum_{g=1}^{K} \\alpha_g ; \\mathbb{E}{(x_g, y_w, y_l) \\sim D_g} \\left[ -\\log \\sigma\\left(\\beta \\cdot h{\\pi_\\theta}(x_g, y_w, y_l)\\right) \\right] ]\nThis formulation ensures that groups with poorer performance (i.e., higher loss) receive higher weights during training, guiding the model to improve in those areas.\nGradient Update:\nThe gradient update for the parameters (\\theta) is derived from the weighted loss. If we denote the loss on a sample as:\n[ l(\\pi_\\theta; (x_g, y_w, y_l)) = \\log \\sigma\\left(\\beta \\cdot h_{\\pi_\\theta}(x_g, y_w, y_l)\\right) ]\nthen the gradient update (ignoring normalization factors) is:\n[ \\nabla_\\theta l(\\pi_\\theta; (x_g, y_w, y_l)) \\propto \\sigma\\Big(r_\\theta(x_g, y_l) - r_\\theta(x_g, y_w)\\Big) \\left[ \\nabla_\\theta \\log \\pi_\\theta(y_w|x_g) - \\nabla_\\theta \\log \\pi_\\theta(y_l|x_g) \\right] ]\nwhere ( r_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} ). This update explicitly increases the probability of the preferred response while decreasing that of the rejected one, with additional weighting from the group-specific factors (\\alpha_g).\nc. Rejection Sampling and Synthetic Data Generation Objective:\nTo refine the model’s outputs by selecting only high-quality samples. Method:\nOnce the RL process has nearly converged, the model generates multiple outputs for a given prompt. A rejection sampling mechanism filters these outputs using the same rule-based criteria (coherence, formatting, etc.), creating a synthetic dataset of high-quality reasoning examples. This dataset typically contains on the order of 600k reasoning samples and 200k non-reasoning samples, which are then used to further fine-tune the model via supervised training. d. Final RL Alignment Objective:\nTo balance high-level reasoning with human-aligned attributes such as helpfulness and harmlessness. Method:\nA final round of RL is applied over diverse prompts. This stage consolidates improvements from the previous stages while ensuring that the model’s behavior aligns with user expectations. The final objective remains similar to the GRPO loss, ensuring that no particular subgroup (or aspect of the task) is neglected. 3. Inference: Chain-of-Thought Generation During inference, DeepSeek-R1 generates a two-part output:\nReasoning Content:\nThe intermediate chain-of-thought (CoT) that details the step-by-step reasoning process. Final Answer:\nThe concise output generated after the reasoning steps. The dual output is a direct consequence of the multi-stage training pipeline. The training not only instills raw reasoning power but also structures the output to separate the thought process from the final answer—an innovation that facilitates transparency and error analysis.\n4. Distillation into Smaller Models An additional contribution of DeepSeek-R1 is the distillation of its reasoning capabilities into smaller models (ranging from 1.5B to 70B parameters). The distilled models inherit the chain-of-thought behavior and high reasoning performance, often outperforming models trained solely with RL. This distillation leverages the observation that reasoning patterns discovered by larger models can be effectively transferred to compact architectures without sacrificing performance.\nSummary DeepSeek-R1 works through a meticulously designed multi-stage training process:\nCold-Start SFT to provide a solid, human-readable foundation. Pure RL using GRPO to boost reasoning capability by optimizing for worst-case group performance. Rejection Sampling and SFT to refine outputs by creating a high-quality synthetic dataset. Final RL Alignment to ensure the model adheres to human values and achieves a balanced performance. Mathematically, the model’s optimization is expressed via a robust version of the Direct Preference Optimization (DPO) loss, where group-specific losses are balanced through a minimax formulation.\nImplications The release of DeepSeek-R1 was not just another model but it represents a shift in how advanced LLMs can be built and deployed:\nDemocratization of Advanced AI: Against the overwhelming trend of closed-source models, DeepSeek-R1 is open-sourced and still reaches on-par performance with commercial models. This openness invites collaborative improvements and accelerates innovation in AI.\nCost-Effective Scaling: DeepSeek-R1 dramatically reduces token costs (30× cheaper for outputs and 55× cheaper for inputs than comparable commercial models). This cost efficiency challenges the narrative that state-of-the-art reasoning demands vast GPU clusters and massive labeled datasets, initiating new innovation in AI training paradigms.\nValidation of Pure RL for Reasoning: The success of DeepSeek-R1-Zero proves that advanced reasoning can emerge solely through reinforcement learning. This breakthrough inspires further research into RL-first training paradigms, potentially reshaping how future LLMs are developed.\nInfluence on Commercial Strategies: By openly publishing its training methods—including its efficient distillation and group-based optimization techniques—DeepSeek-R1 pressures commercial entities to adopt more transparent and community-friendly approaches. This could lead to a more competitive and innovative AI ecosystem.\nPitfalls While DeepSeek-R1 sets a new standard, several challenges remain:\nReadability and Formatting:\nThe initial R1-Zero model exhibited issues such as poor readability and language mixing. Although the multi-stage training process addresses these concerns, some residual inconsistencies may persist in complex scenarios.\nInference Latency:\nDeepSeek-R1’s chain-of-thought generation is computationally intensive. Inference times are slower compared to models optimized solely for speed, which may limit its use in latency-critical applications.\nLimited Inference Flexibility:\nThe current API version does not support many adjustable parameters (e.g., temperature, top_p, etc.), making it harder to fine-tune output behavior for production environments.\nRisk of Overfitting to Rule-Based Rewards:\nRelying on fixed, rule-based rewards simplifies training but may also constrain the model’s adaptability. In cases where nuanced human judgment is required, this approach might not capture every subtlety.\nScalability of Pure RL:\nAlthough pure RL has proven effective here, it typically requires longer training times and can be sensitive to reward design. The balance between cost-effectiveness and training complexity remains a delicate one.\nConclusion In summary, DeepSeek-R1 shows that it’s possible to achieve advanced reasoning capabilities in large language models without the traditionally high costs. By using a modest yet carefully curated cold-start supervised phase combined with pure reinforcement learning via GRPO—and further refined with rejection sampling—the model delivers performance that can stand alongside commercial systems like those from OpenAI. Its open-source approach and transparent training process offer practical benefits for both researchers and practitioners.\nThat said, DeepSeek-R1 is not without its challenges. There remain issues with output readability, inference speed, and some limitations in fine-tuning flexibility. These factors indicate that there is still room for improvement as we continue to explore and refine these methods.\nOverall, DeepSeek-R1 represents a thoughtful step forward in the development of cost-efficient, robust, and accessible AI. As the community builds on these ideas, we can expect further progress that will help broaden the range of available tools and techniques in the field.\nRessources https://thelmbook.com/articles/#!./DeepSeek-R1.md\nhttps://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\nhttps://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it\nhttps://arxiv.org/pdf/2405.20304\n","permalink":"https://www.patrickschnass.de/posts/deepseek/","summary":"TL;DR This blog post explains how the recent release of DeepSeek may benefit the open source community and why it is considered a game changer for AI industry.\nWhy all the rumors? DeepSeek-R1 represents a significant innovation in the AI landscape, outperforming or rivaling top commercial models including reasoning capabilities. Previously, such sophisticated models were exclusive to tech giants like OpenAI and Google, but R1 now joins this category as the only open-weights model of its kind.","title":"Why DeepSeek-R1 was a significant step for Open Source AI"},{"content":"TL;DR This blog post explains the Byte Latent Transformer (BLT), a tokenizer-free architecture for NLP tasks. BLT processes raw byte data dynamically, making it a scalable, efficient, and robust alternative to traditional token-based models.\nWhy Should I Care? Traditional LLMs rely on tokenization—a preprocessing step that compresses text into a fixed vocabulary. While effective, tokenization introduces several challenges:\nHigh Costs: Fine-tuning LLMs with domain-specific data demands extensive computational resources, often requiring significant financial investments. Even cloud-based solutions incur high costs, making this approach inaccessible for many organizations. Limited Robustness: Tokenized models are sensitive to input noise, domain-specific jargon, and multilingual data, leading to degraded performance or hallucinated outputs when encountering unfamiliar structures or formats. Retraining Requirements: Token-based architectures require frequent retraining to incorporate new information, which is both time-consuming and computationally expensive. Additionally, retraining does not guarantee the model’s understanding aligns perfectly with evolving data. BLT addresses these limitations by eliminating tokenization and dynamically processing raw byte data. This enables:\nSimpler Pipelines: The removal of tokenization simplifies preprocessing and reduces dependencies on domain-specific tools. Improved Robustness: BLT’s byte-level approach inherently handles noise, rare words, and multilingual inputs more effectively. Efficient Scaling: By dynamically adjusting compute resources via entropy-based patching, BLT scales efficiently to meet diverse data demands. Understanding the Byte Latent Transformer What is BLT? BLT is a novel architecture that replaces tokenization with dynamic patching. Instead of relying on a predefined vocabulary, BLT processes raw bytes directly, grouping them into variable-sized patches based on data complexity. This approach allows the model to allocate computational resources where they are most needed.\nKey Features Dynamic Patching:\nBytes are segmented into patches based on entropy, which measures data complexity. High-entropy regions (e.g., complex sentences or rare words) are allocated finer patches, ensuring sufficient model focus. Conversely, low-entropy regions (e.g., repetitive sequences) are compressed into coarser patches to save resources. Three-Component Architecture:\nLocal Encoder: This lightweight module transforms raw byte sequences into patch-level embeddings. It uses a series of small transformer layers to maintain computational efficiency while extracting rich representations. Latent Transformer: Acting as the central processing unit, the latent transformer operates globally on patches, capturing long-range dependencies and contextual relationships. Local Decoder: The decoder reconstructs byte-level outputs from patch embeddings, ensuring accurate representation of the original data. Efficient Scaling:\nBLT models scale gracefully, achieving competitive performance with token-based models like LLaMA while requiring fewer computational resources. This scalability extends to both model size and training data volume. Efficiency Gains: Compute vs. Performance The Problem with Tokenization Tokenization imposes equal computational effort on all tokens, regardless of their complexity. For example, simple words or repetitive phrases consume the same resources as complex or ambiguous structures. This inefficiency results in wasted computational power and slower processing.\nThe BLT Advantage Better Scaling: BLT’s dynamic patching ensures that compute resources are concentrated on high-complexity regions. Experiments show that BLT outperforms tokenized models in bits-per-byte (BPB) performance, a key metric for language modeling efficiency. Improved Inference Efficiency: By grouping predictable data into larger patches, BLT reduces inference costs by up to 50%, making it highly attractive for real-world deployments. Robustness: BLT’s byte-level granularity enhances its ability to handle noisy or domain-specific inputs, outperforming traditional models in these scenarios. How Does BLT Work? 1. Patching Module Indexing: Bytes are segmented into patches using entropy thresholds. This step involves analyzing the predictability of each byte and dynamically determining patch boundaries. Dynamic Allocation: High-entropy regions result in smaller, finer-grained patches, allowing the model to dedicate more attention to complex parts of the input. Low-entropy regions are grouped into larger patches, optimizing computational efficiency. 2. Processing Module Local Encoder: Converts byte sequences into patch embeddings through lightweight transformer layers. It uses local attention mechanisms to capture relationships within each patch efficiently. Latent Transformer: Processes patch embeddings globally, leveraging its ability to focus on relevant parts of the data while ignoring redundant information. Local Decoder: Decodes patch representations back into bytes for output. This step ensures that the model’s predictions maintain high fidelity to the input. 3. Scaling and Flexibility BLT’s architecture supports simultaneous scaling of model and patch size. Larger patches save compute during inference, allowing resources to be reallocated to increase model capacity or process longer contexts. Practical Implications Simplified Implementation: By removing tokenization, BLT eliminates the need for domain-specific vocabularies and reduces preprocessing overhead. This simplification accelerates deployment and maintenance. Enhanced Robustness: BLT’s byte-level approach handles diverse data formats and noisy inputs more effectively, making it ideal for applications in multilingual and low-resource settings. Scalable Design: BLT’s dynamic patching mechanism ensures efficient handling of large datasets, whether in training or inference, making it well-suited for enterprise-level applications. Benefits of Using BLT Improved Precision: BLT generates outputs grounded in byte-level details, reducing reliance on pre-learned token structures. Addressing Knowledge Gaps: By bypassing tokenization, BLT can adapt seamlessly to domain-specific data and rare linguistic patterns. Scalability: Its dynamic architecture efficiently scales with data size and complexity, making it adaptable for varied use cases. Reduced Hallucinations: By dynamically allocating resources, BLT mitigates the risks of generating inaccurate or nonsensical outputs. Pitfalls Like all architectures, BLT has trade-offs:\nComplexity of Implementation: While tokenization is eliminated, the dynamic patching mechanism introduces additional layers of design and debugging complexity. Assumptions in Patching: BLT’s reliance on entropy-based segmentation assumes semantic similarity between high-entropy regions and important content, which may not always hold true. Conclusion The Byte Latent Transformer changes the way we look at tokenization for NLP modeling. The paper by Meta introduces the way to a robust alternative to traditional architectures.\nRessources 13.12.2024 - Byte Latent Transformer: Patches Scale Better Than Tokens\n","permalink":"https://www.patrickschnass.de/posts/blt/","summary":"TL;DR This blog post explains the Byte Latent Transformer (BLT), a tokenizer-free architecture for NLP tasks. BLT processes raw byte data dynamically, making it a scalable, efficient, and robust alternative to traditional token-based models.\nWhy Should I Care? Traditional LLMs rely on tokenization—a preprocessing step that compresses text into a fixed vocabulary. While effective, tokenization introduces several challenges:\nHigh Costs: Fine-tuning LLMs with domain-specific data demands extensive computational resources, often requiring significant financial investments.","title":"A view on Byte Latent Transformers"},{"content":"LLM Quantization in a Nutshell: A Detailed Exploration Quantization is a critical technique to reduce the precision of large language models (LLMs), making them lighter and more efficient without a significant loss in performance. This post delves into the nuts and bolts of quantization, explains the underlying math, and examines practical aspects using tools like llama.cpp.\nTL;DR LLM quantization reduces the numerical precision of model parameters to decrease memory usage and improve inference speed. Whether applying post-training quantization or quantization-aware training, the goal is to strike an optimal balance between model efficiency and accuracy. Tools like llama.cpp make deploying quantized LLMs on commodity hardware increasingly accessible.\nIntroduction to Quantization The Technical Foundation of LLM Quantization Quantization, in the machine learning context, means mapping a continuous range of values (usually 32-bit floating points) to a discrete set (such as 8-bit integers). This conversion is similar to reducing the bit depth of an image or audio file: fewer bits mean less data to store and process, which is essential when working with LLMs that can contain billions of parameters.\nIllustration: Quantization Process\nThe above diagram represents how continuous floating-point numbers are mapped to a set of discrete values via scaling and rounding.\nUnderstanding Quantization Quantization relies on two fundamental operations:\nScaling: Adjusting the range of values so they fit within a target precision. Rounding/Truncation: Converting these scaled values into discrete numbers. Mathematically, for a given weight ( w ), quantization typically follows:\n[ q = \\text{round}\\left(\\frac{w}{s}\\right) ]\nwhere:\n( q ) is the quantized integer. ( s ) is the scaling factor determined from the range of ( w ). To recover an approximation of the original weight during inference, the formula is inverted:\n[ w \\approx s \\times q ]\nThis simple yet powerful operation is applied to all model parameters, significantly reducing the overall memory footprint.\nPost-Training Quantization Post-training quantization (PTQ) is the process of converting a fully trained model\u0026rsquo;s weights to a lower precision after training is complete. PTQ is attractive for its simplicity and speed. However, since the weights are not optimized for low precision during training, a slight degradation in model performance may occur.\nQuantization-Aware Training Quantization-aware training (QAT) integrates the quantization process into the training phase. By simulating low-precision arithmetic during training, the model learns to adjust its weights to mitigate precision loss. This approach generally yields better performance than PTQ, especially when using very low bit-widths.\nOptimization Techniques in Quantization Once a model is quantized, further optimizations become possible. Choosing the appropriate bit-width is crucial—too few bits and you risk significant accuracy loss; too many and the efficiency gains are marginal. Techniques such as dynamic scaling, careful calibration, and even layer-wise mixed precision are employed to fine-tune performance.\nMathematical Note:\nA common enhanced quantization formula incorporates a zero-point ( z ) to better handle distributions centered around zero:\n[ q = \\text{clip}\\left(\\text{round}\\left(\\frac{w}{s}\\right) + z,, q_{\\text{min}},, q_{\\text{max}}\\right) ]\nThis modification helps minimize the quantization error, ensuring the overall model accuracy remains high.\nA Technical Examination Diving Deeper into llama.cpp llama.cpp is a C++ library designed for running quantized LLaMA models efficiently on CPUs. Its primary goal is to facilitate inference with low-precision models by taking advantage of integer arithmetic. The library’s lightweight design makes it particularly useful for deploying large models on less powerful hardware.\nQuantization in llama.cpp Using llama.cpp involves several key steps:\nInstallation \u0026amp; Setup:\nInstall the library and its dependencies. Model Conversion:\nConvert a pre-trained LLaMA model into a quantized format using provided scripts. Inference:\nLoad the quantized model and perform inference efficiently using the optimized C++ routines. The tool’s script outputs a quantized model file that is specifically formatted for efficient CPU inference.\nStoring Quantized Models: GGML \u0026amp; GGUF Quantized models are typically stored in binary formats such as GGML or its successor, GGUF.\nGGML: Developed by Georgi Gerganov, GGML provides a compact, CPU-friendly storage format. GGUF: An evolution of GGML, GGUF is more extensible and optimized for reduced RAM usage. Where to Find Models For those looking to experiment with quantized LLMs, the Hugging Face Hub offers a range of pre-quantized models. These models are available in various configurations, allowing users to select a balance between efficiency and accuracy based on their specific use case.\nConclusion Advantages of Quantization Quantization reduces the memory footprint and computational requirements of LLMs. This is especially beneficial when deploying models on edge devices or in environments with limited resources. Additional benefits include:\nReduced Energy Consumption: Lower precision arithmetic requires less power. Faster Inference: More efficient computation enables real-time applications. Challenges and Considerations While quantization offers compelling benefits, it is not without challenges:\nAccuracy Trade-offs: Lowering precision can lead to minor accuracy degradation, necessitating careful calibration and evaluation. Tooling Complexity: Integrating quantization into the training and inference pipelines may require additional tooling and expertise. Practical Expertise in Quantized LLMs Mastering quantization involves balancing theory and practice. As techniques evolve, tools like llama.cpp continue to push the boundaries of what is possible, making it easier to deploy efficient LLMs in production environments.\nFinal Thought:\nQuantization is a key enabler for the next generation of LLM deployments, transforming models into agile, resource-friendly systems without sacrificing their impressive capabilities.\nFor further reading and hands-on experimentation, explore the quantized models on Hugging Face and the llama.cpp repository on GitHub.\n","permalink":"https://www.patrickschnass.de/posts/quantllm/","summary":"LLM Quantization in a Nutshell: A Detailed Exploration Quantization is a critical technique to reduce the precision of large language models (LLMs), making them lighter and more efficient without a significant loss in performance. This post delves into the nuts and bolts of quantization, explains the underlying math, and examines practical aspects using tools like llama.cpp.\nTL;DR LLM quantization reduces the numerical precision of model parameters to decrease memory usage and improve inference speed.","title":"LLM Quantization in a nutshell"},{"content":"TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.\nWhy should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.\nParameter Efficient Fine-Tuning is a collection of techniques that aim to reduce computational and storage resources during the fine-tuning process. Rather than tuning all parameters, these methods strategically select and fine-tune a fraction of them. The rest of the parameters are frozen or updated with a lower precision format, saving memory and computational requirements. This technique also mitigates the risk of overfitting, enhances learning efficiency, and allows for more adaptability in applying large language models across a variety of tasks.\nUse Parameter-Efficient Fine-Tuning The process of parameter-efficient fine-tuning (PEFT) may exhibit variations depending on the specific implementation and the pre-trained model in use. Nevertheless, below are summarized all steps involved in PEFT:\nPre-training: Initially, a large-scale model undergoes pre-training on a substantial dataset, commonly for a generic task like image classification or language modeling. This phase equips the model with foundational knowledge and meaningful data representations. Task-specific dataset: Assemble or generate a dataset tailored to the particular task for which you intend to fine-tune the pre-trained model. This dataset must be labeled and faithfully represent the target task.\nParameter identification: Identify or estimate the significance and relevance of parameters within the pre-trained model for the target task. This step helps in discerning which parameters should be prioritized during the fine-tuning process. Techniques such as importance estimation, sensitivity analysis, or gradient-based methods can be employed for parameter assessment.\nSubset selection: Choose a subset of the pre-trained model\u0026rsquo;s parameters based on their importance or applicability to the target task. The selection process can involve setting specific criteria, like a threshold on importance scores or selecting the top-k most relevant parameters.\nFine-tuning: Initialize the chosen subset of parameters with values from the pre-trained model and lock the remaining parameters. Fine-tune the selected parameters by employing the task-specific dataset. This typically entails training the model on the target task data using optimization techniques like Stochastic Gradient Descent (SGD) or Adam.\nEvaluation: Assess the performance of the fine-tuned model on a validation set or by utilizing relevant evaluation metrics for the target task. This step serves to gauge the efficacy of PEFT in achieving the desired performance while reducing the number of parameters.\nIterative refinement (optional): Depending on performance and specific requirements, you may opt to iterate and refine the PEFT process. This can involve adjusting the criteria for parameter selection, exploring different subsets, or conducting additional fine-tuning epochs to further optimize the model\u0026rsquo;s performance. It\u0026rsquo;s crucial to note that the specific implementation details and techniques employed in PEFT can differ across research papers and real-world applications.\nPEFT Techniques Adapter Modules Adapter modules represent a specialized type of component that can be integrated into pre-trained language models to tailor their hidden representations during the fine-tuning process. These adapters are typically inserted following the multi-head attention and feed-forward layers within the transformer architecture, allowing for selective updates to the adapter parameters while keeping the remainder of the model parameters unchanged.\nIncorporating adapters is a straightforward procedure. The essential steps involve adding adapter modules to each transformer layer and positioning a classifier layer atop the pre-trained model. By updating the parameters of the adapters and the classifier head, one can enhance the pre-trained model\u0026rsquo;s performance on specific tasks without the need for a comprehensive model update. This approach not only saves time but also conserves computational resources while delivering notable results.\nBut how does fine-tuning using an adapter actually work? The adapter module itself consists of two feed-forward projection layers linked by a non-linear activation layer, and it incorporates a skip connection that bypasses the feed-forward layers.\nConsider an adapter placed immediately after the multi-head attention layer. In this case, the input to the adapter layer is the hidden representation denoted as h' derived from the multi-head attention layer. Within the adapter layer, h follows two distinct paths: one through the skip connection, which leaves the input unaltered, and the other through the feed-forward layers, facilitating the necessary modifications.\npicture from A Guide to PEFT\nTo begin, the first feed-forward layer initially projects h into a lower-dimensional space, which has fewer dimensions than h itself. Subsequently, the input traverses through a non-linear activation function, and the second feed-forward layer then reprojects it back to the same dimensionality as h. The outcomes generated by these two paths are combined through summation to yield the adapter module\u0026rsquo;s ultimate output.\nThe skip-connection dutifully maintains the original input h of the adapter, while the feed-forward path generates an incremental alteration, denoted as Δh, predicated on the original h. By introducing this incremental change, Δh, acquired from the feed-forward layer, to the original h from the previous layer, the adapter orchestrates a modification to the hidden representation calculated by the pre-trained model. This process empowers the adapter to adapt the pre-trained model\u0026rsquo;s hidden representation, thereby influencing its output for a specific task.\nLoRA Low-Rank Adaptation (LoRA) in the context of fine-tuning large language models offers an alternative approach for tailoring models to specific tasks or domains. Much like adapters, LoRA is a compact trainable submodule that seamlessly integrates into the transformer architecture. LoRA operation involves preserving the pre-trained model weights while introducing trainable rank decomposition matrices into each layer of the transformer architecture. This process significantly reduces the number of trainable parameters for downstream tasks. Remarkably, LoRA can achieve a reduction of up to 10,000 times in the number of trainable parameters and reduce GPU memory requirements by a factor of 3, all while maintaining or surpassing the performance of fine-tuned models across various tasks. LoRA also facilitates more efficient task-switching, lowers hardware constraints, and incurs no additional inference latency.\nSo, how does LoRA function? LoRA modules are inserted in parallel with the components of the pre-trained transformer model, specifically adjacent to the feed-forward layers. Each feed-forward layer includes two projection layers separated by a non-linear layer, wherein an input vector is transformed into an output vector with different dimensionality using an affine transformation. The LoRA layers are positioned alongside each of the two feed-forward layers.\npicture from A Guide to PEFT\nNow, let\u0026rsquo;s delve into the interaction between the feed-forward up-project layer and the adjacent LoRA module. The original parameters of the feed-forward layer take the output from the preceding layer, which is represented by dmodel, and project it into dFFW (where FFW stands for feed-forward). The adjacent LoRA module comprises two feed-forward layers. The first of these takes the same input as the feed-forward up-project layer and projects it into an r-dimensional vector, significantly smaller than dmodel. Subsequently, the second feed-forward layer transforms this vector into another vector with a dimensionality of dFFW. The two vectors are then combined through addition to create the final representation.\nAs discussed earlier, fine-tuning involves adjusting the hidden representation h computed by the original transformer model. In this context, the hidden representation generated by the feed-forward up-project layer of the original transformer corresponds to h. Simultaneously, the vector computed by the LoRA module represents the incremental change Δh applied to modify the original h. Consequently, the summation of the original representation and the incremental change yields the updated hidden representation h.\nBy incorporating LoRA modules alongside the feed-forward layers and a classifier head atop the pre-trained model, task-specific parameters for each individual task are kept to a minimum.\nQLoRA Compressing model parameters even more, QLoRA – or Quantized LoRA – takes the concepts of LoRA and applies a quantization technique. Instead of storing these new low-rank parameters in standard floating-point format, QLoRA stores them in a lower precision format, such as INT8 or INT4.\nThis method drastically reduces the number of bits required to store these parameters. As a result, QLoRA offers a significant reduction in the memory and computational requirements, vastly improving parameter efficiency during the fine-tuning process.\nPrefix Tuning Prefix-tuning offers a lightweight and cost-effective alternative to the conventional fine-tuning of large pre-trained language models for natural language generation tasks. Traditional fine-tuning entails the comprehensive updating and storage of all model parameters for each specific task, an endeavor that can be financially burdensome given the expansive scale of contemporary models. In contrast, prefix-tuning maintains the pre-trained language model parameters unchanged and focuses on optimizing a small, continuous, task-specific vector known as the \u0026ldquo;prefix.\u0026rdquo; In prefix-tuning, the prefix constitutes a set of independent parameters that undergo training alongside the language model. The primary aim of prefix-tuning is to discover a context that guides the language model to generate text that effectively addresses a particular task.\nThe prefix can be perceived as a sequence of \u0026ldquo;virtual tokens\u0026rdquo; to which subsequent tokens can attend. Remarkably, by updating a mere 0.1% of the model\u0026rsquo;s parameters, prefix-tuning manages to achieve performance comparable to traditional fine-tuning in full-data settings, outperforming it in scenarios with limited data, and exhibiting superior extrapolation to examples featuring topics not encountered during training.\npicture from A Guide to PEFT\nMuch like the aforementioned PEFT techniques, the ultimate objective of prefix-tuning is to attain h prime (h'). Prefix-tuning utilizes these prefixes to adapt the hidden representations derived from the original pre-trained language models. The modification is achieved by adding the incremental change Δh to the initial hidden representation h' resulting in the modified representation h prime (h').\nIn the realm of prefix-tuning, only the prefixes undergo updates, while the remainder of the model\u0026rsquo;s layers remain static and unchanged. This focused approach allows for enhanced efficiency and economy in addressing natural language generation tasks.\nPrompt Tuning Prompt tuning stands as a potent PEFT technique designed for the precise adaptation of pre-trained language models to specific downstream tasks. Unlike the conventional \u0026ldquo;model tuning\u0026rdquo; approach, where every parameter in the pre-trained model undergoes adjustments for each task, prompt tuning introduces the concept of learning soft prompts through backpropagation. These soft prompts can be fine-tuned for distinct tasks by incorporating labeled examples. The results are remarkable, as prompt tuning excels in outperforming few-shot learning methods like GPT-3, particularly gaining competitiveness as model sizes expand. It bolsters the robustness of domain transfer and opens the door to efficient prompt ensembling. Unlike model tuning, which necessitates creating task-specific copies of the entire pre-trained model for each task, prompt tuning simply requires storing a small task-specific prompt for each task. This approach simplifies the process of reusing a single frozen model across a multitude of downstream tasks.\npicture from A Guide to PEFT\nPrompt tuning serves as a simplified variant of prefix tuning. In this method, specific vectors are appended at the commencement of a sequence, specifically at the input layer. When presented with an input sentence, the embedding layer proceeds to convert each token into its corresponding word embedding, while the prefix embeddings are prepended to the sequence of token embeddings. Subsequently, the pre-trained transformer layers process the entire embedding sequence in a manner akin to how a transformer model treats a standard sequence. During the fine-tuning process, only the prefix embeddings undergo adjustments, while the remainder of the transformer model remains locked and unaltered.\nPrompt tuning offers numerous advantages compared to traditional fine-tuning approaches. It significantly enhances efficiency and mitigates computational demands. Furthermore, the exclusive fine-tuning of prefix embeddings minimizes the risk of overfitting to the training data, thereby producing models that are more resilient and capable of generalizing effectively.\nBenefits Cost Savings: PEFT significantly reduces computational and storage costs by fine-tuning a small number of additional model parameters while keeping most of the pre-trained LLM parameters frozen.\nMitigating Knowledge Loss: PEFT effectively addresses the issue of catastrophic forgetting that can occur during full fine-tuning of LLMs, as it updates only a limited set of parameters.\nEnhanced Performance in Low-Data Scenarios: PEFT methods have demonstrated superior performance in situations with limited data, and they exhibit better generalization to out-of-domain scenarios compared to full fine-tuning.\nPortability Advantage: PEFT approaches enable users to obtain compact checkpoints, typically a few megabytes in size, in contrast to the larger checkpoints produced by full fine-tuning. This makes it convenient to deploy and utilize the trained weights from PEFT for various tasks without the need to replace the entire model.\nComparable Performance to Full Fine-Tuning: PEFT allows achieving performance on par with full fine-tuning while utilizing only a small number of trainable parameters.\nConclusion The challenges associated with fine-tuning large language models, such as high memory requirements and computational burden, have prompted researchers to create innovative solutions like LoRa, QLoRa and other parameter efficient fine-tuning methods. By creating a balance between computational power and performance, these techniques make the power of large language models more accessible and applicable to an array of tasks. As natural language processing continues to advance, the importance of such computation-friendly and efficient methods will undoubtedly continue to grow.\nResources https://github.com/huggingface/peft [Repo]\nIntroduction to PEFT [Blogpost]\nA Guide to PEFT [Blogpost]\nA practical guide to PEFT [Blogpost]\n","permalink":"https://www.patrickschnass.de/posts/peft/","summary":"TL;DR Parameter Efficient Fine-Tuning is a technique that aims to reduce computational and storage resources during the fine-tuning of Large Language Models.\nWhy should i care? Fine-tuning is a common technique used to enhance the performance of large language models. Essentially, fine-tuning involves training a pre-trained model on a new, similar task. It has become a crucial step in model optimization. However, when these models consist of billions of parameters, fine-tuning becomes computational and storage heavy, leading to the development of Parameter Efficient Fine-Tuning methods.","title":"What is Parameter Efficient Finetuning?"},{"content":"TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model\nHands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.\nRetrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM\u0026rsquo;s with long context windows.\nPrerequisites All the code mentioned here can be found on github. The code can be run in a Docker container(even on a Raspberry Pi if you like). You need to add contextual data which you want to query and also use an API Key from OpenAI.\nDependencies The complete python code is containerized with docker and can be run via docker compose. It uses the following main dependencies:\nstreamlit as an easy to use an easy to implement Frontend. No need to set up Flask and debug through your CSS. Streamlit is open-source. llama_index which is used to build the retrieval engine. It is a simple, flexible data framework for connecting custom data sources to large language models. It is somehow similar to LangChain openai provides access to the OpenAI API Other packages are used to convert the context data. All dependencies can be found in the requirements.txt\nOverview The Knowledge Bot is a web-based chatbot that provides information and answers questions related to any data which is given as context based on Retrieval Augmented Generation Architecture. It utilizes the llama_index library for data indexing and OpenAI\u0026rsquo;s GPT-3.5-Turbo model for generating responses.\nThe chatbot is designed to assist users in finding information by answering questions based on indexed documents.\npicture from streamlit\nFeatures Ask questions related to your indexed documents. Receive informative responses based on indexed data. Convenient web-based interface powered by Streamlit. Setup To run the Knowledge Bot locally with docker, follow these steps:\nClone this repository to your local machine:\ngit clone https://github.com/PatrickPT/RAG_LLM_example.git Create your OpenAI Key\ncd RAG_LLM_example cd .streamlit nano .streamlit/secrets.toml # Insert your API Key as openai_key = \u0026#34;API Key\u0026#34; and save Create your documents or change the input_dir parameter in config.yaml to your folder(which needs to be accessible from the docker container)\ncd data # Insert the contextual documents the LLM should use in that folder Change the config.yaml file accordingly to your prior changes\n-config: api: gpt-3.5-turbo info: This bot knows everything about PromptEngineering which is mentioned in the guides in https://www.promptingguide.ai/ input_dir: ./data name: Knowledge Bot system_prompt: You are an expert on Prompt Engineering and Retrieval Augmented Generation with Large Language Models. Assume that all questions are related to Prompt Engineering. Keep your answers technical and based on facts. Do not hallucinate features. Run docker compose\ndocker compose up -d PS: content in /.streamlit and /data is ignored by git.\nCode This small project is a Streamlit-based web application that serves as a chatbot powered by the \u0026ldquo;llama_index\u0026rdquo; package and OpenAI\u0026rsquo;s GPT-3.5-Turbo model. It allows users to ask questions related to all documents which are stored in /data and provides informative responses.\nSeveral libraries, including streamlit, llama_index, openai, and others are imported.\nimport streamlit as st from llama_index import VectorStoreIndex, ServiceContext, Document from llama_index.llms import OpenAI import openai from llama_index import SimpleDirectoryReader import yaml The configuration is imported from config.yaml\nwith open(\u0026quot;config.yaml\u0026quot;, \u0026quot;r\u0026quot;) as yamlfile: config = yaml.load(yamlfile, Loader=yaml.FullLoader) # import configuration from yaml name = config[0]['config']['name'] info = config[0]['config']['info'] input_dir = config[0]['config']['input_dir'] system_prompt = config[0]['config']['system_prompt'] api = config[0]['config']['api'] The Streamlit app\u0026rsquo;s title, icon, layout, and sidebar state are configured.\n# Set Streamlit page configuration st.set_page_config( page_title=name, page_icon=\u0026quot;🦙\u0026quot;, layout=\u0026quot;centered\u0026quot;, initial_sidebar_state=\u0026quot;auto\u0026quot;, menu_items=None ) OpenAI API key is set using a secret obtained from Streamlit secrets. The key is stored in /.streamlit/secrets.toml\n# Set OpenAI API key openai.api_key = st.secrets.openai_key Create the main interface: title and information message about the bot\u0026rsquo;s capabilities is configured.\n# Create main interface st.title(name) st.info(info, icon=\u0026quot;📃\u0026quot;) A list called messages is initialized in Streamlit session state, which will be used to store the chat history.\n# Initialize the chat messages history if \u0026quot;messages\u0026quot; not in st.session_state.keys(): st.session_state.messages = [ {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Ask me a question\u0026quot;} ] A function called load_data is built, that loads and indexes data from /data. This data is used for responding to user queries.\n# Function to load data @st.cache_resource(show_spinner=False) # data is cached in memory so limit the knowledge base according to your machine def load_data(): with st.spinner(text=\u0026quot;Loading and indexing the provided data\u0026quot;): reader = SimpleDirectoryReader(input_dir=input_dir, recursive=True) # read recursively all directories docs = reader.load_data() # load data and create docs service_context = ServiceContext.from_defaults(llm=OpenAI(model=api, temperature=0.5, system_prompt=system_prompt)) # add a permanent service prompt which is added index = VectorStoreIndex.from_documents(docs, service_context=service_context) # create your vector database return index Call the load_data function to load and index the data. Also a chat engine is initialized using the indexed data.\n# Load data and create the chat engine index = load_data() chat_engine = index.as_chat_engine(chat_mode=\u0026quot;condense_question\u0026quot;, verbose=True) Check if the user has entered a question through the Streamlit chat input widget. If there is user input, it is appended to the chat history.\n# User input and chat history if prompt := st.chat_input(\u0026quot;Your question\u0026quot;): st.session_state.messages.append({\u0026quot;role\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;content\u0026quot;: prompt}) Loop through the chat history and displays all previous messages in the chat interface.\n# Display chat history for message in st.session_state.messages: with st.chat_message(message[\u0026quot;role\u0026quot;]): st.write(message[\u0026quot;content\u0026quot;]) Checks if the last message in the chat history is not from the assistant (bot). If it\u0026rsquo;s not from the assistant, a response is generated using the chat engine and added to the chat history.\n# Generate a response if the last message is not from the assistant if st.session_state.messages[-1][\u0026quot;role\u0026quot;] != \u0026quot;assistant\u0026quot;: with st.chat_message(\u0026quot;assistant\u0026quot;): with st.spinner(\u0026quot;Thinking...\u0026quot;): response = chat_engine.chat(prompt) st.write(response.response) message = {\u0026quot;role\u0026quot;: \u0026quot;assistant\u0026quot;, \u0026quot;content\u0026quot;: response.response} st.session_state.messages.append(message) Resources Build a chatbot with custom data sources, powered by LlamaIndex\nStreamlit Secrets management\n","permalink":"https://www.patrickschnass.de/posts/hands-on-rag/","summary":"TL;DR This blogpost shows an example for a Chatbot that uses Retrieval Augmented Generation to retrieve domain specific knowledge before querying a Large Language Model\nHands on with Retrieval Augmented Generation For a primer on Retrieval Augmented Generation please read my other post What is Retrieval Augmented Generation?.\nRetrieval Augmented Generation can be a powerful architecture to easily built knowledge retrieval applications which (based on a recent study) even outperform LLM\u0026rsquo;s with long context windows.","title":"Hands on with Retrieval Augmented Generation"},{"content":"TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.\nWhy should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.\npicture from Neo4J\nBut fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:\nAcquiring the massive computational infrastructure required to train LLMs effectively demands significant financial investment. Even if you rely on Cloud Providers instead of on-premise the cost to train is a factor which cannot be neglected currently. Assembling high-quality, domain-specific datasets for fine-tuning can be time-consuming and expensive, as it often involves labor-intensive data collection and annotation efforts. Other downsides are:\nHallucinations: If you think about it consequently fine-tuning will not guarantee that the model behaves the way you would like it to behave. We are able to steer how the model is \u0026ldquo;learning\u0026rdquo; when fine-tuning but we cannot control the outcome. The probability that we receive hallucinations is way higher if we fine-tune in contrast to if we just directly provide additional context in the prompt which is matching to our question. Retraining and Knowledge Cut-offs: It would require constant compute-intensive retraining for even small changes. As you would not do that every day every LLM has a training end date, post which it is unaware of events or information. Context Window: Each Large Language Model (LLM) functions within a contextual window, which essentially defines the maximum volume of information it can simultaneously accommodate. When external data sources provides information surpassing this window\u0026rsquo;s capacity, it needs to be segmented into smaller portions that align with the model\u0026rsquo;s contextual limitations. Use Retrieval Augmented Generation Research on NLP was there before the hype around GPT\u0026rsquo;s and specifically on OpenAIs service ChatGPT(based on GPT3.5) started in 2022. In 2020 Lewis et al. from Meta AI published a paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\nRetrieval-augmented generation (RAG) is an AI architecture for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLMs internal representation of information.\npicture from Neo4J\n“It’s the difference between an open-book and a closed-book exam,” Lastras said. “In a RAG system, you are asking the model to respond to a question by browsing through the content in a book, as opposed to trying to remember facts from memory.”\nIt is a fusion of two powerful NLP techniques: retrieval-based models and generative models. Retrieval-based models excel at finding relevant information from a vast pool of knowledge, often in the form of pre-existing text or documents. On the other hand, generative models are proficient in generating human-like text based on the given input.\nIncorporating it into an LLM architecture enables the model to combine the best of both worlds.\nIt can retrieve contextually relevant information from a vast knowledge base and use that information to generate coherent and contextually appropriate responses. This approach leads to more accurate and context-aware interactions with the language model.\npicture from Lewis et al. (2021)\nThe architecture of an LLM incorporating retrieval augmented generation typically consists of two main components:\nRetrieval Module The retrieval module is the heart of the architecture and responsible for searching and retrieving relevant information from a large knowledge base. This knowledge base can be a collection of texts, documents, or even web pages. Retrieval can be broken down into two stages:\nIndexing You will index the knowledge base which you want to give as context to the model. To index your data you can simply use libraries like LangChain llama_index or transformers which do the work for you. The data is loaded, chunked and tokenized. Data will be fetched from your various sources, segmented into bite-sized chunks to optimize them for embedding and search and afterwards tokenized to create embeddings. The embeddings are stored as high dimensional vectors in vector databases and build the foundation for RAG.\nLet\u0026rsquo;s look into the implementation of RAG in llama_index, a famous orchestration network to further understand the indexing stage:\npicture and text from llama_index\nData Connectors: A data connector (i.e. Reader) ingest data from different data sources and data formats into a simple Document representation (text and simple metadata).\nDocuments / Nodes: A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. It’s a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\nData Indexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a format that’s easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the VectorStoreIndex\nQuerying The data from the vector databases can be used for Semantic Search. Meaning that when a query is processed(into an embedding) the retriever can search for the most appropriate matching data in the vector database and give it to the generator as context. The retrieved data is combined with the original prompt, creating an enhanced or augmented prompt. This augmented prompt provides additional context. This is even more relevant for domain specific data which may not be part of the corpus used for training the model.\nAgain let\u0026rsquo;s look how this is implemented llama_index, to also understand the querying stage:\npicture and text from llama_index\nRetrievers: A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. The specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.\nNode Postprocessors: A node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them.\nResponse Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\nIn summary: We start by searching for relevant documents or excerpts within an extensive dataset. For this a dense retrieval mechanism that leverages embeddings to represent both the query and the documents is used. These embeddings are then utilized to calculate similarity scores, leading to the retrieval of the top-ranked documents.\nGeneration Module The generation module is based on a generative language model like GPT-3.5. It takes as input the retrieved information and the user\u0026rsquo;s query or context and generates a coherent response.\nExample An example architecture using llama_index would look like the following:\npicture from streamlit\nThe code for this example can be found on github. Also you can find a dedicated blogpost on my blog Hands on with Retrieval Augmented Generation.\nBenefits of using RAG Improved Precision: Through the utilization of external data sources, the RAG LLM can produce responses that are not solely derived from its training dataset but are also enriched by the most pertinent and current information accessible within the retrieval corpus.\nAddressing Knowledge Gaps: RAG effectively confronts the inherent knowledge limitations of LLM, whether stemming from the model\u0026rsquo;s training cut-off or the absence of domain-specific data in its training corpus.\nAdaptability: RAG can seamlessly integrate with an array of external data sources, encompassing proprietary databases within organizations and publicly accessible internet data. This adaptability makes it suitable for a broad spectrum of applications and industries.\nMitigating Hallucinations: A challenge associated with LLMs is the potential occurrence of \u0026ldquo;hallucinations,\u0026rdquo;. By incorporating real-time data context, RAG decreases the probability of such outputs.\nScalability: An advantage of RAG LLMs lies in its scalability. Through the separation of the retrieval and generation processes, the model can manage extensive datasets, making it well-suited for real-world scenarios characterized by abundant data.\nPitfalls Of course there is a downside with retrieval systems. They rely on semantic search. Semantic Search has a simple assumption which leads to problems:\nQuestion and answer have a high semantic similarity.\nThis assumption seems easy and straightforward but it is oversimplifying human language. Semantic Search only looks for explicit matches not for negation or implicit matches.\nA simple example Think of following example:\ninitial sentence:\n\u0026ldquo;I like Bananas\u0026rdquo;\nvs\n\u0026ldquo;I don\u0026rsquo;t like Bananas\u0026rdquo;\n\u0026ldquo;I like every fruit but bananas\u0026rdquo;\n\u0026ldquo;I love this divine yellow fruit. It is curved like a smile and I feel like a monkey when I eat it.\u0026rdquo;\nI think it is clear where i am pointing at. The sentences are already ordered from highest to lowest similarity.\nOf course there is potential to overcome this with additional heuristics but for sure we need to understand that every architecture we built is implicitly build on assumptions that influence the outcome.\nConclusion Retrieval augmented generation is a versatile hack.\nIn the early days of LLM adoption - where we are still at - it is a good technique to rapidly deliver production grade systems which are more efficient and less prone for hallucinations.\nDue to the implicit assumptions and with it bias humans are bringing into the architecture in the future there will be better ways to deliver a good outcome. But for now it is highly relevant to consider Retrieval Augmented Generation for building LLM powered solutions.\nResources Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\nIBM: What is retrieval-augmented generation?\nRetrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models\nRetrieval meets Long Context Large Language Models\nA Deep Dive into Retrieval-Augmented Generation in LLM\nPitfalls of Semantic Search\n","permalink":"https://www.patrickschnass.de/posts/rag_intro/","summary":"TL;DR This blogpost tries to explain Retrieval Augmented Generation. Retrieval Augmented Generation is an Architecture used for NLP tasks which can be used to productionize LLM models for enterprise architecture easily.\nWhy should i care? Intuitive would be to train a Large Language Model with domain specific data, in other words, to fine-tune the model-weights with custom data.\npicture from Neo4J\nBut fine-tuning large language models (LLMs) is a complex and resource-intensive process due to several key factors:","title":"What is Retrieval Augmented Generation?"},{"content":"TL;DR This blogpost focusses on ML Design and Architecture and tries to give some intuition and hints for deciding between one generalized and multiple specialized models for the same business requriement and dataset.\nConsider it a nudge to dive deeper into the topic\nWhy should i care? Transparency in machine learning is crucial for business stakeholders because it fosters trust and informed decision-making. Business Stakeholders need to understand not only the potential benefits but also the limitations and risks associated with machine learning models.\nAs Data scientists we play an important role in this process, as we bridge the gap between complex algorithms and business objectives. Educating stakeholders about the inner workings of machine learning algorithms, their inputs, outputs, and potential biases, empowers them to make well-informed decisions, manage expectations, and mitigate risks effectively.\nBut this customer segment has way higher churn, shouldn\u0026rsquo;t the model focus on this segment only?\nThis is one of the examples when you as a ML Engineer need to shed some light on your design decisions and explain why you decide on a specific algorithm for the source data.\nUseCase We are focussing on a \u0026ldquo;Bread and Butter\u0026rdquo; Binary Classification Model for Churn in the Telecommunications sector and try to make a comparison between the two approaches.\n\u0026ldquo;Churn\u0026rdquo; refers to the phenomenon where customers switch from one telecommunications service provider to another or discontinue their subscription altogether. It is a significant metric and concern for telecommunications companies because retaining existing customers is often more cost-effective than acquiring new ones.\nA binary classification model is a type of machine learning model used to classify data into one of two categories: a positive class (labeled as 1) or a negative class (labeled as 0). It learns from labeled data during training and uses features to make predictions. The model is evaluated based on metrics like accuracy, precision, recall, and F1-score and can be applied to various tasks, such as spam detection, disease diagnosis, and sentiment analysis. It\u0026rsquo;s a fundamental tool for making decisions with two possible outcomes.\nGeneral Models vs Specialized Models Suppose we have a dataset which consist of a matrix of predictors (called X) and a target variable (called y). X contains n columns(features) that could be used to segment the dataset. E.g. a dataset can be segmented by age, product, sales channel, network experience\u0026hellip;\nSo what are the two approaches we can choose between?\nGeneral Model\nOne main model is fitted on the whole training set including all segments, then the performance is measured on the test set.\nSpecialized Model\nThe dataset is splitted into each segment and for each of these subsets a unique model is fitted. This means that we repeat training and testing for the number of k segments.\nIntuition Using specialized models has obviously some practical disadvantages as you need to do some tasks k times which leads to\nhigher complexity higher effort on maintenance almost redundant processes So why should anyone favour specialized models at any point in time?\nThe prejudice against general models is as following: Advocates for specialized models argue that a single, all-encompassing model might lack precision within a specific subset, as it would have learned the characteristics of various other subsets.\nThis intuition was in my opinion built on top of the assumption that all Machine Learning Models work similary to simple models like e.g. linear regression.\nThe type of model counts Linear regression assumes that there is a linear relationship between the independent variable(s) X and the dependent variable y. This means that the relationship can be approximated by a straight line. While it assumes linearity, it can be extended to capture more complex relationships by introducing higher-order terms or using more advanced regression techniques like polynomial regression or multiple linear regression when dealing with multiple independent variables. Still it lacks accuracy if the dataset contains different behaviours.\nThe intuition does not neccessarily hold true for the de facto standard algorithm for tabular data: Boosted Tree Models like XGBoost,LightGBM or CatBoost.\nBoosted tree algorithms, outperform linear models like linear regression in modeling complexity. The main advantages above linear models which are relevant to defuse the intuition against general models is their ability to:\nHandle Non-Linearity: Boosted trees capture non-linear relationships, unlike linear regression, which assumes linearity.\nEnsemble Learning: They use ensemble learning to combine multiple decision trees, enabling them to create complex models.\nFeature Interactions: Boosted trees automatically detect and model feature interactions, which linear regression struggles with.\nModel Flexibility: Boosted trees adapt to data complexity by adding more trees to the ensemble.\nThis is the main reason why there is no theoretical reason to prefer several specialized models over one general model. Other adavantages are:\nRobustness to Outliers: They are more robust to outliers and noisy data, making them suitable for real-world scenarios.\nHandling Heterogeneous Data: They handle various data types, including categorical features, without extensive preprocessing.\nFewer Assumptions: Unlike linear regression, they have fewer rigid assumptions, making them versatile in diverse datasets.\nAnd what about the data When looking at the data and keeping in mind the capabilities of non-linear models we should always prefer general model over specialized models when there is some similarity across the segments composing the dataset.\nAs the segments diverge further from one another, the benefits of employing a universal model diminish progressively. If the segments are entirely dissimilar, the disparity between the two approaches should converge to zero.\nBut what would that mean when looking at our Real-Life UseCase? Will customer segments be completely different. Shall we assume that the behaviour of people based on age, product, sales channel, network experience is completely different?\nAs you already can see this is a rhetoric question because you cannot assume completely different behaviour for a more or less homogenous group.\nI want proof The former thoughts are based on two very nice blogposts by Samuele Mazzanti which are also referenced below.\nHe did the math and calculated based on two experiments whether a general model outperforms specialized models.\nOn average the general model outperforms the specialized models. He did a short stress test with completely diverged groups which should be a really rare occurence in any Real-Life Business Case. Even then the general model performed only 0.53% AUC worse than the specialized model.\nConclusion Whether a generalized model or specialized models are better for the same dataset depends on various factors, including the nature of the data, the used algorithm, the specific problem you\u0026rsquo;re trying to solve, and your objectives.\nGeneralized Model:\nAdvantages:\nSimplicity: Generalized models are often simpler to implement and maintain. Resource Efficiency: Training and deploying a single model can be more resource-efficient than multiple specialized models. Applicability: Generalized models can be useful when the differences between subgroups in the data are relatively small, and a single model can provide satisfactory performance across all groups. Use Cases:\nWhen the dataset is relatively homogenous, and there are no strong reasons to believe that different subgroups require significantly different modeling approaches. In scenarios where model interpretability and ease of deployment are critical. Specialized Models:\nAdvantages:\nImproved Performance: Specialized models can potentially provide better predictive performance for specific subgroups or behaviors within the data if the data subgroups are Customization: They allow you to tailor the model to the unique characteristics of different segments, which can lead to more accurate predictions. Flexibility: Specialized models can handle cases where the relationships between features and the target variable vary significantly between subgroups. Use Cases:\nWhen there are clear distinctions or significant variations in behavior or patterns among different subgroups within the dataset. In cases where the overall dataset is large, but specific subgroups have limited data, making it challenging for a generalized model to capture their nuances effectively. When optimizing performance for specific subgroups is critical, even if it requires more complex model development and maintenance. In practice, it\u0026rsquo;s all about the data: If the data doesn\u0026rsquo;t exhibit significant subgroup differences in my view it is alway beneficial to use a generalized model. A specialized model may only be benefical due to a very heterogenous dataset or specific requirements.\nAs written above also the model plays a crucial part and the intuition is often a bit misleading. The results from the quoted experiments show clearly that using boosted tree models in one general model will be favourable.\nThe trade-offs between model complexity and performance are always to be taken into account.\nRessources Samuele Mazzanti: What Is Better: One General Model or Many Specialized Models?\nSamuele Mazzanti: The Unreasonable Effectiveness of General Models\n","permalink":"https://www.patrickschnass.de/posts/generalvsspecializedmodels/","summary":"TL;DR This blogpost focusses on ML Design and Architecture and tries to give some intuition and hints for deciding between one generalized and multiple specialized models for the same business requriement and dataset.\nConsider it a nudge to dive deeper into the topic\nWhy should i care? Transparency in machine learning is crucial for business stakeholders because it fosters trust and informed decision-making. Business Stakeholders need to understand not only the potential benefits but also the limitations and risks associated with machine learning models.","title":"Generalized Models vs Specialized Models"},{"content":"Transformers: A Deep Dive into the Transformer Architecture Below is a revised, more detailed version of the article that removes the BERT code snippet, expands on both the mathematical background and implementation details, and includes additional illustrative images.\nTransformers: A Deep Dive into the Transformer Architecture Transformers are a revolutionary type of neural network architecture introduced in the seminal paper Attention is All You Need by Vaswani et al. (2017). They have dramatically advanced the field of Natural Language Processing (NLP) and beyond, powering tasks like language translation, text summarization, and even applications in computer vision.\nAn illustrative diagram of the Transformer model architecture.\nIntroduction Before the advent of Transformers, sequence-to-sequence tasks were predominantly handled by Recurrent Neural Networks (RNNs) and their variants (LSTMs and GRUs). While effective in many scenarios, RNNs process input tokens sequentially and struggle with long-range dependencies. Transformers overcome these limitations through the use of attention mechanisms that allow for parallel processing and a more flexible handling of context.\nKey Components of the Transformer 1. Self-Attention Mechanism At the core of the Transformer architecture lies the self-attention mechanism. This mechanism allows each token in the input to dynamically focus on other tokens, thereby capturing contextual relationships regardless of their distance in the sequence.\nThe self-attention computation is defined as:\n[ \\text{Attention}(Q, K, V) = \\text{softmax}!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V ]\n( Q ) (Query), ( K ) (Key), and ( V ) (Value): These matrices are derived from the input embeddings. ( d_k ): The dimension of the key vectors. Scaling Factor: The division by (\\sqrt{d_k}) prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. This formulation enables the model to compute a weighted sum of the values, where the weights are determined by the relevance of each token in the context of others.\n2. Multi-Head Attention Rather than applying a single attention operation, Transformers use multi-head attention. This technique involves splitting the embeddings into several subspaces, applying self-attention in parallel, and then concatenating the results. Each \u0026ldquo;head\u0026rdquo; can capture different aspects of the relationships between tokens.\nMulti-head attention enables the model to focus on various types of relationships concurrently.\n3. Positional Encoding Because Transformers process input sequences in parallel, they lack an inherent sense of token order. Positional encodings are added to the input embeddings to inject sequence information. A common method is to use sine and cosine functions at different frequencies:\n[ PE_{(pos, 2i)} = \\sin!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) ] [ PE_{(pos, 2i+1)} = \\cos!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) ]\nHere, ( pos ) indicates the token\u0026rsquo;s position and ( i ) is the dimension index. These encodings provide the necessary information about the order of tokens.\n4. Encoder-Decoder Structure The original Transformer architecture is divided into two main parts:\nEncoder: Processes the input sequence using layers that include multi-head self-attention and feed-forward networks, each augmented by residual connections and layer normalization.\nThe encoder stack processes the entire input simultaneously, capturing contextual relationships effectively.\nDecoder: Generates the output sequence. It employs masked self-attention (to prevent future tokens from influencing the current prediction) along with encoder-decoder attention, ensuring that the generated output aligns with the input context.\nMathematical Background and Implementation Details Mathematical Insights The mathematical formulations underpinning Transformers are central to their effectiveness:\nScaled Dot-Product: The division by (\\sqrt{d_k}) is essential. Without it, the dot product values could become excessively high for large ( d_k ), pushing the softmax into regions with very small gradients and thus hampering training. Linear Projections: The learned projections for ( Q ), ( K ), and ( V ) allow the model to extract diverse aspects of the input features. When these projections are split across multiple heads, the model learns to capture different patterns and dependencies simultaneously. Practical Implementation Building a Transformer involves several key steps:\nData Preparation:\nTokenization: Convert raw text into tokens. Embedding: Map tokens into continuous vector space. Positional Encoding: Add positional information to the embeddings. Constructing the Model Architecture:\nEncoder Layers: Stack layers that include multi-head self-attention and feed-forward networks, each wrapped with residual connections and layer normalization. Decoder Layers: Stack layers that perform masked self-attention, followed by encoder-decoder attention, and then feed-forward networks. Training the Model:\nLoss Function: Typically, a cross-entropy loss is used for tasks like language translation. Optimization: Modern frameworks like PyTorch and TensorFlow enable efficient training on GPUs/TPUs, capitalizing on the parallelizable nature of Transformers. Fine-Tuning:\nPre-trained Transformer models are often fine-tuned on specific downstream tasks, a process that has led to state-of-the-art performance in many NLP benchmarks. Conclusion Transformers have fundamentally transformed the landscape of machine learning by introducing a mechanism that can effectively capture long-range dependencies while enabling parallel computation. Their versatility and performance have not only pushed the boundaries in NLP but are also inspiring innovations in other fields like computer vision and reinforcement learning.\nAs research continues, the Transformer architecture is likely to be adapted for even broader applications.\nFor those interested in a deeper dive, the original paper Attention is All You Need provides comprehensive insights into the mathematical foundations and design choices that make Transformers so effective.\n","permalink":"https://www.patrickschnass.de/posts/transformers/","summary":"Transformers: A Deep Dive into the Transformer Architecture Below is a revised, more detailed version of the article that removes the BERT code snippet, expands on both the mathematical background and implementation details, and includes additional illustrative images.\nTransformers: A Deep Dive into the Transformer Architecture Transformers are a revolutionary type of neural network architecture introduced in the seminal paper Attention is All You Need by Vaswani et al. (2017). They have dramatically advanced the field of Natural Language Processing (NLP) and beyond, powering tasks like language translation, text summarization, and even applications in computer vision.","title":"What are Transformers?"},{"content":"I enlisted for the free Get Certified Program from Google in June 2023 which offers a 10 week curated curriculum to prepare for the Data Engineer exam. Unfortunately the content is restricted and cannot be shared. Following article should summarize all additional sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places. I just try to condense it to the minimum here.\nRessources Official Google Exam Curriculum\nGoogle Cloudskillboost\nMost helpful Google ressources How to prepare for the certification\nGoogle Example Questions\nGoogle Cloud Architecture Center\nOther helpful ressources Helpful Published Title/Link Author 10 - Google Cloud Cheat Sheet - 7 2022/12 Awesome GCP Certifications Repo Satish Vijai 9 2023/01 Mastering the Google Cloud Professional Data Engineer Exam in 2023: Strategies and Resources Syam Kakarla Key takeaways The questions on Cloudskillboost not neccessarily help with passing the exam You need a basic understanding of how Data Engineering works and which Google solutions supports which part in the process Think efficient - what is the easiest solution Read carefully the question and look for key-words: Cost, time, serverless, etc. Read carefully the answers and think of consequences of the solutions and try to identify the best solution Look into other sources on Medium of people who recently took the exam and try to identify new topics Search for Exam Dumps on deidcated sources like Examtopics. They may be helpful in preparation. ","permalink":"https://www.patrickschnass.de/posts/PDE/","summary":"I enlisted for the free Get Certified Program from Google in June 2023 which offers a 10 week curated curriculum to prepare for the Data Engineer exam. Unfortunately the content is restricted and cannot be shared. Following article should summarize all additional sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.","title":"Road to GCP Professional Data Engineer"},{"content":"TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?\nLarge language models rely basically on neural networks – more specifically, deep neural networks – to process natural language and can be trained on large amounts of text data, such as books, articles, and social media posts. They parse through the data and generate predictions about what words will come next in a sentence or phrase.\nThe larger the dataset, the more information that can be fed into the network for analysis. This increases the performance since the model has more data to evaluate against.\nOne of the key techniques used in large language models is called \u0026ldquo;pretraining.\u0026rdquo; This involves training the model on a large corpus of text data using a technique called unsupervised learning, where the model tries to learn the underlying structure of the language without any explicit supervision. This pretraining step helps the model to learn a general understanding of the language, which can then be fine-tuned for specific tasks like language translation or text generation.\nAnother important concept in large language models is \u0026ldquo;attention,\u0026rdquo; which refers to the ability of the model to focus on specific parts of the input sequence when making predictions. Attention mechanisms allow the model to selectively weight the importance of different parts of the input sequence based on their relevance to the task at hand.\nArchitectures There are several architectures used for Large Language Models (LLMs), but some of the most popular ones are:\nRecurrent Neural Networks (RNNs) RNNs are a type of neural network that is commonly used for processing sequences, including natural language. They work by processing each input element (in this case, each word in a sentence) in a sequence, and updating their internal state based on the previous inputs. This makes them well-suited for language modeling tasks, where the model needs to keep track of the context of the sentence to make predictions. Convolutional Neural Networks (CNNs): CNNs are another type of neural network that have been used for LLMs. They are typically used for image recognition tasks, but they can also be used for language processing tasks by treating the words in a sentence as a 1-dimensional signal. In this case, the convolution operation can help the model to identify important features of the sentence, such as n-grams or phrases. Transformer Models: Transformer models are a more recent architecture that has been widely used for LLMs. They are based on a self-attention mechanism that allows the model to selectively attend to different parts of the input sequence. This makes them highly effective for modeling long-range dependencies in text data, and they have been used to achieve state-of-the-art results in a wide range of language processing tasks. GPT (Generative Pre-trained Transformer) Models: GPT models are a type of transformer model that are pre-trained on large amounts of text data using unsupervised learning. They have been used for a wide range of natural language processing tasks, including language translation, text summarization, and text generation. The GPT series has evolved over time with each version improving on the previous in terms of size and performance. BERT (Bidirectional Encoder Representations from Transformers) Models: BERT models are also based on transformer architecture but are trained using a different pre-training objective. They are trained to understand the context of words based on their surrounding words, both before and after, unlike traditional LLMs which are only trained to consider context before the word. This makes BERT models particularly useful for tasks such as question answering and sentiment analysis. Each of these architectures has its own strengths and weaknesses, and the choice of architecture depends on the specific task and the nature of the data being processed. However, in recent years, transformer-based models such as GPT and BERT have emerged as the most effective architectures for large language modeling tasks.\nWhy the Hype The statistical methods behind LLM\u0026rsquo;s are rather simple. The recent innovation comes from investing a huge amount of money and a clever strategy to finetune. Reinforcement Learning on Human Feedback.\nReinforcement Learning with Human Feedback (RLHF) is a subfield of reinforcement learning (RL) that aims to incorporate human feedback into the training of RL agents. In RLHF, humans provide feedback to the RL agent during the training process to help guide its behavior.\nThe basic idea behind RLHF is to leverage the unique strengths of both humans and machines. While machines are capable of processing large amounts of data quickly and making decisions based on objective criteria, humans are better at handling uncertainty, understanding complex contexts, and incorporating ethical considerations into decision-making.\nThere are several approaches to incorporating human feedback into RL training. One common approach is called \u0026ldquo;reward shaping,\u0026rdquo; where humans provide additional reward signals to the agent to help guide its behavior. For example, if an agent is learning to play a game, a human might provide additional rewards for certain actions that the agent is not considering on its own.\nAnother approach is called \u0026ldquo;active learning,\u0026rdquo; where the agent actively seeks out feedback from humans to improve its performance. For example, the agent might ask a human to provide feedback on a specific decision it is considering, or it might ask for help in identifying certain features of the environment that are important for making decisions.\nOverall, RLHF has the potential to make RL agents more effective in real-world applications, where the agent must interact with humans or operate in complex, uncertain environments. However, it also raises important ethical considerations, such as how to ensure that the human feedback is fair and unbiased, and how to handle situations where the human feedback is conflicting or inconsistent.\nHands On As said it is rather simple to set up your own Language Model and if you are eager to do it you can use the nanoGPT implementation of Andrej Karpathy on GitHub](https://github.com/karpathy/nanoGPT) to test out your own model.\nRessources Large Language Models\nChatGPT\nnanoGPT\n","permalink":"https://www.patrickschnass.de/posts/language_models/","summary":"TL;DR A short summary on (Large) Language Models: What are the ideas and concepts behind Language Models?\nWhat are Language Models Large Language Models are rapidly transforming natural language processing tasks and have led to a huge hype around Artificial Intelligence and associated tasks. Especially with the introduction of GPT3.5(ChatGPT) in 2022 many people are interested in the capabilities of ML. But what is the foundation for the products that could impact our future world significantly?","title":"What are (Large) Language Models?"},{"content":"TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.\nTrain your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.\nThinking about the current possibilities and being lazy i thought about how to write software that could do the job for me and that this would be even more satisfying than playing it myself. I have knonw about the magnitude of recent emulators for all kinds of plattforms and wondered if some of those already contain an API for automating any training.\nAfter some search there it is: A GameBoy Emulator written in Python which can be fully controlled from a Python Script: PyBoy (I can\u0026rsquo;t highlight how big my nerdy enthusiasm for the fact of an emulator in Python is)\nSo the place is set up and we have everything we need:\nA GameBoy Emulator which allows us to interact via python A Rom of the game you already own (Legally speaking Security Backups are allowed) Time and Motivation to start The capabilities of PyBoy Installation Assuming that you already have a working Python environment the instalation is super easy. Just pip install pyboy and you are ready to go.\nStart Now if you are just interested in playing a game it is also super easy as you can just start pyboy from your terminal: $ pyboy path/to/your/file.rom\nPyBoy is loadable as an object in Python. This means, it can be initialized from another script, and be controlled and probed by the scrip:\nfrom pyboy import PyBoy pyboy = PyBoy('ROMs/gamerom.gb') while not pyboy.tick(): pass pyboy.stop() API When the emulator is running you can access PyBoy\u0026rsquo;s API.\nfrom pyboy import WindowEvent pyboy.send_input(WindowEvent.PRESS_ARROW_DOWN) pyboy.tick() # Process one frame to let the game register the input pyboy.send_input(WindowEvent.RELEASE_ARROW_DOWN) pil_image = pyboy.screen_image() pil_image.save('screenshot.png') Now you have everything you need and find all relevant commands in the PyBoy Documentation.\nThe Setup for your Reinforcement Learning Algorithm Concepts of RL Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The goal of RL is to find an optimal policy that maximizes a long-term reward signal. The agent takes actions in the environment and receives feedback in the form of rewards or penalties. The agent then updates its policy based on the feedback received, in order to maximize the total reward it receives over time.\nOur setup is following the typical concepts of RL\nEnvironment The world that an agent interacts with and learns from.\nAction $a$ : How the Agent responds to the Environment. The set of all possible Actions is called action-space.\nState $s$ : The current characteristic of the Environment. The set of all possible States the Environment can be in is called state-space.\nReward $r$ : Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called Return.\nOptimal Action-Value function $Q^*(s,a)$ : Gives the expected return if you start in state $s$, take an arbitrary action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the “quality” of the action in a state. We try to approximate this function.\nSpecification of Q Learning Q-Learning is a popular reinforcement learning algorithm that learns the optimal action-value function, also known as the Q-function, for a given environment. The Q-function represents the expected long-term reward for taking a particular action in a given state. The Q-learning algorithm updates the Q-function based on the rewards received and the estimated Q-values for the next state. The agent then uses the updated Q-function to select the next action.\nThe Q-learning algorithm is based on the Bellman equation, which is a recursive equation that expresses the optimal Q-value in terms of the expected reward for the current action and the expected Q-value for the next state. The Q-learning algorithm uses a greedy approach to select actions, meaning that it always chooses the action with the highest estimated Q-value.\nQ-learning stores the results between iterations in a Q-table, which is essentially a lookup table that contains the expected reward values for every state-action pair in the environment. The Q-table is initialized with arbitrary values and is updated over time as the agent interacts with the environment.\nAt each iteration, the agent observes the current state of the environment, selects an action based on the Q-values in the Q-table, performs the action, and observes the reward and the next state. The agent then updates the Q-value for the state-action pair based on the Bellman equation, which expresses the optimal Q-value for a state-action pair as the sum of the immediate reward and the discounted expected future reward.\nThe Q-value update equation is as follows:\nQ(s,a) = Q(s,a) + alpha * (r + gamma * max(Q(s\u0026rsquo;,a\u0026rsquo;)) - Q(s,a))\nwhere:\nQ(s,a) is the current Q-value for the state-action pair (s,a) alpha is the learning rate, which determines how much the Q-value is updated based on the new information r is the immediate reward received for taking the action a in state s gamma is the discount factor, which determines how much weight is given to future rewards max(Q(s\u0026rsquo;,a\u0026rsquo;)) is the maximum Q-value for the next state s\u0026rsquo; and all possible actions a\u0026rsquo; that can be taken from s' After the Q-value update, the agent moves to the next state and repeats the process until it reaches the terminal state.\nAs the agent continues to interact with the environment, the Q-table is gradually updated with more accurate estimates of the optimal Q-values. The agent uses the Q-table to select the optimal action in each state, based on the highest Q-value in the table for that state. By learning from experience and updating the Q-values over time, Q-learning allows the agent to make better decisions and maximize the cumulative reward it receives from the environment.\nIt\u0026rsquo;s a me Mario PyBoy is giving an inspiration on how to set up the Agent for Super Mario.\nEnvironment The environment is in this case the world itself: The Floor, Pipes, Blocks the Background and of course the enemies. To capture the evironment means to capture the complete observation space in every single frame.\nAction The actions chosen could be all actions from the actual Window Event: List[WindowEvent] but for the game it would make absolutely no sense to test the Start Buttone or Select Button. Therefore we focus on LEFT,RIGHT and A.\nbaseActions = [WindowEvent.PRESS_ARROW_RIGHT, WindowEvent.PRESS_BUTTON_A, WindowEvent.PRESS_ARROW_LEFT] State Here it is defined by GameState.\nReward The reward is defined by the progress in the game: deaths, time, level and movement.\nclock = current_mario.time_left - prevGameState.time_left movement = current_mario.real_x_pos - prevGameState.real_x_pos death = -15*(current_mario.lives_left - prevGameState.lives_left) levelReward = 15*max((current_mario.world[0] - prevGameState.world[0]), (current_mario.world[1] - prevGameState.world[1])) # +15 if either new level or new world reward = clock + death + movement + levelReward Let the games begin I built my own simplified agent on following example and had a ton of fun here\nRessources PyBoy Repo\nPyBoy Documentation\nPlay SNES Super Mario with RL\nExample RL for PyBoy\n","permalink":"https://www.patrickschnass.de/posts/super_mario/","summary":"TL;DR Learn how to train a Reinforcement Learning Agent to play GameBoy games in a Python written Emulator. With PyBoy, Q-Learning and Super Mario.\nTrain your own RL Agent to play Super Mario Recently i stumbled upon my old GameBoy an immediately started it and tried to start where i left off 20 years ago. Unfortunately my hand eye coordination is not what it used to be so i died a few fast deaths.","title":"How to use a Reinforment Learning Agent to play Super Mario"},{"content":"Today i passed the Google Professional Machine Learning Engineer Exam in a onsite test center in Duesseldorf. I prepared for it 4 weeks with different ressources. I have multiple years experience on Data Science and ML and about one year experience with ML on GCP. Following article should summarize all sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places. I just try to condense it to the minimum here.\nRessources Official Google Documentation\nGoogle Exam Curriculum\nGoogle Cloudskillboost\nMost helpful Google ressources Google ML Crash Course\nGoogle Example Questions\nBest practices for implementing machine learning on Google Cloud\nGoogle Architecture for MLOps using TensorFlow Extended, Vertex AI Pipelines, and Cloud Build\nGoogle Rules of ML\nGoogle Cloud Architecture Center\nGoogle Guidelines for ML Solutions\nOther helpful ressources Helpful Published Title/Link Author 10 - Google Cloud Cheat Sheet - 10 2022/11 Journey to PMLE(O\u0026rsquo;Reilly Paywall) Dr. Logan Song 7 2022/12 Awesome GCP Certifications Repo Satish Vijai 9 2022/12 Passing the Google cloud professional machine learning engineer exam Hil Liao 5 2022/12 How to clear GCP PMLE Exam? Shadab Hussain 3 2022/03 How I passed the Google Cloud PMLEexam (Vertex AI) Joshua Tan 2 2022/01 How to prepare for the GCP PMLEexam Gabriel Cassimiro 10 2022/01 A cromprehensive Study Guide Jeffrey Luppes 6 2021/03 Study guide to ace Google Cloud Certification on PMLE Rahul Pandey 2 2021/01 Learning Notes Sehgal Namit Key takeaways A significant amount of knowledge covered in the exam also came from Google’s machine learning crash course. The questions on Cloudskillboost not neccessarily help with passing the exam You need a basic understanding of how MLOps works and which Google solutions supports which part in the process Think efficient - what is the easiest solution Think like a ML practicioner Read carefully the question and look for key-words: Cost, time, serverless, etc. Read carefully the answers and think of consequences of the solutions and try to identify the best solution Look into other sources on Medium of people who recently took the exam and try to identify new topics Search for Exam Dumps on deidcated sources like Examtopics. They may be helpful in preparation. Key topics Distributed Learning Imbalanced Data Efficiency (Scalability, Ease of Use, Reproducability) Speed (BQML vs Vertex Pipelines) Dataflow Triggers Metrics (Business vs ML) Where to do what (Inference on Device vs Pipeline) Privacy. How to set up a pipeline for the DLP(Quarantine Bucket) Tensorflow and Keras ","permalink":"https://www.patrickschnass.de/posts/GCPMLE/","summary":"Today i passed the Google Professional Machine Learning Engineer Exam in a onsite test center in Duesseldorf. I prepared for it 4 weeks with different ressources. I have multiple years experience on Data Science and ML and about one year experience with ML on GCP. Following article should summarize all sources which were helpful for me in preparation of the certification exam. If you are interested in how the exam is actually happening there are plenty of other articles on medium or at other places.","title":"Road to GCP Professional Machine Learning Engineer"},{"content":"Storytelling and presentation are the keys to a succesful adoption of your ideas. While storytelling is often verbal, a presentation is what stays and often is the thing that decideds whether your stakeholders pay attention or not so much. So it has to be aesthetic, clear and simple.\nI show a lot of ideas and often i tend to recreate stuff from my code documentation enriched with diagrams. It is often a tedious work, recreating things in Powerpoint. It bores me and somehow it never looks like i want it to look.\nSince i started with this blog and looked into different ways to represent knowledge to others i started using MARP and \u0026hellip; i love it!\nIt simply makes me more productive, efficient and faster.\nHello MARP\nWhat is MARP Marp (also known as the Markdown Presentation Ecosystem) is a fully open-source tool that let\u0026rsquo;s you transform any Markdown file to a PDF, HTML or PPTX slide deck.\nFor whom is MARP the right choice? It\u0026rsquo;s a rather simple solution capable of basic transformations, figures and tables so it is nothing to create your new super fancy slides for the executive committee. Still it is highly valuable in preparing presentations for your colleagues showing code, math and pictures out of our existing documenation.\nIt definetly is not for everyone but i like it.\nHow can i try it out? It is super easy. It comes as an extension for Virtual Studio Code and can simply be installed and is ready to go from any .md file.\nHow can i create slides? 1. Create a .md file Create in any folder an .md file.\n2. Add frontmatter You need to add some boilerplate frontmatter. frontmatter is like the configuration of a markdown to add more complex data to the content than just text. It has to be on the top of the file and begins and ends with three dashes ---.\nIt is optional Each content partial has the following structure layout: partial. In our case we need to configure the frontmatter to\n--- marp: true --- 2. Create Slides Slides can be created with the use of dashes. Each slide ends with three dashes ---.\n--- marp: true --- # This my first slide Some content --- # This is my second slide Some other content 2. Export to slide deck Now you can use the extension and export to a slide of your choosing.\nAnd now you have your first slide deck.\nIt is fine but how can i make it more fancy? You have your first slide deck and now want to further tweak the syle of your presentation?\nConfiguration is given via comments to each slide.\nOverall layout:\n--- layout: partial --- Layout for the actual slide and all following:\n\u0026lt;!-- _layout: partial --\u0026gt; Themes There are different Built-in Themes in MARP which you can use to bring some variety to your slide decks.\nThey are also fully customizable and you can configure styling for the whole slide deck as well as for only one specific slide.\nCode It is just as in normal markdown. Code blocks are seperated by ```\n``` this is my code ``` Math Again like in normal markdown you need to define math: true in frontmatter and can put in mathematical formulas with $\n$x^2$ becomes\n$x^2$\nMath Typesetting\nPictures Again simple markdown syntax\n![Caption](link/to/image.jpg) Also easy to resize images\n![width:200px](link/to/image.jpg) Or set the picture into the background\n![bg fit](link/to/image.jpg) Or plenty of other tweaks: Image Syntax\nCSS Layouts are fully customizable with CSS stylesheets\n\u0026lt;style\u0026gt; \u0026lt;/style\u0026gt; CSS Stylesheets\nPresenter Notes Include Notes which are shown in presentation mode with\n\u0026lt;!-- This is a note for my presentation--\u0026gt; Automated slide decks Writing markdowns with code is easy and with MARP you can even automatically create slide decks containing results or reports. So you never need to copy paste or send bad looking texts or Excels to anyone.\nFor this you can use MARP CLI\nAll conversions require a browser installation but there is also docker-container available.\nWith a local installtion it would look like the following to convert to pdf.\nmarp --pdf slide-deck.md marp slide-deck.md -o converted.pdf Simple as that.\nTry it out!\nReferences Marp\nMarp-Team Repo\nMarpit Docs\nTables in MARP\n","permalink":"https://www.patrickschnass.de/posts/marp/","summary":"Storytelling and presentation are the keys to a succesful adoption of your ideas. While storytelling is often verbal, a presentation is what stays and often is the thing that decideds whether your stakeholders pay attention or not so much. So it has to be aesthetic, clear and simple.\nI show a lot of ideas and often i tend to recreate stuff from my code documentation enriched with diagrams. It is often a tedious work, recreating things in Powerpoint.","title":"Creating slides with MARP"},{"content":"TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.\nThe following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.\nIllustrations by Jay Alammar\nSohl-Dickstein et al., 2015\nHo et al. 2020\nRombach \u0026amp; Blattmann, et al. 2022\nLilian Weng on Diffusion Models\nSergios Karagiannakos on Diffusion Models\nHugging Face on Annotated Diffusion Model\nIf you recently introduced yourself to strangers and told them about your job in AI, there is a good chance they asked you about the current hype-train around the next generation of generative AI models and related data products like Dall-E, Stable Diffusion or Midjourney.\nLatent Diffusion Models like the ones above had some significant media attention. While no one outside AI community bats an eye if Deepmind creates an algorithm, that beats the (almost ancient) and important Strassen-Algorithm by some percent in computation complexity(which is a tremendous progress), nearly everyone is excited to create made up pictures of cats doing crazy stuff through a simple interface.\nStable Diffusion \u0026ldquo;cat surfing waves at sunset, comic style\u0026rdquo;\nWhile those models already made a name for themselves by winning art competitions, are adopted by companies into their related data products(Canva.com, Shutterstock.com) and start-ups creating those products raising billions in venture capital you may ask yourself:\nWhat is all the fuzz about? What is behind the hype? What are Latent Diffusion Models? What is the math behind them? Do they impact my life? What is the best way to leverage their power? Let me briefly introduce you to Diffusion Models and Latent Diffusion Models and explain the math.\nIf you are interested in a Hands-On you can find that in my other post:\nHands on Latent Diffusion Models\nIf you want an awesome visual introduction with diagrams i strongly advise to visit the blog by Jay Alammar.\nWhat is this post about: A brand-new category of cutting-edge generative models called diffusion models generate a variety of high-resolution images. They have already received a great deal of attention as a result of OpenAI, Nvidia, and Google\u0026rsquo;s success in training massive models. We\u0026rsquo;ll take a deeper look into Denoising Diffusion Probabilistic Models (also known as DDPMs, diffusion models, score-based generative models or simply autoencoders) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include GLIDE and DALL-E 2 by OpenAI, Latent Diffusion by the University of Heidelberg and ImageGen by Google Brain.\nIntuition on Diffusion Models You may ask \u0026ldquo;What is the intuition behind diffusion models?\u0026rdquo; Let\u0026rsquo;s break it down with a short example to make it clear: You are a painter hired by Vatican with the task to repaint the fresco at the ceiling of sixtinian chapel.\nThe requirement is to recreate the fresco with pictures of cats. The requirement is based on the old fresco and the vatican wants to have the same scenes as currently there but witch cats. So you start remembering the fresco and start bringing up a base coat and the old fresco soon becomes a big grey noise.\nMost likely you will be overwhelmed with creating a big fresco and immediately think of structuring your work into smaller chunks. Then you may outline the structures you want to paint based on your memory on the old picture and your vision on the new one.\nMaybe you start with one object like the arm of Adam, from the creation of Adam and then focus on the hand. Gradually you add more details and finally are happy with the scene, decide to finish it and tackle the next part. Still later you may change things after you decided that it fits better with the overall fresco.\nFinally you are building a masterpiece lasting centuries until someone thinks dogs are nicer than cats(so never).\nThe same idea comes with Diffusion: Gradually add noise and create the best representation of the input vision in many small steps. Breaking up the image sampling allows the models to correct itself over those small steps iteratively and produces a good sample.\nUnfortunately nothing is cost-neutral. Like it will cost a painter like Michealangelo almost 4 years to finish the fresco in Sixtinian chapel, the iterative process makes the Models slow at sampling(At least compared to GANs).\nWhat are Diffusion Models? The idea of diffusion for generative modeling was introduced in (Sohl-Dickstein et al., 2015). However, it took until (Song et al., 2019) (at Stanford University), and then (Ho et al., 2020) (at Google Brain) who independently improved the approach. DDPM which we are focussing on originally was introduced in a paper by (Ho et al., 2020).\nAt a high level, they work by first representing the input signal as a set of latent variables, which are then transformed through a series of probabilistic transformations to produce the output signal. The transformation process is designed to smooth out the noise in the input signal and reconstruct a cleaner version of the signal.\nTransformation consist of 2 processes.\nIn a bit more detail for images, the set-up consists of 2 processes:\na fixed (or predefined) forward diffusion process \\(q\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise a learned reverse denoising diffusion process \\(p_\\theta\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. Diffusion Models are basically generative models: Overview of the different types of generative models\nI want to see the math Diffusion In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. E.g. Brownian motion\nWikipedia says:\nA Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\nA continuous-time Markov chain (CTMC) is a continuous stochastic process in which, for each state, the process will change state according to an exponential random variable and then move to a different state as specified by the probabilities of a stochastic matrix\nDiffusion consists of 2 processes:\na fixed (or predefined) forward diffusion process $q$ of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise a learned reverse denoising diffusion process $p_\\theta$, where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise. (Image source: Ho et al. 2020) and Lilian Weng\nForward Diffusion Given a data point from a real data distribution $x_0 \\sim q(x)$ we define a forward diffusion in which we add small Gaussian noise stepwise for $T$ steps producing noisy samples $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$\nStep sizes are controlled by a variance schedule $0 \u0026lt; \\beta_1 \u0026lt; \\beta_2 \u0026lt; \u0026hellip; \u0026lt; \\beta_T \u0026lt; 1$.\nIt is defined as\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I})$ with $\\sqrt{1 - \\beta_t} x_{t-1}$ as decay towards origin and\n$\\beta_t \\mathbf{I}$ as the addition of small noise.\nRephrased: A normal distribution (also called Gaussian distribution) is defined by 2 parameters:\na mean $\\mu$ and a variance $\\sigma^2 \\geq 0$. Basically, each new (slightly noisier) image at time step $t$ is drawn from a conditional Gaussian distribution with\n$\\mathbf{\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}$ and $\\sigma^2_t = \\beta_t$, which we can do by sampling $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$ and then setting $\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\mathbf{\\epsilon}$.\nGiven a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an isotropic Gaussian distribution at $t=T$ via a gradual process.\nIsotropic means the probability density is equal (iso) in every direction (tropic). In gaussians this can be achieved with a $\\sigma^2 I$ covariance matrix.\nOne property of the diffusion process is, that you can sample $x_t$ at any time step $t$ using reparameterization trick. Let $\\alpha_{t} = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n$$ \\begin{aligned} \\mathbf{x}_t \u0026amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \u0026amp; \\text{ ;where } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\ \u0026amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \u0026amp; \\text{ ;where } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2} \u0026amp; \\text{ ;where } \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\text{ merges two Gaussians.} \\\\ \u0026amp;= \\dots \\\\ \u0026amp;= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}\\\\ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) \u0026amp;= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I}) \\end{aligned} $$\nSo the sampling of noise and creation of $x_t$ is done in one step only and can be sampled at any timestep.\nReverse Diffusion $q(x_{t-1} \\vert x_t)$ which denotes the Reverse Process is intractable since statistical estimates of it require computations involving the entire dataset and therefore we need to learn a model $p_0$ to approximate these conditional probabilities in order to run the reverse diffusion process.\nWe need to learn a model $p_0$ to approximate these conditional probabilities\nSince $q(x_{t-1} \\vert x_t)$ will also be Gaussian, for small enough $\\beta_t$, we can choose $p_0$ to be Gaussian and just parameterize the mean and variance as the Reverse Diffusion:\n$\\quad p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N} (\\mathbf{x}_{t-1}; {\\mu}_\\theta(\\mathbf{x}_t, t), {\\Sigma}_\\theta(\\mathbf{x}_t, t)) $\nwith $\\mu_\\theta(x_{t},t)$ as the mean and $\\Sigma_\\theta (x_{t},t)$ as the variance\nconditioned on the noise level $t$ as the to be learned functions of drift and covariance of the Gaussians(by a Neural Net).\nAs the target image is already defined the problem can be described as a supervised learning problem.\nAn example of training a diffusion model for modeling a 2D swiss roll data. (Image source: Sohl-Dickstein et al., 2015)\nHence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to keep the variance fixed, and let the neural network only learn (represent) the mean $\\mu_\\theta$ of this conditional probability distribution.\nOptimization of the Loss Function To derive an objective function to learn the mean of the backward process, the authors observe that the combination of $q$ and $p_\\theta$ can be seen as a variational auto-encoder (VAE) (Kingma et al., 2013).\nA Diffusion Model can be trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood\n$- \\log p_\\theta(\\mathbf{x}_0)$.\nAfter transformation(Lilian Weng for reference), we can write the evidence lower bound (ELBO) as follows:\n$$ \\begin{aligned} - \\log p_\\theta(\\mathbf{x}_0) \u0026amp;\\leq - \\log p_\\theta(\\mathbf{x}_0)+ D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\vert p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) ) \\end{aligned} $$\nIntuition on the optimization: For a function $f(x)$, which can\u0026rsquo;t be computed(like e.g. the above negative log-likelihood) and have also a function $g(x)$, which we can compute and fullfills the condition $g(x) \u0026lt;= f(x)$. If we then maximize $g(x)$ we can be certain that $f(x)$ will also increase.\nFor optimization we use Kullback-Leibler (KL) Divergences. The KL Divergence is a statistical distance measure of how much one probability distribution $P$ differs from a reference distribution $Q$.\nWe are interested in formulating the Loss function in terms of KL divergences because the transition distributions in our Markov chain are Gaussians, and the KL divergence between Gaussians has a closed form. For a closer look please look here\nIf we rewrite the above Loss function and apply the bayesian rule the upper term can be summarized to a joint probability and will be trainsformed to the Variational Lower Bound:\n$$ \\begin{aligned} \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{\\mathbf{x}_{1:T}\\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\ \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \\text{Let }L_\\text{VLB} \u0026amp;= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\geq - \\mathbb{E}_{q(\\mathbf{x}_0)} \\log p_\\theta(\\mathbf{x}_0) \\end{aligned} $$\nComplete calculation can be found here together with a really nice explanation here\nThe objective can be further rewritten to be a combination of several KL-divergence and entropy terms(Detailed process in Appendix B in Sohl-Dickstein et al., 2015)\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \u0026amp;= \\dots \\\\ \u0026amp;= \\mathbb{E}_q [\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} + \\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ] \\end{aligned} $$\nReshaped:\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= L_T + L_{T-1} + \\dots + L_0 \\\\ \\text{where } L_T \u0026amp;= D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T)) \\\\ L_t \u0026amp;= D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1 \\\\ L_0 \u0026amp;= - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1) \\end{aligned} $$\nEvery KL term in $L_\\text{VLB}$ except for $L_0$ compares two Gaussian distributions and therefore they can be computed in closed form. $L_T$ is constant and can be ignored during training because $q$ has no learnable parameters and $x_T$ is a Gaussian noise. $L_t$ formulates the difference between the desired denoising steps and the approximated ones.\nIt is evident that through the ELBO, maximizing the likelihood boils down to learning the denoising steps $L_t$.\nWe would like to train $\\boldsymbol{\\mu}_\\theta$ to predict $\\tilde{\\boldsymbol{\\mu}}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)$. Because $\\mathbf{x}_t$ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict $\\boldsymbol{\\epsilon}_t$ from the input $\\mathbf{x}_t$ at time step $t$:\n$$ \\begin{aligned} \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \u0026amp;= \\color{red}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)} \\\\ \\text{Thus }\\mathbf{x}_{t-1} \u0026amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) \\end{aligned} $$\nThe loss term $L_t$ is parameterized to minimize the difference from $\\tilde\\mu$ :\n$$ \\begin{aligned} L_t \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 | \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) |^2_2} | \\color{blue}{\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} |^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 |\\boldsymbol{\\Sigma}_\\theta |^2_2} | \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big)} |^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) | \\boldsymbol{\\Sigma}_\\theta |^2_2} |\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) | \\boldsymbol{\\Sigma}_\\theta |^2_2} |\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)|^2 \\Big] \\end{aligned} $$\nThe final objective function $L_t$ then looks as follows (for a random time step $t$ given $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ ) as shown by Ho et al. (2020)\n$$ | \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) |^2 = | \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t) } \\mathbf{\\epsilon}, t) |^2.$$\nHere, $\\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\\mathbf{\\epsilon}$ is the pure noise sampled at time step $t$, and $\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\nHere, $\\mathbf{x}_0$ is the initial (real, uncorrupted) image, and we see the direct noise level $t$ sample given by the fixed forward process. $\\mathbf{\\epsilon}$ is the pure noise sampled at time step $t$, and $\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)$ is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\nThe training algorithm now looks as follows:\nIn other words:\nwe take a random sample $\\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\\mathbf{x}_0)$ we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step) we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ (using the nice property defined above) the neural network is trained to predict this noise based on the corrupted image $\\mathbf{x}_t$ (i.e. noise applied on $\\mathbf{x}_0$ based on known schedule $\\beta_t$ ) Neural Nets The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?\nWhat is typically used here is very similar to that of an Autoencoder, which you may remember from typical \u0026ldquo;intro to deep learning\u0026rdquo; tutorials. Autoencoders have a so-called \u0026ldquo;bottleneck\u0026rdquo; layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the \u0026ldquo;bottleneck\u0026rdquo;, and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\nIn terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).\nThe math on Latent Diffusion It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as can be up to one or a few thousand steps. One data point from Song et al. 2020: “For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.”\nLatent diffusion model (LDM; Rombach \u0026amp; Blattmann, et al. 2022) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.\nIt is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression.\nLDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.\nThe perceptual compression process relies on an autoencoder model.\nAn encoder $\\mathcal{E}$ is used to compress the input image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times 3}$ to a smaller 2D latent vector $\\mathbf{z} = \\mathcal{E}(\\mathbf{x}) \\in \\mathbb{R}^{h \\times w \\times c}$ , where the downsampling rate $f=H/h=W/w=2^m, m \\in \\mathbb{N}$.\nThen an decoder $\\mathcal{D}$ reconstructs the images from the latent vector, $\\tilde{\\mathbf{x}} = \\mathcal{D}(\\mathbf{z})$.\nThe paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.\nKL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to VAE. VQ-reg: Uses a vector quantization layer within the decoder, like VQVAE but the quantization layer is absorbed by the decoder. The diffusion and denoising processes happen on the latent vector $\\mathbf{z}$. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image).\nThe design is equivalent to fuse representation of different modality into the model with cross-attention mechanism.\nEach type of conditioning information is paired with a domain-specific encoder $\\tau_\\theta$ to project the conditioning input $y$ to an intermediate representation that can be mapped into cross-attention component, $\\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_\\tau}$:\n$$ \\begin{aligned} \u0026amp;\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\Big(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d}}\\Big) \\cdot \\mathbf{V} \\\\ \u0026amp;\\text{where }\\mathbf{Q} = \\mathbf{W}^{(i)}_Q \\cdot \\varphi_i(\\mathbf{z}_i),; \\mathbf{K} = \\mathbf{W}^{(i)}_K \\cdot \\tau_\\theta(y),; \\mathbf{V} = \\mathbf{W}^{(i)}_V \\cdot \\tau_\\theta(y) \\\\ \u0026amp;\\text{and } \\mathbf{W}^{(i)}_Q \\in \\mathbb{R}^{d \\times d^i_\\epsilon},; \\mathbf{W}^{(i)}_K, \\mathbf{W}^{(i)}_V \\in \\mathbb{R}^{d \\times d_\\tau},; \\varphi_i(\\mathbf{z}_i) \\in \\mathbb{R}^{N \\times d^i_\\epsilon},; \\tau_\\theta(y) \\in \\mathbb{R}^{M \\times d_\\tau} \\end{aligned} $$\nPicture from J. Rafid Siddiqui\nAnd what is the result? Stable Diffusion \u0026ldquo;a cat looking at the ocean at sunset\u0026rdquo;\nStable Diffusion \u0026ldquo;a cat looking at the ocean at sunset\u0026rdquo;\nRessources The Annotated Diffusion Model\nIllustrations by Jay Alammar\nSohl-Dickstein et al., 2015\nRombach \u0026amp; Blattmann, et al. 2022\nHo et al. 2020\nLilian Weng on Diffusion Models\nSergios Karagiannakos on Diffusion Models\nHugging Face on Annotated Diffusion Model\nAssembly AI on Diffusion\nJ. Rafid Siddiqui on Latent Diffusion\nHow does Stable Diffusion work? – Latent Diffusion Models EXPLAINED\nStable Diffusion videos from fast.ai\n","permalink":"https://www.patrickschnass.de/posts/latent-diffusion-models/","summary":"TL;DR The following learning notes try to give some intuition on how Stable Diffusion works, some mathematical intuition on Diffusion Models and an introduction to Latent Diffusion Models.\nThe following sources were extensively used during creation of this learning notes. Passages may be reutilized to create a reasonable overview of the topic. Credit goes to the authors of the following papers and posts.\nIllustrations by Jay Alammar\nSohl-Dickstein et al., 2015","title":"Latent Diffusion Models: What is all the fuzz about?"},{"content":"Prerequitises\nTo test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:\nTools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects. Recap on Latent Diffusion Models There are mutiple sites and blog posts which explain Latent Diffusion Models including my own Latent Diffusion Models: What is all the fuzz about?\nTo keep it a bit lightweight i can recommend one which explains everything with diagrams(Because i like diagrams for learning).\nBlogpost of Jay Alammar\nI don\u0026rsquo;t understand anything You don\u0026rsquo;t have any idea what this is all about?\nYou can generate beautiful pictures with the help of AI All you need to do is create a prompt and enter it into any tool using an algorithm like stable-diffusion which renders your image then. So\nThink of a prompt Examples with prompt search and Go to Dall-E Open an account and try it out. NoCode Quickstart You are not interested in getting your hands dirty? You don\u0026rsquo;t want to code? You just want to produce some nice looking images and test your prompt skills? You are not willing to pay a certain amount to use the capabilities of OpenAI\u0026rsquo;s Dall-E?\nThen this is for you:\nPrompt Ideas and References For starters, do you have any idea what you want to create and how to best create your initial prompt?\nYes\nAwesome, but as in Google Search: When you try to find the correct search prompt you need to tune the semantics of your thoughts to get what you want: How to write stable-diffusion prompts\nOf course AI can help you with this: Prompt Tuning\nNo\nNo worries, you are not the first one to create a prompt and there are already a lot of examples out there:\nExamples with prompt search\nAtlas on examples with topics\nUse an Endpoint with Stable Diffusion There are already a few websites giving you access to endpoints for free. I recommend to use one where you still have access to the codebase of the model and some evaluation. StabilityAI, the creators of stable-diffusion, an open source latent diffusion model host their model on Huggingface and give access to an endpoint (here called spaces) to test it out:\nStable Diffusion 2.1 Demo by Stability AI\nModel Card of Stable Diffusion v2\nExample Following prompt:\n\u0026ldquo;oil painting of a cat sitting on a rainbow\u0026rdquo;\nbecomes after finetuning:\n\u0026ldquo;oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha\u0026rdquo;\nand creates this picture with stable-diffusion: I like cats!(Like everyone else on the internet i guess)\nEnjoy exploring!\nIf you are interested in understanding how to create a Notebook with diffusors please see the following section.\nStable Diffusion \u0026hellip;using Hugging Face\u0026rsquo;s diffusers\n*The following section focusses on inference and is based on Quickstart with diffusers and Intro on diffusers\nIf you want to get a more hands-on guide on training diffusion models, please have a look at Training with Diffusers\nSummary on Diffusion Models Diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\nThe underlying model, often a neural network, is trained to predict a way to slightly denoise the image in each step. After certain number of steps, a sample is obtained.\nThe diffusion process consists in taking random noise of the size of the desired output and pass it through the model several times. The process ends after a given number of steps, and the output image should represent a sample according to the training data distribution of the model, for instance an image of a cat.\nDuring training we show many samples of a given distribution, such as images of cat. After training, the model will be able to process random noise to generate similar cat images.\nWithout going in too much detail, the model is usually not trained to directly predict a slightly less noisy image, but rather to predict the \u0026ldquo;noise residual\u0026rdquo; which is the difference between a less noisy image and the input image (for a diffusion model called \u0026ldquo;DDPM\u0026rdquo;).\nTo do the denoising process, a specific noise scheduling algorithm is thus necessary and \u0026ldquo;wrap\u0026rdquo; the model to define how many diffusion steps are needed for inference as well as how to compute a less noisy image from the model\u0026rsquo;s output.\nSummary on diffusers Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion, proposed in High-Resolution Image Synthesis with Latent Diffusion Models.\nIt is created by the researchers and engineers from CompVis, Stability AI and LAION. It\u0026rsquo;s trained on 512x512 images from a subset of the LAION-5B database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs. See the model card for more information.\nHowever, most of the recent research on diffusion models, e.g. DALL-E 2 and Imagen, is unfortunately not accessible to the broader machine learning community and typically remains behind closed doors.\nHere comes Hugging Face\u0026rsquo;s library for diffusion model: diffusers with the goals to:\ngather recent diffusion models from independent repositories in a single and long-term maintained project that is built by and for the community, reproduce high impact machine learning systems such as DALLE and Imagen in a manner that is accessible for the public, and create an easy to use API that enables one to train their own models or re-use checkpoints from other repositories for inference. The core API of diffusers is divided into three components:\nPipelines: high-level classes designed to rapidly generate samples from popular trained diffusion models in a user-friendly fashion. Models: popular architectures for training new diffusion models, e.g. UNet. Schedulers: various techniques for generating images from noise during inference as well as to generate noisy images for training. How-to create an Image Install diffusers !pip install diffusers==0.11.0 !pip install transformers scipy ftfy accelerate !pip install \u0026#34;ipywidgets\u0026gt;=7,\u0026lt;8\u0026#34; !pip install safetensors Input your Hugging Face Token As mentioned earlier you need a token with huggingface to import the pretrained snapshots\nfrom huggingface_hub import notebook_login notebook_login() Pipeline StableDiffusionPipeline is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.\nFirst, we load the pre-trained weights of all components of the model. Here we use Stable Diffusion version 2.1 (stabilityai/stable-diffusion-2-1), but there are other variants that you may want to try:\nrunwayml/stable-diffusion-v1-5 stabilityai/stable-diffusion-2-1-base stabilityai/stable-diffusion-2-1. This version can produce images with a resolution of 768x768, while the others work at 512x512. This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.\nIn addition to the model id stabilityai/stable-diffusion-2-1, we\u0026rsquo;re also passing a specific torch_dtype to the from_pretrained method.\nThe weights are loaded from the half-precision branch fp16 and we need to tell diffusers to expect the weights in float16 precision by passing torch_dtype=torch.float16.\nWe can import the DDPMPipeline, which will allow you to do inference with a couple of lines of code. The from_pretrained() method allows downloading the model and its configuration from the Hugging Face Hub, a repository of over 60,000 models shared by the community.\nimport torch from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler model_id = \u0026#34;stabilityai/stable-diffusion-2-1\u0026#34; # Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16) pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) The pipe shows now all components contained in your desired process.\npipe StableDiffusionPipeline { \u0026#34;_class_name\u0026#34;: \u0026#34;StableDiffusionPipeline\u0026#34;, \u0026#34;_diffusers_version\u0026#34;: \u0026#34;0.11.0\u0026#34;, \u0026#34;feature_extractor\u0026#34;: [ \u0026#34;transformers\u0026#34;, \u0026#34;CLIPImageProcessor\u0026#34; ], \u0026#34;requires_safety_checker\u0026#34;: false, \u0026#34;safety_checker\u0026#34;: [ null, null ], \u0026#34;scheduler\u0026#34;: [ \u0026#34;diffusers\u0026#34;, \u0026#34;DDIMScheduler\u0026#34; ], \u0026#34;text_encoder\u0026#34;: [ \u0026#34;transformers\u0026#34;, \u0026#34;CLIPTextModel\u0026#34; ], \u0026#34;tokenizer\u0026#34;: [ \u0026#34;transformers\u0026#34;, \u0026#34;CLIPTokenizer\u0026#34; ], \u0026#34;unet\u0026#34;: [ \u0026#34;diffusers\u0026#34;, \u0026#34;UNet2DConditionModel\u0026#34; ], \u0026#34;vae\u0026#34;: [ \u0026#34;diffusers\u0026#34;, \u0026#34;AutoencoderKL\u0026#34; ] } Model Instances of the model class are neural networks that take a noisy sample as well as a timestep as inputs to predict a less noisy output sample.\nHere a simple UNet2DConditionModel which was released with the DDPM Paper is used.\npipe.unet Similarly to what we\u0026rsquo;ve seen for the pipeline class, we can load the model configuration and weights with one line, using the from_pretrained() method. It caches the model weights locally.\nScheduler Schedulers define the noise schedule which is used to add noise to the model during training, and also define the algorithm to compute the slightly less noisy sample given the model output (here noisy_residual).\nIt is important to stress here that while models have trainable weights, schedulers are usually parameter-free (in the sense they have no trainable weights) and simply define the algorithm to compute the slightly less noisy sample.\npipe.scheduler Generate Image To generate an image, we simply run the pipeline and don\u0026rsquo;t even need to give it any input, it will generate a random initial noise sample and then iterate the diffusion process. Here we use the inital prompt from above\nThe pipeline returns as output a dictionary with a generated sample of interest.\nprompt = \u0026#34;oil painting of a cat sitting on a rainbow grass florest, sunset, cliffside ocean scene, diffuse lighting, fantasy, intricate, elegant, highly detailed, lifelike, photorealistic, digital painting, artstation, illustration, concept art, smooth, sharp focus, art by John Collier and Albert Aublet and Krenz Cushart and Artem Demura and Alphonse Mucha\u0026#34; image = pipe(prompt).images[0] # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/) image.save(f\u0026#34;rainbow_cat.png\u0026#34;) Et voila\nA good video on the topic combining intuition, code and a hands-on can be found on the Youtube Channel by Edan Meyer\nReferences Latent Diffusion Models: What is all the fuzz about?\nHugging Face\nBlogpost of Jay Alammar\nDall-E\nExamples with prompt search\nAtlas on examples with topics\nHow to write stable-diffusion prompts\nPrompt Tuning\nFurther Links What\u0026rsquo;s HuggingFace on Medium\n","permalink":"https://www.patrickschnass.de/posts/hands-on-latent-diffusion-models/","summary":"Prerequitises\nTo test the models here you need to have an account with HuggingFace - for loading the checkpoint or using the endpoints. Hugging Face is a community and data science platform that provides:\nTools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. A place where a broad community of data scientists, researchers, and ML engineers can come together and share ideas, get support and contribute to open source projects.","title":"Hands on with Latent Diffusion Models"},{"content":"This summary is based on another post: An introduction to ChatGPT written by ChatGPT\nWhile testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous. In this post i tried to condense the meaningful information from the other post.\n2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content. Leading generative AI tools are: DeepMind’s Alpha Code (GoogleLab), OpenAI\u0026rsquo;s ChatGPT, GPT-3.5, DALL-E, MidJourney, Jasper, and Stable Diffusion, which are large language models and image generators.\nEspecially ChatGPT(Generative Pre-trained Transformer) by OpenAI(an artificial intelligence research laboratory), the latest in AI language models had serious media attention in the last weeks. Derek Thompson wrote in The Atlantic\u0026rsquo;s \u0026ldquo;Breakthroughs of the Year\u0026rdquo; for 2022, that ChatGPT as part of \u0026ldquo;the generative-AI eruption\u0026rdquo; that \u0026ldquo;may change our mind about how we work, how we think, and what human creativity really is\u0026rdquo;.\nChatGPT is based on GPT3 (Generative Pre-trained Transformer 3) from OpenAI, a large language model that was trained using deep learning. Large language are large language models that attempt to generate human-like text and can be used for a variety of natural language processing tasks, including language translation and question answering.\nHow does ChatGPT work? The success of ChatGPT is based on training with human feedback, the so-called reinforcement learning on human feedback. This is also one of the biggest differences to previous language models. In the latest ChatGPT version, OpenAI incentivizes the feedback process in order to get even more feedback data, and sees RLHF as fundamental to artificial intelligence that takes human needs into account and thus for the development of further AI systems.\nTo collect data for training ChatGPT, the model is exposed to a variety of conversational data, including transcripts of real-world conversations, dialogues from books and movies, and other sources of conversational text. This data is used to train the model and help it understand the structure and patterns of natural language. In addition, the training is enriched by human responses and thus tuned.\nThe reward model in ChatGPT is used to evaluate the model\u0026rsquo;s performance and provide feedback on its answers. This is done through a process known as reinforcement learning, in which the model is rewarded when it generates responses that are relevant, appropriate, and human-like, and penalized when it generates responses that are irrelevant or nonsensical. This feedback helps the model learn and improve its performance over time.\nPolicies help prevent ChatGPT from generating inappropriate or offensive responses and can ensure that the model behaves in a way that is consistent with the values and norms of the organization or the people using it. For example, a policy could dictate that ChatGPT may not generate sexist, racist, or otherwise discriminatory responses.\nWhat are the limitations and pitfalls of ChatGPT? ChatGPT is not capable of real understanding, and its answers are based on the data it was trained on and the algorithms that drive its behavior. ChatGPT is unable to really handle the complexities of human language and conversation. ChatGPT is not intelligent.\nIt is trained to generate words based on a given input without the ability to really understand the meaning behind those words. That means any answers it generates are likely to be superficial and lack depth and insight. When using it, you also notice a repetitive and monotonous language. There are already models that are able to distinguish between text generated by a GPT model and text written by a human.\nIf the model was trained on data that is biased or discriminatory, it can generate responses that are unfair or offensive but sound reasonable or logical. Therefore, it is important to carefully review and evaluate ChatGPT\u0026rsquo;s responses. Because the model definitely generates wrong answers that sound semantically correct. It is therefore important to ensure that ChatGPT is used in an ethical and responsible manner.\nWhat brings the future? The entry of AI models like ChatGPT into normal everyday work will become reality in the coming months to years. Microsoft - shareholder of OpenAI - is already thinking aloud about using the model in Bing or Office products. Despite some problems, they still have enormous potential to speed up and improve daily work.\nAnother concrete example is Github Copilot. Copilot is powered by OpenAI Codex, another model of OpenAI. Codex is a modified production version of GPT-3. The Codex model is also trained on source code and specifically aims at generating program code. The Copilot beta ended in 2022 and you can already significantly improve your code generation workflow.\nChatGPT is an impressive development in the field of artificial intelligence, and marks a media milestone, but is only a snapshot. GPT4 is the next generation of the GPT language model and will be even more powerful than GPT3. GPT-4 consists of 170 trillion parameters compared to GPT-3\u0026rsquo;s 175 billion parameters. The accuracy of the models will therefore increase further in the future and will continue to find their way into our everyday lives.\nReferences ChatGPT: Optimizing Language Models for Dialogue\nAtlantic: The rise of AI\nAI Hopes and Horrors\n","permalink":"https://www.patrickschnass.de/posts/summary-chatgpt/","summary":"This summary is based on another post: An introduction to ChatGPT written by ChatGPT\nWhile testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous. In this post i tried to condense the meaningful information from the other post.\n2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content.","title":"A short summary on ChatGPT"},{"content":"ChatGPT: Optimizing Language Models for Dialogue\nWhat to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.\nWhat is ChatGPT? Are you tired of boring, robotic chatbots that can’t hold a conversation or understand your needs? If so, you’ll be excited to hear about ChatGPT, the latest and greatest in AI language models. In this blog post, we’ll take a closer look at ChatGPT, its capabilities, and why it’s a major step forward for AI. So if you’re interested in the future of conversational AI, keep reading!\nChatGPT is a new AI language model developed by OpenAI that has the ability to understand and respond to conversational inputs in a natural and engaging way. This is a major step forward in the field of natural language processing, as it allows for more effective and engaging communication with AI systems. In this blog post, we’ll take a closer look at ChatGPT and its capabilities.\nOne of the key capabilities of ChatGPT is its ability to understand and respond to a wide range of conversational inputs. This includes both simple requests and more complex questions, making it a powerful tool for a variety of applications. ChatGPT has also been trained on a massive amount of data, allowing it to have a deep understanding of natural language and the ability to generate human-like responses.\nHow is ChatGPT related to GPT3? ChatGPT and GPT3 are related in that ChatGPT is a variant of GPT3 that has been specifically designed for conversational AI applications. GPT3, also known as Generative Pretrained Transformer 3, is a large language model trained by OpenAI. It is capable of generating human-like text and has been used for a wide range of natural language processing tasks, including language translation, summarization, and question answering.\nChatGPT is based on GPT3 and shares many of its capabilities, including its ability to generate human-like text. However, ChatGPT has been trained on a large amount of conversational data, allowing it to understand and respond to conversational inputs in a natural and engaging way. This makes it a powerful tool for conversational AI applications, such as virtual assistants and chatbots.\nIn summary, ChatGPT and GPT3 are related in that ChatGPT is a specialized variant of GPT3 that has been designed for conversational AI applications. While both models share many similar capabilities, ChatGPT has been specifically trained and optimized for conversational AI tasks.\nWhat happens when ChatGPT4 will be published in the next few months? While ChatGPT is an impressive development in AI, it is still only a teaser for what is to come with GPT4. GPT4 is the next generation of the GPT language model, and is expected to be even more powerful and capable than ChatGPT. With GPT4, we can expect even more advanced natural language processing capabilities, as well as the ability to handle a wider range of tasks and applications.\nOverall, ChatGPT is a major step forward for AI and natural language processing, but it is only a glimpse of what is to come with GPT4. As we continue to advance in the field of AI, we can expect even more impressive developments and capabilities in the near future.\nHow does the collection of data and reward model in ChatGPT work? The collection of data and reward model in ChatGPT are two key components of the model’s training process. In order to generate human-like text and engage in natural conversations, ChatGPT must be trained on large amounts of data and be able to learn from the feedback it receives.\nTo collect data for ChatGPT, the model is exposed to a wide range of conversational data, including transcripts of real-life conversations, dialogue from books and movies, and other sources of conversational text. This data is used to train the model and help it understand the structure and patterns of natural language.\nThe reward model in ChatGPT is used to evaluate the model’s performance and provide feedback on its responses. This is done through a process known as reinforcement learning, in which the model is rewarded for generating responses that are relevant, appropriate, and human-like, and is penalized for generating responses that are irrelevant or nonsensical. This feedback helps the model to learn and improve its performance over time.\nIn summary, the collection of data and reward model are key components of ChatGPT’s training process. The model is trained on large amounts of conversational data, and uses reinforcement learning to evaluate its performance and improve its responses.\nWhy are policies important for ChatGPT? Policies are important for ChatGPT because they help to ensure that the model behaves in an appropriate and ethical manner. In the context of conversational AI, policies are sets of rules and guidelines that dictate how the model should respond to different inputs and situations.\nPolicies can help to prevent ChatGPT from generating inappropriate or offensive responses, and can ensure that the model behaves in a way that aligns with the values and standards of the organization or individuals using it. For example, a policy might dictate that ChatGPT should not generate responses that are sexist, racist, or otherwise discriminatory.\nPolicies can also help to ensure that ChatGPT is used in a responsible and ethical manner. For example, a policy might dictate that ChatGPT should not be used for malicious purposes, such as spreading misinformation or harassing individuals.\nIn summary, policies are important for ChatGPT because they help to ensure that the model behaves in an appropriate and ethical manner. By defining rules and guidelines for the model’s behavior, policies can help to prevent inappropriate or offensive responses, and can ensure that ChatGPT is used in a responsible and ethical way.\nWhat are the pitfalls of ChatGPT? The pitfalls of ChatGPT include the potential for the model to generate inappropriate or offensive responses, the risk of bias and discrimination, and the potential for the model to be used for malicious purposes.\nOne of the main pitfalls of ChatGPT is the potential for the model to generate inappropriate or offensive responses. Because ChatGPT is trained on large amounts of data, it is possible that the model may be exposed to offensive or inappropriate language. If this language is included in the model’s training data, ChatGPT may generate responses that are offensive or inappropriate, even if it is not intended to do so.\nAnother potential pitfall of ChatGPT is the risk of bias and discrimination. Because the model is trained on data from the real world, it may inherit the biases and prejudices that are present in this data. For example, if the model is trained on data that is predominantly from a certain demographic group, it may generate responses that are biased towards or discriminatory towards other groups.\nFinally, another pitfall of ChatGPT is the potential for the model to be used for malicious purposes. Because ChatGPT is capable of generating human-like text, it could be used to spread misinformation or to harass or deceive individuals. This could have harmful consequences, and it is important to ensure that ChatGPT is used in an ethical and responsible manner.\nIn summary, the pitfalls of ChatGPT include the potential for the model to generate inappropriate or offensive responses, the risk of bias and discrimination, and the potential for the model to be used for malicious purposes. It is important to carefully consider these potential pitfalls and to develop policies and guidelines to prevent and mitigate them.\nI heard rumors, is ChatGPT producing wrong answers? It is possible that ChatGPT may sometimes produce results that are wrong but sound right. As a large language model, ChatGPT is capable of generating human-like text and engaging in natural conversations. However, because it is trained on data from the real world, it may inherit the biases, prejudices, and errors that are present in this data.\nFor example, if ChatGPT is trained on data that contains misinformation or false information, it may generate responses that are incorrect but sound convincing. Similarly, if the model is trained on data that is biased or discriminatory, it may generate responses that are unfair or offensive, but that sound reasonable or logical.\nTherefore, it is important to carefully monitor and evaluate ChatGPT’s responses, and to provide the model with high-quality, accurate data to train on. This can help to prevent the model from generating wrong answers that sound right, and can help to ensure that ChatGPT is used in an ethical and responsible manner.\nHow is ChatGPT related to ELIZA effect and AGI? ChatGPT is related to the ELIZA effect in that both involve the use of artificial intelligence to simulate natural language conversation. The ELIZA effect is a phenomenon in which individuals may attribute human-like qualities to a conversational AI system, even if the system is not capable of genuine intelligence or understanding. ChatGPT is a large language model that has been trained on a wide range of conversational data, allowing it to generate human-like text and engage in natural language conversations. However, like all AI systems, ChatGPT is not capable of true intelligence or understanding, and its responses are based on the data it has been trained on and the algorithms that govern its behavior. Despite this, individuals who interact with ChatGPT may be susceptible to the ELIZA effect, and may attribute human-like qualities to the model.\nChatGPT is also related to artificial general intelligence (AGI), which is a term used to describe the hypothetical ability of a machine to understand or learn any intellectual task that a human being can. While ChatGPT is a powerful and capable language model, it is not capable of AGI, and is not capable of understanding or learning any intellectual task that a human being can. ChatGPT is a specialized AI system that has been designed for a specific purpose — natural language processing — and is not capable of the general intelligence that is characteristic of AGI.\nIn summary, ChatGPT is related to the ELIZA effect in that both involve the use of AI to simulate natural language conversation, and individuals who interact with ChatGPT may be susceptible to the ELIZA effect. ChatGPT is also related to AGI in that it is a powerful and capable language model, but it is not capable of the general intelligence that is characteristic of AGI.\nSummary ChatGPT is a large language model that has been specifically designed for conversational AI applications. It has been trained on a wide range of conversational data, allowing it to understand and respond to a wide range of inputs in a natural and engaging way. Prompts are used to provide ChatGPT with a starting point for generating text, and the model is evaluated and improved through a process known as reinforcement learning.\nHowever, ChatGPT is not perfect, and may sometimes generate responses that are incorrect or inappropriate. The potential for ChatGPT to produce wrong answers is one of the pitfalls of the model, and is something that should be carefully considered when using the model for conversational AI applications. Additionally, the use of ChatGPT may be subject to the ELIZA effect, in which individuals may attribute human-like qualities to the model, even if it is not capable of genuine intelligence or understanding.\nOverall, ChatGPT is a major step forward for AI and is a powerful tool for conversational AI applications. However, it is important to carefully monitor and evaluate the model’s performance, and to provide it with high-quality training data. It is also important to develop policies and guidelines to ensure that the model is used in an ethical and responsible manner.\nFurther Links Introducing ChatGPT by Cassie Kozyrkov\nhttps://medium.com/@kozyrkov/introducing-chatgpt-aa824ad89623\nThe Batch by DeepLearningAI — Andrew Ng on LLM\nhttps://www.deeplearning.ai/the-batch/issue-174/\nAI Snake Oil on why ChatGPT is a bullshit generator\nhttps://aisnakeoil.substack.com/p/chatgpt-is-a-bullshit-generator-but\n","permalink":"https://www.patrickschnass.de/posts/chatgpt-on-chatgpt/","summary":"ChatGPT: Optimizing Language Models for Dialogue\nWhat to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.\nWhat is ChatGPT? Are you tired of boring, robotic chatbots that can’t hold a conversation or understand your needs?","title":"An introduction to ChatGPT written by ChatGPT"},{"content":"Welcome to my blog.\nWhy not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.\nIsn\u0026rsquo;t it actually dumb to create your own blog instead of using these possibilities?\nMaybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.\nWell, couldn\u0026rsquo;t you have done this also on Medium?\nI tried, but starting to create content i saw some pitfalls with Medium:\nThe writing interface is straightforward and easy to use but it is actually too simple and gives almost no flexibility. Medium attracts a lot of good writers but also a lot of people just producing content with low quality. If you don\u0026rsquo;t optimize your articles you won\u0026rsquo;t attract any readers. So for my purpose there is no difference whether i use a private blog or Medium. Medium does not support Math Formulas! I could not believe it but there was no decent possibility to include math formulas with Latex or somehow else. An absolute No Go in my eyes Ok i get it, Medium is not for you but a blog is a lot of maintenance and effort. How do you find time for this?\nLuckily it is actually quite easy to create your own static site with Github Pages and Hugo. Also it is a good chance to learn somehting new and make yourself familiar with web design again. Ok, i am curious. How did you do it?\nHow did you create your blog? Which tools are you using for your blog? As written, i use Github Pages and the open-source static site generator Hugo which is written in Go. (I chose Hugo without having done a lot of research. There are other generators like e.g. Jekyll but Hugo was open-source, easy to use, blazing fast and free so no need to look further).\nAnd how did you do actually do it? Install Hugo\nbrew install hugo If you have a different system than MacOS check the official installation guide.\nCreate a new site in Hugo\nWhen you decide on a name, think that the name is also the folder and the name of the Repo. To work with Github Pages it needs to have the same name as your git user followed by .github.io\nhugo new site \u0026lt;user\u0026gt;.github.io -f yml I decided to use PaperMod theme and it recommends to use .yml Config instead of .toml - and i am fine with that because i like yaml files.\nInstall your favorite Hugo Theme\nThe project is created but if you try to run it, it will just be an empty page. A style is needed to make it fully functional.\nYou could create one from scratch but Hugo has a bunch of themes already prepared and ready to use! You can go to https://themes.gohugo.io/ and choose a theme you like. I like PaperMod\ngit init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod Adjust your config.yml with theme: PaperMod\nbaseURL: http://\u0026lt;user\u0026gt;.github.io/ languageCode: en-us title: My New Hugo Site theme: PaperMod Create Content\nImportant is your content folder. The folder tree in this folder will reflect your site folder tree. Either you create folders and markdown files in those folders manually or you use\nhugo new e.g.\nhugo new /content/posts/first-post/first-post.md I would recommend to create a folder per post to store your pictures in the same folder as your post they relate to\n--- heading: \u0026quot;Welcome to my blog\u0026quot; subheading: \u0026quot;This is my first-post\u0026quot; --- Look at your new site\nWith\nhugo you create the html out of your content in your public folder. Please keep in mind that for Github Pages you can only choose docs as your root directory for your index.html. Therefore you need to add following config to your config.yml\npublishDir: docs With\nhugo server -D you can see the site on localhost with port 1313. -D is used to include drafts.\nPush to github\nNow you can push your site to github\ngit add . git commit -m \u0026quot;initial commit\u0026quot; git push origin main Configure Github Pages In your repo settings under Pages the root folder needs to be adjusted and your site will hopefully be deployed soon.\nWith this short introduction you should be able to set up your own blog really fast and in worst case troubleshoot your way through. Enjoy!\nPS: In Germany you have to add an imprint and a privacy policy to your website if it is not for friends and family only. As the definition is vague it seems that any public site private or commercial needs it. If you decide to make your website public please consider to add imprint and privacy policy. You will find a lot of generators on the internet which can provide the neccesary texts.\nI am happy with my new blog and will further play around with it. If you like my content connect via LinkedIN.\nReferences [1] Github Pages (https://pages.github.com)\n[2] Hugo (https://gohugo.io)\n[3] PaperMod (https://github.com/adityatelange/hugo-PaperMod)\n[4] Markdownguide (https://www.markdownguide.org/basic-syntax/)\n[5] Image clickable (https://discourse.gohugo.io/t/how-can-i-make-images-clickable-so-i-can-zoom-them-to-full-screen/34279)\nFurther Links [6] Folder Structure (https://jpdroege.com/blog/hugo-file-organization/)\n[7] Markdown with VS Code (https://code.visualstudio.com/docs/languages/markdown)\n[8] Trouble with Image paths (https://github.com/adityatelange/hugo-PaperMod/discussions/690)\n[9] Work in Codespaces (https://shotor.com/blog/build-a-hugo-static-site-in-your-browser-using-github-codespaces/)\n[10] Katex for PaperMod (https://adityatelange.github.io/hugo-PaperMod/posts/math-typesetting/)\n[11] Google Analytics with Hugo (https://gohugo.io/templates/internal/#use-the-google-analytics-template)\n","permalink":"https://www.patrickschnass.de/posts/how-to-blog/","summary":"Welcome to my blog.\nWhy not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.\nIsn\u0026rsquo;t it actually dumb to create your own blog instead of using these possibilities?\nMaybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.","title":"Why and how do you create your own AI blog?"},{"content":"Datenschutzerklärung\nStand: 21. Januar 2023\nInhaltsübersicht\nVerantwortlicher Übersicht der Verarbeitungen Maßgebliche Rechtsgrundlagen Sicherheitsmaßnahmen Übermittlung von personenbezogenen Daten Datenverarbeitung in Drittländern Löschung von Daten Einsatz von Cookies Bereitstellung des Onlineangebotes und Webhosting Blogs und Publikationsmedien Kontakt- und Anfragenverwaltung Webanalyse, Monitoring und Optimierung Präsenzen in sozialen Netzwerken (Social Media) Plugins und eingebettete Funktionen sowie Inhalte Änderung und Aktualisierung der Datenschutzerklärung Rechte der betroffenen Personen Verantwortlicher\nÜbersicht der Verarbeitungen\nDie nachfolgende Übersicht fasst die Arten der verarbeiteten Daten und die Zwecke ihrer Verarbeitung zusammen und verweist auf die betroffenen Personen.\nArten der verarbeiteten Daten\nBestandsdaten. Standortdaten. Kontaktdaten. Inhaltsdaten. Nutzungsdaten. Meta-, Kommunikations- und Verfahrensdaten. Kategorien betroffener Personen\nKommunikationspartner. Nutzer. Zwecke der Verarbeitung\nErbringung vertraglicher Leistungen und Kundenservice. Kontaktanfragen und Kommunikation. Sicherheitsmaßnahmen. Reichweitenmessung. Tracking. Verwaltung und Beantwortung von Anfragen. Feedback. Marketing. Profile mit nutzerbezogenen Informationen. Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit. Informationstechnische Infrastruktur. Maßgebliche Rechtsgrundlagen\nIm Folgenden erhalten Sie eine Übersicht der Rechtsgrundlagen der DSGVO, auf deren Basis wir personenbezogene Daten verarbeiten. Bitte nehmen Sie zur Kenntnis, dass neben den Regelungen der DSGVO nationale Datenschutzvorgaben in Ihrem bzw. unserem Wohn- oder Sitzland gelten können. Sollten ferner im Einzelfall speziellere Rechtsgrundlagen maßgeblich sein, teilen wir Ihnen diese in der Datenschutzerklärung mit.\nEinwilligung (Art. 6 Abs. 1 S. 1 lit. a) DSGVO) - Die betroffene Person hat ihre Einwilligung in die Verarbeitung der sie betreffenden personenbezogenen Daten für einen spezifischen Zweck oder mehrere bestimmte Zwecke gegeben. Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO) - Die Verarbeitung ist zur Wahrung der berechtigten Interessen des Verantwortlichen oder eines Dritten erforderlich, sofern nicht die Interessen oder Grundrechte und Grundfreiheiten der betroffenen Person, die den Schutz personenbezogener Daten erfordern, überwiegen. Zusätzlich zu den Datenschutzregelungen der DSGVO gelten nationale Regelungen zum Datenschutz in Deutschland. Hierzu gehört insbesondere das Gesetz zum Schutz vor Missbrauch personenbezogener Daten bei der Datenverarbeitung (Bundesdatenschutzgesetz – BDSG). Das BDSG enthält insbesondere Spezialregelungen zum Recht auf Auskunft, zum Recht auf Löschung, zum Widerspruchsrecht, zur Verarbeitung besonderer Kategorien personenbezogener Daten, zur Verarbeitung für andere Zwecke und zur Übermittlung sowie automatisierten Entscheidungsfindung im Einzelfall einschließlich Profiling. Des Weiteren regelt es die Datenverarbeitung für Zwecke des Beschäftigungsverhältnisses (§ 26 BDSG), insbesondere im Hinblick auf die Begründung, Durchführung oder Beendigung von Beschäftigungsverhältnissen sowie die Einwilligung von Beschäftigten. Ferner können Landesdatenschutzgesetze der einzelnen Bundesländer zur Anwendung gelangen.\nNeben den Datenschutzbestimmungen der DSGVO können bei nationalen Bezugspunkten auch die jeweiligen nationalen Datenschutzbestimmungen gelten, die wir ebenfalls befolgen.\nSicherheitsmaßnahmen\nWir treffen nach Maßgabe der gesetzlichen Vorgaben unter Berücksichtigung des Stands der Technik, der Implementierungskosten und der Art, des Umfangs, der Umstände und der Zwecke der Verarbeitung sowie der unterschiedlichen Eintrittswahrscheinlichkeiten und des Ausmaßes der Bedrohung der Rechte und Freiheiten natürlicher Personen geeignete technische und organisatorische Maßnahmen, um ein dem Risiko angemessenes Schutzniveau zu gewährleisten.\nZu den Maßnahmen gehören insbesondere die Sicherung der Vertraulichkeit, Integrität und Verfügbarkeit von Daten durch Kontrolle des physischen und elektronischen Zugangs zu den Daten als auch des sie betreffenden Zugriffs, der Eingabe, der Weitergabe, der Sicherung der Verfügbarkeit und ihrer Trennung. Des Weiteren haben wir Verfahren eingerichtet, die eine Wahrnehmung von Betroffenenrechten, die Löschung von Daten und Reaktionen auf die Gefährdung der Daten gewährleisten. Ferner berücksichtigen wir den Schutz personenbezogener Daten bereits bei der Entwicklung bzw. Auswahl von Hardware, Software sowie Verfahren entsprechend dem Prinzip des Datenschutzes, durch Technikgestaltung und durch datenschutzfreundliche Voreinstellungen.\nÜbermittlung von personenbezogenen Daten\nIm Rahmen unserer Verarbeitung von personenbezogenen Daten kommt es vor, dass die Daten an andere Stellen, Unternehmen, rechtlich selbstständige Organisationseinheiten oder Personen übermittelt oder sie ihnen gegenüber offengelegt werden. Zu den Empfängern dieser Daten können z.B. mit IT-Aufgaben beauftragte Dienstleister oder Anbieter von Diensten und Inhalten, die in eine Webseite eingebunden werden, gehören. In solchen Fällen beachten wir die gesetzlichen Vorgaben und schließen insbesondere entsprechende Verträge bzw. Vereinbarungen, die dem Schutz Ihrer Daten dienen, mit den Empfängern Ihrer Daten ab.\nDatenverarbeitung in Drittländern\nSofern wir Daten in einem Drittland (d.h., außerhalb der Europäischen Union (EU), des Europäischen Wirtschaftsraums (EWR)) verarbeiten oder die Verarbeitung im Rahmen der Inanspruchnahme von Diensten Dritter oder der Offenlegung bzw. Übermittlung von Daten an andere Personen, Stellen oder Unternehmen stattfindet, erfolgt dies nur im Einklang mit den gesetzlichen Vorgaben.\nVorbehaltlich ausdrücklicher Einwilligung oder vertraglich oder gesetzlich erforderlicher Übermittlung verarbeiten oder lassen wir die Daten nur in Drittländern mit einem anerkannten Datenschutzniveau, vertraglichen Verpflichtung durch sogenannte Standardschutzklauseln der EU-Kommission, beim Vorliegen von Zertifizierungen oder verbindlicher internen Datenschutzvorschriften verarbeiten (Art. 44 bis 49 DSGVO, Informationsseite der EU-Kommission: https://ec.europa.eu/info/law/law-topic/data-protection/international-dimension-data-protection_de).\nLöschung von Daten\nDie von uns verarbeiteten Daten werden nach Maßgabe der gesetzlichen Vorgaben gelöscht, sobald deren zur Verarbeitung erlaubten Einwilligungen widerrufen werden oder sonstige Erlaubnisse entfallen (z.B. wenn der Zweck der Verarbeitung dieser Daten entfallen ist oder sie für den Zweck nicht erforderlich sind). Sofern die Daten nicht gelöscht werden, weil sie für andere und gesetzlich zulässige Zwecke erforderlich sind, wird deren Verarbeitung auf diese Zwecke beschränkt. D.h., die Daten werden gesperrt und nicht für andere Zwecke verarbeitet. Das gilt z.B. für Daten, die aus handels- oder steuerrechtlichen Gründen aufbewahrt werden müssen oder deren Speicherung zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person erforderlich ist.\nIm Rahmen unserer Datenschutzhinweise können wir den Nutzern weitere Informationen zu der Löschung sowie zu der Aufbewahrung von Daten mitteilen, die speziell für die jeweiligen Verarbeitungsprozesses gelten.\nEinsatz von Cookies\nCookies sind kleine Textdateien, bzw. sonstige Speichervermerke, die Informationen auf Endgeräten speichern und Informationen aus den Endgeräten auslesen. Z.B. um den Login-Status in einem Nutzerkonto, einen Warenkorbinhalt in einem E-Shop, die aufgerufenen Inhalte oder verwendete Funktionen eines Onlineangebotes speichern. Cookies können ferner zu unterschiedlichen Zwecken eingesetzt werden, z.B. zu Zwecken der Funktionsfähigkeit, Sicherheit und Komfort von Onlineangeboten sowie der Erstellung von Analysen der Besucherströme.\nHinweise zur Einwilligung: Wir setzen Cookies im Einklang mit den gesetzlichen Vorschriften ein. Daher holen wir von den Nutzern eine vorhergehende Einwilligung ein, außer wenn diese gesetzlich nicht gefordert ist. Eine Einwilligung ist insbesondere nicht notwendig, wenn das Speichern und das Auslesen der Informationen, also auch von Cookies, unbedingt erforderlich sind, um dem den Nutzern einen von ihnen ausdrücklich gewünschten Telemediendienst (also unser Onlineangebot) zur Verfügung zu stellen. Die widerrufliche Einwilligung wird gegenüber den Nutzern deutlich kommuniziert und enthält die Informationen zu der jeweiligen Cookie-Nutzung.\nHinweise zu datenschutzrechtlichen Rechtsgrundlagen: Auf welcher datenschutzrechtlichen Rechtsgrundlage wir die personenbezogenen Daten der Nutzer mit Hilfe von Cookies verarbeiten, hängt davon ab, ob wir Nutzer um eine Einwilligung bitten. Falls die Nutzer einwilligen, ist die Rechtsgrundlage der Verarbeitung Ihrer Daten die erklärte Einwilligung. Andernfalls werden die mithilfe von Cookies verarbeiteten Daten auf Grundlage unserer berechtigten Interessen (z.B. an einem betriebswirtschaftlichen Betrieb unseres Onlineangebotes und Verbesserung seiner Nutzbarkeit) verarbeitet oder, wenn dies im Rahmen der Erfüllung unserer vertraglichen Pflichten erfolgt, wenn der Einsatz von Cookies erforderlich ist, um unsere vertraglichen Verpflichtungen zu erfüllen. Zu welchen Zwecken die Cookies von uns verarbeitet werden, darüber klären wir im Laufe dieser Datenschutzerklärung oder im Rahmen von unseren Einwilligungs- und Verarbeitungsprozessen auf.\nSpeicherdauer: Im Hinblick auf die Speicherdauer werden die folgenden Arten von Cookies unterschieden:\nTemporäre Cookies (auch: Session- oder Sitzungs-Cookies): Temporäre Cookies werden spätestens gelöscht, nachdem ein Nutzer ein Online-Angebot verlassen und sein Endgerät (z.B. Browser oder mobile Applikation) geschlossen hat. Permanente Cookies: Permanente Cookies bleiben auch nach dem Schließen des Endgerätes gespeichert. So können beispielsweise der Login-Status gespeichert oder bevorzugte Inhalte direkt angezeigt werden, wenn der Nutzer eine Website erneut besucht. Ebenso können die mit Hilfe von Cookies erhobenen Daten der Nutzer zur Reichweitenmessung verwendet werden. Sofern wir Nutzern keine expliziten Angaben zur Art und Speicherdauer von Cookies mitteilen (z. B. im Rahmen der Einholung der Einwilligung), sollten Nutzer davon ausgehen, dass Cookies permanent sind und die Speicherdauer bis zu zwei Jahre betragen kann. Allgemeine Hinweise zum Widerruf und Widerspruch (Opt-Out): Nutzer können die von ihnen abgegebenen Einwilligungen jederzeit widerrufen und zudem einen Widerspruch gegen die Verarbeitung entsprechend den gesetzlichen Vorgaben im Art. 21 DSGVO einlegen. Nutzer können ihren Widerspruch auch über die Einstellungen ihres Browsers erklären, z.B. durch Deaktivierung der Verwendung von Cookies (wobei dadurch auch die Funktionalität unserer Online-Dienste eingeschränkt sein kann). Ein Widerspruch gegen die Verwendung von Cookies zu Online-Marketing-Zwecken kann auch über die Websites https://optout.aboutads.info und https://www.youronlinechoices.com/ erklärt werden.\nWeitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nVerarbeitung von Cookie-Daten auf Grundlage einer Einwilligung: Wir setzen ein Verfahren zum Cookie-Einwilligungs-Management ein, in dessen Rahmen die Einwilligungen der Nutzer in den Einsatz von Cookies, bzw. der im Rahmen des Cookie-Einwilligungs-Management-Verfahrens genannten Verarbeitungen und Anbieter eingeholt sowie von den Nutzern verwaltet und widerrufen werden können. Hierbei wird die Einwilligungserklärung gespeichert, um deren Abfrage nicht erneut wiederholen zu müssen und die Einwilligung entsprechend der gesetzlichen Verpflichtung nachweisen zu können. Die Speicherung kann serverseitig und/oder in einem Cookie (sogenanntes Opt-In-Cookie, bzw. mithilfe vergleichbarer Technologien) erfolgen, um die Einwilligung einem Nutzer, bzw. dessen Gerät zuordnen zu können. Vorbehaltlich individueller Angaben zu den Anbietern von Cookie-Management-Diensten, gelten die folgenden Hinweise: Die Dauer der Speicherung der Einwilligung kann bis zu zwei Jahren betragen. Hierbei wird ein pseudonymer Nutzer-Identifikator gebildet und mit dem Zeitpunkt der Einwilligung, Angaben zur Reichweite der Einwilligung (z. B. welche Kategorien von Cookies und/oder Diensteanbieter) sowie dem Browser, System und verwendeten Endgerät gespeichert. Bereitstellung des Onlineangebotes und Webhosting\nWir verarbeiten die Daten der Nutzer, um ihnen unsere Online-Dienste zur Verfügung stellen zu können. Zu diesem Zweck verarbeiten wir die IP-Adresse des Nutzers, die notwendig ist, um die Inhalte und Funktionen unserer Online-Dienste an den Browser oder das Endgerät der Nutzer zu übermitteln.\nVerarbeitete Datenarten: Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus). Betroffene Personen: Nutzer (z.B. Webseitenbesucher, Nutzer von Onlinediensten). Zwecke der Verarbeitung: Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit; Informationstechnische Infrastruktur (Betrieb und Bereitstellung von Informationssystemen und technischen Geräten (Computer, Server etc.).); Sicherheitsmaßnahmen. Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Weitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nErhebung von Zugriffsdaten und Logfiles: Der Zugriff auf unser Onlineangebot wird in Form von so genannten \u0026ldquo;Server-Logfiles\u0026rdquo; protokolliert. Zu den Serverlogfiles können die Adresse und Name der abgerufenen Webseiten und Dateien, Datum und Uhrzeit des Abrufs, übertragene Datenmengen, Meldung über erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite) und im Regelfall IP-Adressen und der anfragende Provider gehören. Die Serverlogfiles können zum einen zu Zwecken der Sicherheit eingesetzt werden, z.B., um eine Überlastung der Server zu vermeiden (insbesondere im Fall von missbräuchlichen Angriffen, sogenannten DDoS-Attacken) und zum anderen, um die Auslastung der Server und ihre Stabilität sicherzustellen; Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO); Löschung von Daten: Logfile-Informationen werden für die Dauer von maximal 30 Tagen gespeichert und danach gelöscht oder anonymisiert. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endgültigen Klärung des jeweiligen Vorfalls von der Löschung ausgenommen. Blogs und Publikationsmedien\nWir nutzen Blogs oder vergleichbare Mittel der Onlinekommunikation und Publikation (nachfolgend \u0026ldquo;Publikationsmedium\u0026rdquo;). Die Daten der Leser werden für die Zwecke des Publikationsmediums nur insoweit verarbeitet, als es für dessen Darstellung und die Kommunikation zwischen Autoren und Lesern oder aus Gründen der Sicherheit erforderlich ist. Im Übrigen verweisen wir auf die Informationen zur Verarbeitung der Besucher unseres Publikationsmediums im Rahmen dieser Datenschutzhinweise.\nVerarbeitete Datenarten: Bestandsdaten (z.B. Namen, Adressen); Kontaktdaten (z.B. E-Mail, Telefonnummern); Inhaltsdaten (z.B. Eingaben in Onlineformularen); Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus). Betroffene Personen: Nutzer (z.B. Webseitenbesucher, Nutzer von Onlinediensten). Zwecke der Verarbeitung: Erbringung vertraglicher Leistungen und Kundenservice; Feedback (z.B. Sammeln von Feedback via Online-Formular); Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit; Sicherheitsmaßnahmen; Verwaltung und Beantwortung von Anfragen. Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Weitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nKommentare und Beiträge: Wenn Nutzer Kommentare oder sonstige Beiträge hinterlassen, können ihre IP-Adressen auf Grundlage unserer berechtigten Interessen gespeichert werden. Das erfolgt zu unserer Sicherheit, falls jemand in Kommentaren und Beiträgen widerrechtliche Inhalte hinterlässt (Beleidigungen, verbotene politische Propaganda etc.). In diesem Fall können wir selbst für den Kommentar oder Beitrag belangt werden und sind daher an der Identität des Verfassers interessiert. Des Weiteren behalten wir uns vor, auf Grundlage unserer berechtigten Interessen die Angaben der Nutzer zwecks Spamerkennung zu verarbeiten. Auf derselben Rechtsgrundlage behalten wir uns vor, im Fall von Umfragen die IP-Adressen der Nutzer für deren Dauer zu speichern und Cookies zu verwenden, um Mehrfachabstimmungen zu vermeiden. Die im Rahmen der Kommentare und Beiträge mitgeteilten Informationen zur Person, etwaige Kontakt- sowie Webseiteninformationen als auch die inhaltlichen Angaben werden von uns bis zum Widerspruch der Nutzer dauerhaft gespeichert; Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Kontakt- und Anfragenverwaltung\nBei der Kontaktaufnahme mit uns (z.B. per Kontaktformular, E-Mail, Telefon oder via soziale Medien) sowie im Rahmen bestehender Nutzer- und Geschäftsbeziehungen werden die Angaben der anfragenden Personen verarbeitet soweit dies zur Beantwortung der Kontaktanfragen und etwaiger angefragter Maßnahmen erforderlich ist.\nVerarbeitete Datenarten: Kontaktdaten (z.B. E-Mail, Telefonnummern); Inhaltsdaten (z.B. Eingaben in Onlineformularen); Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus). Betroffene Personen: Kommunikationspartner. Zwecke der Verarbeitung: Kontaktanfragen und Kommunikation; Verwaltung und Beantwortung von Anfragen; Feedback (z.B. Sammeln von Feedback via Online-Formular); Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit. Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Webanalyse, Monitoring und Optimierung\nDie Webanalyse (auch als \u0026ldquo;Reichweitenmessung\u0026rdquo; bezeichnet) dient der Auswertung der Besucherströme unseres Onlineangebotes und kann Verhalten, Interessen oder demographische Informationen zu den Besuchern, wie z.B. das Alter oder das Geschlecht, als pseudonyme Werte umfassen. Mit Hilfe der Reichweitenanalyse können wir z.B. erkennen, zu welcher Zeit unser Onlineangebot oder dessen Funktionen oder Inhalte am häufigsten genutzt werden oder zur Wiederverwendung einladen. Ebenso können wir nachvollziehen, welche Bereiche der Optimierung bedürfen.\nNeben der Webanalyse können wir auch Testverfahren einsetzen, um z.B. unterschiedliche Versionen unseres Onlineangebotes oder seiner Bestandteile zu testen und optimieren.\nSofern nachfolgend nicht anders angegeben, können zu diesen Zwecken Profile, d.h. zu einem Nutzungsvorgang zusammengefasste Daten angelegt und Informationen in einem Browser, bzw. in einem Endgerät gespeichert und aus diesem ausgelesen werden. Zu den erhobenen Angaben gehören insbesondere besuchte Webseiten und dort genutzte Elemente sowie technische Angaben, wie der verwendete Browser, das verwendete Computersystem sowie Angaben zu Nutzungszeiten. Sofern Nutzer in die Erhebung ihrer Standortdaten uns gegenüber oder gegenüber den Anbietern der von uns eingesetzten Dienste einverstanden erklärt haben, können auch Standortdaten verarbeitet werden.\nEs werden ebenfalls die IP-Adressen der Nutzer gespeichert. Jedoch nutzen wir ein IP-Masking-Verfahren (d.h., Pseudonymisierung durch Kürzung der IP-Adresse) zum Schutz der Nutzer. Generell werden die im Rahmen von Webanalyse, A/B-Testings und Optimierung keine Klardaten der Nutzer (wie z.B. E-Mail-Adressen oder Namen) gespeichert, sondern Pseudonyme. D.h., wir als auch die Anbieter der eingesetzten Software kennen nicht die tatsächliche Identität der Nutzer, sondern nur den für Zwecke der jeweiligen Verfahren in deren Profilen gespeicherten Angaben.\nVerarbeitete Datenarten: Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus). Betroffene Personen: Nutzer (z.B. Webseitenbesucher, Nutzer von Onlinediensten). Zwecke der Verarbeitung: Reichweitenmessung (z.B. Zugriffsstatistiken, Erkennung wiederkehrender Besucher); Profile mit nutzerbezogenen Informationen (Erstellen von Nutzerprofilen); Tracking (z.B. interessens-/verhaltensbezogenes Profiling, Nutzung von Cookies); Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit. Sicherheitsmaßnahmen: IP-Masking (Pseudonymisierung der IP-Adresse). Rechtsgrundlagen: Einwilligung (Art. 6 Abs. 1 S. 1 lit. a) DSGVO). Weitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nGoogle Analytics: Webanalyse, Reichweitenmessung sowie Messung von Nutzerströmen; Dienstanbieter: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland; Rechtsgrundlagen: Einwilligung (Art. 6 Abs. 1 S. 1 lit. a) DSGVO); Website: https://marketingplatform.google.com/intl/de/about/analytics/; Datenschutzerklärung: https://policies.google.com/privacy; Auftragsverarbeitungsvertrag: https://business.safety.google/adsprocessorterms; Standardvertragsklauseln (Gewährleistung Datenschutzniveau bei Verarbeitung in Drittländern): https://business.safety.google/adsprocessorterms; Widerspruchsmöglichkeit (Opt-Out): Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=de, Einstellungen für die Darstellung von Werbeeinblendungen: https://adssettings.google.com/authenticated; Weitere Informationen: https://privacy.google.com/businesses/adsservices (Arten der Verarbeitung sowie der verarbeiteten Daten). Präsenzen in sozialen Netzwerken (Social Media)\nWir unterhalten Onlinepräsenzen innerhalb sozialer Netzwerke und verarbeiten in diesem Rahmen Daten der Nutzer, um mit den dort aktiven Nutzern zu kommunizieren oder um Informationen über uns anzubieten.\nWir weisen darauf hin, dass dabei Daten der Nutzer außerhalb des Raumes der Europäischen Union verarbeitet werden können. Hierdurch können sich für die Nutzer Risiken ergeben, weil so z.B. die Durchsetzung der Rechte der Nutzer erschwert werden könnte.\nFerner werden die Daten der Nutzer innerhalb sozialer Netzwerke im Regelfall für Marktforschungs- und Werbezwecke verarbeitet. So können z.B. anhand des Nutzungsverhaltens und sich daraus ergebender Interessen der Nutzer Nutzungsprofile erstellt werden. Die Nutzungsprofile können wiederum verwendet werden, um z.B. Werbeanzeigen innerhalb und außerhalb der Netzwerke zu schalten, die mutmaßlich den Interessen der Nutzer entsprechen. Zu diesen Zwecken werden im Regelfall Cookies auf den Rechnern der Nutzer gespeichert, in denen das Nutzungsverhalten und die Interessen der Nutzer gespeichert werden. Ferner können in den Nutzungsprofilen auch Daten unabhängig der von den Nutzern verwendeten Geräte gespeichert werden (insbesondere, wenn die Nutzer Mitglieder der jeweiligen Plattformen sind und bei diesen eingeloggt sind).\nFür eine detaillierte Darstellung der jeweiligen Verarbeitungsformen und der Widerspruchsmöglichkeiten (Opt-Out) verweisen wir auf die Datenschutzerklärungen und Angaben der Betreiber der jeweiligen Netzwerke.\nAuch im Fall von Auskunftsanfragen und der Geltendmachung von Betroffenenrechten weisen wir darauf hin, dass diese am effektivsten bei den Anbietern geltend gemacht werden können. Nur die Anbieter haben jeweils Zugriff auf die Daten der Nutzer und können direkt entsprechende Maßnahmen ergreifen und Auskünfte geben. Sollten Sie dennoch Hilfe benötigen, dann können Sie sich an uns wenden.\nVerarbeitete Datenarten: Kontaktdaten (z.B. E-Mail, Telefonnummern); Inhaltsdaten (z.B. Eingaben in Onlineformularen); Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus). Betroffene Personen: Nutzer (z.B. Webseitenbesucher, Nutzer von Onlinediensten). Zwecke der Verarbeitung: Kontaktanfragen und Kommunikation; Feedback (z.B. Sammeln von Feedback via Online-Formular); Marketing. Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Weitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nLinkedIn: Soziales Netzwerk; Dienstanbieter: LinkedIn Irland Unlimited Company, Wilton Plaza Wilton Place, Dublin 2, Irland; Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO); Website: https://www.linkedin.com; Datenschutzerklärung: https://www.linkedin.com/legal/privacy-policy; Auftragsverarbeitungsvertrag: https://legal.linkedin.com/dpa; Standardvertragsklauseln (Gewährleistung Datenschutzniveau bei Verarbeitung in Drittländern): https://legal.linkedin.com/dpa; Widerspruchsmöglichkeit (Opt-Out): https://www.linkedin.com/psettings/guest-controls/retargeting-opt-out. Plugins und eingebettete Funktionen sowie Inhalte\nWir binden in unser Onlineangebot Funktions- und Inhaltselemente ein, die von den Servern ihrer jeweiligen Anbieter (nachfolgend bezeichnet als \u0026ldquo;Drittanbieter”) bezogen werden. Dabei kann es sich zum Beispiel um Grafiken, Videos oder Stadtpläne handeln (nachfolgend einheitlich bezeichnet als \u0026ldquo;Inhalte”).\nDie Einbindung setzt immer voraus, dass die Drittanbieter dieser Inhalte die IP-Adresse der Nutzer verarbeiten, da sie ohne die IP-Adresse die Inhalte nicht an deren Browser senden könnten. Die IP-Adresse ist damit für die Darstellung dieser Inhalte oder Funktionen erforderlich. Wir bemühen uns, nur solche Inhalte zu verwenden, deren jeweilige Anbieter die IP-Adresse lediglich zur Auslieferung der Inhalte verwenden. Drittanbieter können ferner sogenannte Pixel-Tags (unsichtbare Grafiken, auch als \u0026ldquo;Web Beacons\u0026rdquo; bezeichnet) für statistische oder Marketingzwecke verwenden. Durch die \u0026ldquo;Pixel-Tags\u0026rdquo; können Informationen, wie der Besucherverkehr auf den Seiten dieser Webseite, ausgewertet werden. Die pseudonymen Informationen können ferner in Cookies auf dem Gerät der Nutzer gespeichert werden und unter anderem technische Informationen zum Browser und zum Betriebssystem, zu verweisenden Webseiten, zur Besuchszeit sowie weitere Angaben zur Nutzung unseres Onlineangebotes enthalten als auch mit solchen Informationen aus anderen Quellen verbunden werden.\nVerarbeitete Datenarten: Nutzungsdaten (z.B. besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten); Meta-, Kommunikations- und Verfahrensdaten (z. B. IP-Adressen, Zeitangaben, Identifikationsnummern, Einwilligungsstatus); Bestandsdaten (z.B. Namen, Adressen); Kontaktdaten (z.B. E-Mail, Telefonnummern); Inhaltsdaten (z.B. Eingaben in Onlineformularen); Standortdaten (Angaben zur geografischen Position eines Gerätes oder einer Person). Betroffene Personen: Nutzer (z.B. Webseitenbesucher, Nutzer von Onlinediensten). Zwecke der Verarbeitung: Bereitstellung unseres Onlineangebotes und Nutzerfreundlichkeit. Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO). Weitere Hinweise zu Verarbeitungsprozessen, Verfahren und Diensten:\nGoogle Maps: Wir binden die Landkarten des Dienstes “Google Maps” des Anbieters Google ein. Zu den verarbeiteten Daten können insbesondere IP-Adressen und Standortdaten der Nutzer gehören; Dienstanbieter: Google Cloud EMEA Limited, 70 Sir John Rogerson’s Quay, Dublin 2, Irland; Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO); Website: https://mapsplatform.google.com/; Datenschutzerklärung: https://policies.google.com/privacy. YouTube-Videos: Videoinhalte; Dienstanbieter: Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Irland; Rechtsgrundlagen: Berechtigte Interessen (Art. 6 Abs. 1 S. 1 lit. f) DSGVO); Website: https://www.youtube.com; Datenschutzerklärung: https://policies.google.com/privacy; Widerspruchsmöglichkeit (Opt-Out): Opt-Out-Plugin: https://tools.google.com/dlpage/gaoptout?hl=de, Einstellungen für die Darstellung von Werbeeinblendungen: https://adssettings.google.com/authenticated. Änderung und Aktualisierung der Datenschutzerklärung\nWir bitten Sie, sich regelmäßig über den Inhalt unserer Datenschutzerklärung zu informieren. Wir passen die Datenschutzerklärung an, sobald die Änderungen der von uns durchgeführten Datenverarbeitungen dies erforderlich machen. Wir informieren Sie, sobald durch die Änderungen eine Mitwirkungshandlung Ihrerseits (z.B. Einwilligung) oder eine sonstige individuelle Benachrichtigung erforderlich wird.\nSofern wir in dieser Datenschutzerklärung Adressen und Kontaktinformationen von Unternehmen und Organisationen angeben, bitten wir zu beachten, dass die Adressen sich über die Zeit ändern können und bitten die Angaben vor Kontaktaufnahme zu prüfen.\nRechte der betroffenen Personen\nIhnen stehen als Betroffene nach der DSGVO verschiedene Rechte zu, die sich insbesondere aus Art. 15 bis 21 DSGVO ergeben:\nWiderspruchsrecht: Sie haben das Recht, aus Gründen, die sich aus Ihrer besonderen Situation ergeben, jederzeit gegen die Verarbeitung der Sie betreffenden personenbezogenen Daten, die aufgrund von Art. 6 Abs. 1 lit. e oder f DSGVO erfolgt, Widerspruch einzulegen; dies gilt auch für ein auf diese Bestimmungen gestütztes Profiling. Werden die Sie betreffenden personenbezogenen Daten verarbeitet, um Direktwerbung zu betreiben, haben Sie das Recht, jederzeit Widerspruch gegen die Verarbeitung der Sie betreffenden personenbezogenen Daten zum Zwecke derartiger Werbung einzulegen; dies gilt auch für das Profiling, soweit es mit solcher Direktwerbung in Verbindung steht. Widerrufsrecht bei Einwilligungen: Sie haben das Recht, erteilte Einwilligungen jederzeit zu widerrufen. Auskunftsrecht: Sie haben das Recht, eine Bestätigung darüber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft über diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend den gesetzlichen Vorgaben. Recht auf Berichtigung: Sie haben entsprechend den gesetzlichen Vorgaben das Recht, die Vervollständigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen. Recht auf Löschung und Einschränkung der Verarbeitung: Sie haben nach Maßgabe der gesetzlichen Vorgaben das Recht, zu verlangen, dass Sie betreffende Daten unverzüglich gelöscht werden, bzw. alternativ nach Maßgabe der gesetzlichen Vorgaben eine Einschränkung der Verarbeitung der Daten zu verlangen. Recht auf Datenübertragbarkeit: Sie haben das Recht, Sie betreffende Daten, die Sie uns bereitgestellt haben, nach Maßgabe der gesetzlichen Vorgaben in einem strukturierten, gängigen und maschinenlesbaren Format zu erhalten oder deren Übermittlung an einen anderen Verantwortlichen zu fordern. Beschwerde bei Aufsichtsbehörde: Entsprechend den gesetzlichen Vorgaben und unbeschadet eines anderweitigen verwaltungsrechtlichen oder gerichtlichen Rechtsbehelfs, haben Sie ferner das Recht, bei einer Datenschutzaufsichtsbehörde, insbesondere einer Aufsichtsbehörde im Mitgliedstaat, in dem Sie sich gewöhnlich aufhalten, der Aufsichtsbehörde Ihres Arbeitsplatzes oder des Ortes des mutmaßlichen Verstoßes, eine Beschwerde einzulegen, wenn Sie der Ansicht sei sollten, dass die Verarbeitung der Ihre Person betreffenden personenbezogenen Daten gegen die DSGVO verstößt.\np . schnass at gmail . com\nPatrick Schnaß Ferdinand Braun Platz 1 Düsseldorf\n","permalink":"https://www.patrickschnass.de/gdpr/","summary":"Datenschutzerklärung\nStand: 21. Januar 2023\nInhaltsübersicht\nVerantwortlicher Übersicht der Verarbeitungen Maßgebliche Rechtsgrundlagen Sicherheitsmaßnahmen Übermittlung von personenbezogenen Daten Datenverarbeitung in Drittländern Löschung von Daten Einsatz von Cookies Bereitstellung des Onlineangebotes und Webhosting Blogs und Publikationsmedien Kontakt- und Anfragenverwaltung Webanalyse, Monitoring und Optimierung Präsenzen in sozialen Netzwerken (Social Media) Plugins und eingebettete Funktionen sowie Inhalte Änderung und Aktualisierung der Datenschutzerklärung Rechte der betroffenen Personen Verantwortlicher\nÜbersicht der Verarbeitungen\nDie nachfolgende Übersicht fasst die Arten der verarbeiteten Daten und die Zwecke ihrer Verarbeitung zusammen und verweist auf die betroffenen Personen.","title":""},{"content":"Welcome to my blog. I\u0026rsquo;m Patrick.\nI\u0026rsquo;m living in Germany, am a proud father of two, work as a Data Scientist and love to dig into AI, Data Science and ML.\nOn this blog i\u0026rsquo;m documenting my learning notes, best practices and how-to\u0026rsquo;s.\nMy blog contains my personal opinions and as i am human also errors. I am happy to receive feedback and corrections. Please feel free to contact me on LinkedIN.\nprivacy policy and imprint\n","permalink":"https://www.patrickschnass.de/about/","summary":"Welcome to my blog. I\u0026rsquo;m Patrick.\nI\u0026rsquo;m living in Germany, am a proud father of two, work as a Data Scientist and love to dig into AI, Data Science and ML.\nOn this blog i\u0026rsquo;m documenting my learning notes, best practices and how-to\u0026rsquo;s.\nMy blog contains my personal opinions and as i am human also errors. I am happy to receive feedback and corrections. Please feel free to contact me on LinkedIN.","title":"About"},{"content":" ","permalink":"https://www.patrickschnass.de/contact/","summary":" ","title":"Contact Me"}]