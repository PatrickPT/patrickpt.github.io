[{"content":"This summary is based on another post: An introduction to ChatGPT written by ChatGPT\nWhile testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous in this post i tried to condense the meaningful information from the other post.\n2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content. Leading generative AI tools are: DeepMind’s Alpha Code (GoogleLab), OpenAI\u0026rsquo;s ChatGPT, GPT-3.5, DALL-E, MidJourney, Jasper, and Stable Diffusion, which are large language models and image generators.\nEspecially ChatGPT(Generative Pre-trained Transformer) by OpenAI(an artificial intelligence research laboratory), the latest in AI language models had serious media attention in the last weeks. Derek Thompson wrote in The Atlantic\u0026rsquo;s \u0026ldquo;Breakthroughs of the Year\u0026rdquo; for 2022, that ChatGPT as part of \u0026ldquo;the generative-AI eruption\u0026rdquo; that \u0026ldquo;may change our mind about how we work, how we think, and what human creativity really is\u0026rdquo;.\nWhat is ChatGPT? ChatGPT technology is built upon OpenAI’s GPT3 AI platform which houses some of the world’s largest language models, using both supervised and unsupervised AI learning techniques. GPT3, also known as Generative Pretrained Transformer 3, is a large language model trained by OpenAI. It is capable of generating human-like text and has been used for a wide range of natural language processing tasks, including language translation, summarization, and question answering.\nHow does ChatGPT work? The collection of data and reward model in ChatGPT are two key components of the model’s training process. In order to generate human-like text and engage in natural conversations, ChatGPT must be trained on large amounts of data and be able to learn from the feedback it receives. To collect data for ChatGPT, the model is exposed to a wide range of conversational data, including transcripts of real-life conversations, dialogue from books and movies, and other sources of conversational text. This data is used to train the model and help it understand the structure and patterns of natural language. The reward model in ChatGPT is used to evaluate the model’s performance and provide feedback on its responses. This is done through a process known as reinforcement learning, in which the model is rewarded for generating responses that are relevant, appropriate, and human-like, and is penalized for generating responses that are irrelevant or nonsensical. This feedback helps the model to learn and improve its performance over time.\nThe success of ChatGPT is based on training with human feedback, so-called reinforcement learning on human feedback. OpenAI incentivises the feedback process in the latest ChatGPT version in order to obtain even more feedback data and sees RLHF as fundamental for artificial intelligence that takes human needs into account and thus for the development of further AI systems. Policies help to prevent ChatGPT from generating inappropriate or offensive responses, and can ensure that the model behaves in a way that aligns with the values and standards of the organization or individuals using it. For example, a policy might dictate that ChatGPT should not generate responses that are sexist, racist, or otherwise discriminatory.\nWhat are the pitfalls of ChatGPT? However, like all current AI systems, ChatGPT is not capable of true intelligence or understanding, and its responses are based on the data it has been trained on and the algorithms that govern its behavior. ChatGPT lacks the ability to truly understand the complexity of human language and conversation. It is simply trained to generate words based on a given input, but it does not have the ability to truly comprehend the meaning behind those words. This means that any responses it generates are likely to be shallow and lacking in depth and insight. Similarly, if the model is trained on data that is biased or discriminatory, it may generate responses that are unfair or offensive, but that sound reasonable or logical. Therefore, it is important to carefully monitor and evaluate ChatGPT’s responses, and to provide the model with high-quality, accurate data to train on. This can help to prevent the model from generating wrong answers that sound right, and can help to ensure that ChatGPT is used in an ethical and responsible manner.\nWhat brings the future? The adoption of conversational AI models like ChatGPT into everday work will be something we see in the next few months to years. Despite all their problems they still have enormous capabilities to fasten and improve daily work. Another recent example is Github Copilot. Copilot is powered by the OpenAI Codex, an artificial intelligence model created by OpenAI The OpenAI Codex is a modified, production version of the Generative Pre-trained Transformer 3 (GPT-3), a language model using deep-learning to produce human-like text. The Codex model is additionally trained on gigabytes of source code in a dozen programming languages. While ChatGPT also can generate Code, Copilot is really focussing on getting the job done and specifically designed to help you coding.\nWhile ChatGPT is an impressive development in AI, it is still only a teaser for what is to come with GPT4. GPT4 is the next generation of the GPT language model, and is expected to be even more powerful and capable than ChatGPT. While GPT-4 consists of 170 trillion parameters compared to GPT-3’s 175 billion parameters. This allows GPT-4 to process and generate text with greater accuracy and fluency.\n2023 will be exciting.\nReferences ChatGPT: Optimizing Language Models for Dialogue\nAtlantic: The rise of AI\nAI Hopes and Horrors\n","permalink":"https://patrickpt.github.io/posts/2023_01_12_summary_chatgpt/2023_01_12_summary_chatgpt/","summary":"This summary is based on another post: An introduction to ChatGPT written by ChatGPT\nWhile testing out ChatGPT for some weeks now, i found that texts created by it are often repetitive and monotonous in this post i tried to condense the meaningful information from the other post.\n2022 was the year of generative AI. Generative AI refers to machine learning algorithms that can create new meaning from text, images, code, and other forms of content.","title":"A short summary on ChatGPT"},{"content":"What are Transformers Transformer is a type of neural network architecture that was introduced in the paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification.\nOne of the key innovations of the Transformer architecture is the use of attention mechanisms. In a traditional neural network, each input is processed independently, without any information about the relationships between the inputs. In contrast, the Transformer model uses attention mechanisms to weight the inputs based on their relevance to the output.\nFor example, in a language translation task, the Transformer model might pay more attention to the words at the beginning and end of a sentence, as these are typically more important for determining the overall meaning of the sentence. On the other hand, it might pay less attention to words that are less important or less relevant to the translation.\nAnother key advantage of the Transformer architecture is its ability to process input sequences in parallel. Traditional recurrent neural networks (RNNs), which are commonly used in NLP tasks, process input sequences one element at a time, making them slow and inefficient. In contrast, the Transformer model processes all elements of the input sequence at the same time, allowing it to run much faster and more efficiently.\nWhat is the base for Transformers? At a high level, the Transformer model is based on the idea of self-attention, which allows the model to weight the input elements based on their relevance to the output. Mathematically, self-attention can be computed using the following formula\n$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nHere, \\(Q\\), \\(K\\), and \\(V\\) are matrices of query, key, and value vectors, respectively. \\(d_k\\) is the dimensionality of the key vectors. The dot product of \\(Q\\) and \\(K^T\\) is divided by the square root of \\(d_k\\) to ensure that the dot products do not become too large and blow up the softmax function. The output of the self-attention layer is a weighted sum of the value vectors, with the weights determined by the dot products of the query and key vectors. In addition to self-attention, the Transformer model also includes feed-forward layers and residual connections. The feed-forward layers consist of a linear transformation followed by a non-linear activation function, such as ReLU. The output of the feed-forward layers is then added to the output of the self-attention layers using residual connections.\nOverall, the Transformer model can be described using the following pseudo-code:\nfor each input sequence: encode input sequence using self-attention and feed-forward layers add the output to the original input using residual connections apply layer normalization apply final self-attention and feed-forward layers to obtain output How to set up a Transformer Model? Here is an example of how you can set up a Transformer model in Python using the Transformers library:\nimport transformers # Set up the Transformer model and tokenizer model_name = 'bert-base-cased' # choose a pre-trained model model = transformers.BertModel.from_pretrained(model_name) tokenizer = transformers.BertTokenizer.from_pretrained(model_name) # Tokenize input text text = \u0026quot;This is some input text that I want to feed into the Transformer model.\u0026quot; input_ids = tokenizer.encode(text, return_tensors='pt') # convert text to numerical input # Run input through the model output = model(input_ids) This code sets up a BertModel from the Transformers library, which is a type of Transformer model developed by Google. It also sets up a BertTokenizer, which is used to convert the input text into a numerical representation that can be fed into the model.\nFinally, the code tokenizes the input text and passes it through the model to obtain the output. The output of the model will be a tensor containing the encoded representation of the input text.\nIn summary, the Transformer architecture is a powerful and effective tool for a wide range of NLP tasks. Its ability to weight input elements using attention mechanisms and to process input sequences in parallel make it well-suited to tasks such as language translation, summarization, and text classification.\n","permalink":"https://patrickpt.github.io/posts/2023_01_09_introduction_to_transformers/2023_01_09_introduction_to_transformers/","summary":"What are Transformers Transformer is a type of neural network architecture that was introduced in the paper \u0026ldquo;Attention is All You Need\u0026rdquo; by Vaswani et al. in 2017. Since then, it has become one of the most popular and successful models in natural language processing (NLP) tasks such as language translation, summarization, and text classification.\nOne of the key innovations of the Transformer architecture is the use of attention mechanisms. In a traditional neural network, each input is processed independently, without any information about the relationships between the inputs.","title":"Introduction to Transformers"},{"content":"ChatGPT: Optimizing Language Models for Dialogue\nWhat to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.\nWhat is ChatGPT? Are you tired of boring, robotic chatbots that can’t hold a conversation or understand your needs? If so, you’ll be excited to hear about ChatGPT, the latest and greatest in AI language models. In this blog post, we’ll take a closer look at ChatGPT, its capabilities, and why it’s a major step forward for AI. So if you’re interested in the future of conversational AI, keep reading!\nChatGPT is a new AI language model developed by OpenAI that has the ability to understand and respond to conversational inputs in a natural and engaging way. This is a major step forward in the field of natural language processing, as it allows for more effective and engaging communication with AI systems. In this blog post, we’ll take a closer look at ChatGPT and its capabilities.\nOne of the key capabilities of ChatGPT is its ability to understand and respond to a wide range of conversational inputs. This includes both simple requests and more complex questions, making it a powerful tool for a variety of applications. ChatGPT has also been trained on a massive amount of data, allowing it to have a deep understanding of natural language and the ability to generate human-like responses.\nHow is ChatGPT related to GPT3? ChatGPT and GPT3 are related in that ChatGPT is a variant of GPT3 that has been specifically designed for conversational AI applications. GPT3, also known as Generative Pretrained Transformer 3, is a large language model trained by OpenAI. It is capable of generating human-like text and has been used for a wide range of natural language processing tasks, including language translation, summarization, and question answering.\nChatGPT is based on GPT3 and shares many of its capabilities, including its ability to generate human-like text. However, ChatGPT has been trained on a large amount of conversational data, allowing it to understand and respond to conversational inputs in a natural and engaging way. This makes it a powerful tool for conversational AI applications, such as virtual assistants and chatbots.\nIn summary, ChatGPT and GPT3 are related in that ChatGPT is a specialized variant of GPT3 that has been designed for conversational AI applications. While both models share many similar capabilities, ChatGPT has been specifically trained and optimized for conversational AI tasks.\nWhat happens when ChatGPT4 will be published in the next few months? While ChatGPT is an impressive development in AI, it is still only a teaser for what is to come with GPT4. GPT4 is the next generation of the GPT language model, and is expected to be even more powerful and capable than ChatGPT. With GPT4, we can expect even more advanced natural language processing capabilities, as well as the ability to handle a wider range of tasks and applications.\nOverall, ChatGPT is a major step forward for AI and natural language processing, but it is only a glimpse of what is to come with GPT4. As we continue to advance in the field of AI, we can expect even more impressive developments and capabilities in the near future.\nHow does the collection of data and reward model in ChatGPT work? The collection of data and reward model in ChatGPT are two key components of the model’s training process. In order to generate human-like text and engage in natural conversations, ChatGPT must be trained on large amounts of data and be able to learn from the feedback it receives.\nTo collect data for ChatGPT, the model is exposed to a wide range of conversational data, including transcripts of real-life conversations, dialogue from books and movies, and other sources of conversational text. This data is used to train the model and help it understand the structure and patterns of natural language.\nThe reward model in ChatGPT is used to evaluate the model’s performance and provide feedback on its responses. This is done through a process known as reinforcement learning, in which the model is rewarded for generating responses that are relevant, appropriate, and human-like, and is penalized for generating responses that are irrelevant or nonsensical. This feedback helps the model to learn and improve its performance over time.\nIn summary, the collection of data and reward model are key components of ChatGPT’s training process. The model is trained on large amounts of conversational data, and uses reinforcement learning to evaluate its performance and improve its responses.\nWhy are policies important for ChatGPT? Policies are important for ChatGPT because they help to ensure that the model behaves in an appropriate and ethical manner. In the context of conversational AI, policies are sets of rules and guidelines that dictate how the model should respond to different inputs and situations.\nPolicies can help to prevent ChatGPT from generating inappropriate or offensive responses, and can ensure that the model behaves in a way that aligns with the values and standards of the organization or individuals using it. For example, a policy might dictate that ChatGPT should not generate responses that are sexist, racist, or otherwise discriminatory.\nPolicies can also help to ensure that ChatGPT is used in a responsible and ethical manner. For example, a policy might dictate that ChatGPT should not be used for malicious purposes, such as spreading misinformation or harassing individuals.\nIn summary, policies are important for ChatGPT because they help to ensure that the model behaves in an appropriate and ethical manner. By defining rules and guidelines for the model’s behavior, policies can help to prevent inappropriate or offensive responses, and can ensure that ChatGPT is used in a responsible and ethical way.\nWhat are the pitfalls of ChatGPT? The pitfalls of ChatGPT include the potential for the model to generate inappropriate or offensive responses, the risk of bias and discrimination, and the potential for the model to be used for malicious purposes.\nOne of the main pitfalls of ChatGPT is the potential for the model to generate inappropriate or offensive responses. Because ChatGPT is trained on large amounts of data, it is possible that the model may be exposed to offensive or inappropriate language. If this language is included in the model’s training data, ChatGPT may generate responses that are offensive or inappropriate, even if it is not intended to do so.\nAnother potential pitfall of ChatGPT is the risk of bias and discrimination. Because the model is trained on data from the real world, it may inherit the biases and prejudices that are present in this data. For example, if the model is trained on data that is predominantly from a certain demographic group, it may generate responses that are biased towards or discriminatory towards other groups.\nFinally, another pitfall of ChatGPT is the potential for the model to be used for malicious purposes. Because ChatGPT is capable of generating human-like text, it could be used to spread misinformation or to harass or deceive individuals. This could have harmful consequences, and it is important to ensure that ChatGPT is used in an ethical and responsible manner.\nIn summary, the pitfalls of ChatGPT include the potential for the model to generate inappropriate or offensive responses, the risk of bias and discrimination, and the potential for the model to be used for malicious purposes. It is important to carefully consider these potential pitfalls and to develop policies and guidelines to prevent and mitigate them.\nI heard rumors, is ChatGPT producing wrong answers? It is possible that ChatGPT may sometimes produce results that are wrong but sound right. As a large language model, ChatGPT is capable of generating human-like text and engaging in natural conversations. However, because it is trained on data from the real world, it may inherit the biases, prejudices, and errors that are present in this data.\nFor example, if ChatGPT is trained on data that contains misinformation or false information, it may generate responses that are incorrect but sound convincing. Similarly, if the model is trained on data that is biased or discriminatory, it may generate responses that are unfair or offensive, but that sound reasonable or logical.\nTherefore, it is important to carefully monitor and evaluate ChatGPT’s responses, and to provide the model with high-quality, accurate data to train on. This can help to prevent the model from generating wrong answers that sound right, and can help to ensure that ChatGPT is used in an ethical and responsible manner.\nHow is ChatGPT related to ELIZA effect and AGI? ChatGPT is related to the ELIZA effect in that both involve the use of artificial intelligence to simulate natural language conversation. The ELIZA effect is a phenomenon in which individuals may attribute human-like qualities to a conversational AI system, even if the system is not capable of genuine intelligence or understanding. ChatGPT is a large language model that has been trained on a wide range of conversational data, allowing it to generate human-like text and engage in natural language conversations. However, like all AI systems, ChatGPT is not capable of true intelligence or understanding, and its responses are based on the data it has been trained on and the algorithms that govern its behavior. Despite this, individuals who interact with ChatGPT may be susceptible to the ELIZA effect, and may attribute human-like qualities to the model.\nChatGPT is also related to artificial general intelligence (AGI), which is a term used to describe the hypothetical ability of a machine to understand or learn any intellectual task that a human being can. While ChatGPT is a powerful and capable language model, it is not capable of AGI, and is not capable of understanding or learning any intellectual task that a human being can. ChatGPT is a specialized AI system that has been designed for a specific purpose — natural language processing — and is not capable of the general intelligence that is characteristic of AGI.\nIn summary, ChatGPT is related to the ELIZA effect in that both involve the use of AI to simulate natural language conversation, and individuals who interact with ChatGPT may be susceptible to the ELIZA effect. ChatGPT is also related to AGI in that it is a powerful and capable language model, but it is not capable of the general intelligence that is characteristic of AGI.\nSummary ChatGPT is a large language model that has been specifically designed for conversational AI applications. It has been trained on a wide range of conversational data, allowing it to understand and respond to a wide range of inputs in a natural and engaging way. Prompts are used to provide ChatGPT with a starting point for generating text, and the model is evaluated and improved through a process known as reinforcement learning.\nHowever, ChatGPT is not perfect, and may sometimes generate responses that are incorrect or inappropriate. The potential for ChatGPT to produce wrong answers is one of the pitfalls of the model, and is something that should be carefully considered when using the model for conversational AI applications. Additionally, the use of ChatGPT may be subject to the ELIZA effect, in which individuals may attribute human-like qualities to the model, even if it is not capable of genuine intelligence or understanding.\nOverall, ChatGPT is a major step forward for AI and is a powerful tool for conversational AI applications. However, it is important to carefully monitor and evaluate the model’s performance, and to provide it with high-quality training data. It is also important to develop policies and guidelines to ensure that the model is used in an ethical and responsible manner.\nFurther Links Introducing ChatGPT by Cassie Kozyrkov\nhttps://medium.com/@kozyrkov/introducing-chatgpt-aa824ad89623\nThe Batch by DeepLearningAI — Andrew Ng on LLM\nhttps://www.deeplearning.ai/the-batch/issue-174/\nAI Snake Oil on why ChatGPT is a bullshit generator\nhttps://aisnakeoil.substack.com/p/chatgpt-is-a-bullshit-generator-but\n","permalink":"https://patrickpt.github.io/posts/2023_01_07_intro_chatgpt/2023_01_07_intro_chatgpt/","summary":"ChatGPT: Optimizing Language Models for Dialogue\nWhat to do if you have 15Min time to spare? Feed ChatGPT with prompts to write an introductory article about ChatGPT. And I promise, this is the only part which is not based on a Large Language Model. Everything else was written by ChatGPT. For better readability I replaced the prompts with simple headers.\nWhat is ChatGPT? Are you tired of boring, robotic chatbots that can’t hold a conversation or understand your needs?","title":"An introduction to ChatGPT written by ChatGPT"},{"content":"Welcome to my blog.\nWhy not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.\nIsn\u0026rsquo;t it actually dumb to create your own blog instead of using these possibilities?\nMaybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.\nWell, couldn\u0026rsquo;t you have done this also on Medium?\nI tried, but starting to create content i saw some pitfalls with Medium:\nThe writing interface is straightforward and easy to use but it is actually too simple and gives almost no flexibility. Medium attracts a lot of good writers but also a lot of people just producing content with low quality. If you don\u0026rsquo;t optimize your articles you won\u0026rsquo;t attract any readers. So for my purpose there is no difference whether i use a private blog or Medium. Medium does not support Math Formulas! I could not believe it but there was no decent possibility to include math formulas with Latex or somehow else. An absolute No Go in my eyes Ok i get it, Medium is not for you but a blog is a lot of maintenance and effort. How do you find time for this?\nLuckily it is actually quite easy to create your own static site with Github Pages and Hugo. Also it is a good chance to learn somehting new and make yourself familiar with web design again. Ok, i am curious. How did you do it?\nHow did you create your blog? Which tools are you using for your blog? As written, i use Github Pages and the open-source static site generator Hugo which is written in Go. (I chose Hugo without having done a lot of research. There are other generators like e.g. Jekyll but Hugo was open-source, easy to use, blazing fast and free so no need to look further).\nAnd how did you do actually do it? Install Hugo\nbrew install hugo If you have a different system than MacOS check the official installation guide.\nCreate a new site in Hugo\nWhen you decide on a name, think that the name is also the folder and the name of the Repo. To work with Github Pages it needs to have the same name as your git user followed by .github.io\nhugo new site \u0026lt;user\u0026gt;.github.io -f yml I decided to use PaperMod theme and it recommends to use .yml Config instead of .toml - and i am fine with that because i like yaml files.\nInstall your favorite Hugo Theme\nThe project is created but if you try to run it, it will just be an empty page. A style is needed to make it fully functional.\nYou could create one from scratch but Hugo has a bunch of themes already prepared and ready to use! You can go to https://themes.gohugo.io/ and choose a theme you like. I like PaperMod\ngit init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod Adjust your config.yml with theme: PaperMod\nbaseURL: http://\u0026lt;user\u0026gt;.github.io/ languageCode: en-us title: My New Hugo Site theme: PaperMod Create Content\nImportant is your content folder. The folder tree in this folder will reflect your site folder tree. Either you create folders and markdown files in those folders manually or you use\nhugo new e.g.\nhugo new /content/posts/first-post/first-post.md I would recommend to create a folder per post to store your pictures in the same folder as your post they relate to\n--- heading: \u0026quot;Welcome to my blog\u0026quot; subheading: \u0026quot;This is my first-post\u0026quot; --- Look at your new site\nWith\nhugo you create the html out of your content in your public folder. Please keep in mind that for Github Pages you can only choose docs as your root directory for your index.html. Therefore you need to add following config to your config.yml\npublishDir: docs With\nhugo server -D you can see the site on localhost with port 1313. -D is used to include drafts.\nPush to github\nNow you can push your site to github\ngit add . git commit -m \u0026quot;initial commit\u0026quot; git push origin main Configure Github Pages In your repo settings under Pages the root folder needs to be adjusted and your site will hopefully be deployed soon.\nWith this short introduction you should be able to set up your own blog really fast and in worst case troubleshoot your way through. Enjoy!\nI am happy with my new blog and will further play around with it. If you like my content connect via LinkedIN.\nReferences [1] Github Pages (https://pages.github.com)\n[2] Hugo (https://gohugo.io)\n[3] PaperMod (https://github.com/adityatelange/hugo-PaperMod)\n[4] Markdownguide (https://www.markdownguide.org/basic-syntax/)\n[5] Image clickable (https://discourse.gohugo.io/t/how-can-i-make-images-clickable-so-i-can-zoom-them-to-full-screen/34279)\nFurther Links [6] Folder Structure (https://jpdroege.com/blog/hugo-file-organization/)\n[7] Markdown with VS Code (https://code.visualstudio.com/docs/languages/markdown)\n[8] Trouble with Image paths (https://github.com/adityatelange/hugo-PaperMod/discussions/690)\n[9] Work in Codespaces (https://shotor.com/blog/build-a-hugo-static-site-in-your-browser-using-github-codespaces/)\n[10] Katex for PaperMod (https://adityatelange.github.io/hugo-PaperMod/posts/math-typesetting/)\n","permalink":"https://patrickpt.github.io/posts/2023_01_01_why_a_blog/why_a_blog/","summary":"Welcome to my blog.\nWhy not Medium? It is 2023 and there are plenty of easy ways to create content about ML, Data Science and AI on the internet. In fact with the accessability of platforms like Medium it is super easy.\nIsn\u0026rsquo;t it actually dumb to create your own blog instead of using these possibilities?\nMaybe, but my purpose is not to attract as many readers as possible but to learn something and make my learnings accessable for others.","title":"Why and how do you create your own AI blog?"},{"content":"Welcomy to my blog. I am Patrick.\nI\u0026rsquo;m living in Germany, am a proud father of two, work as a Data Scientist and love to dig into AI, Data Science and ML.\nOn this blog i am documenting my learning notes, best practices and how-to\u0026rsquo;s.\nThis blog may contain errors so i am happy to receive your change requests via Github.\n","permalink":"https://patrickpt.github.io/about/","summary":"Welcomy to my blog. I am Patrick.\nI\u0026rsquo;m living in Germany, am a proud father of two, work as a Data Scientist and love to dig into AI, Data Science and ML.\nOn this blog i am documenting my learning notes, best practices and how-to\u0026rsquo;s.\nThis blog may contain errors so i am happy to receive your change requests via Github.","title":"About"}]